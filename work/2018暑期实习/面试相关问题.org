#+LATEX_HEADER: \usepackage{xeCJK}
#+LATEX_HEADER: \setmainfont{"微软雅黑"}
#+ATTR_LATEX: :width 5cm :options angle=90
#+TITLE: 面试相关问题
#+AUTHOR: 杨 睿
#+EMAIL: yangruipis@163.com
#+KEYWORDS: 
#+OPTIONS: H:4 toc:t 


* 实习准备
** 印象最深的项目
VISAP
** 最难的问题以及如何解决

- 文本挖掘工具 匹配规则制定（根据表达式匹配）
  - 充分结合数据结构方面的知识：后缀表达式（逆波兰式）
  - 满足了分析师的新需求，优先级，或且非，括号（公司系统没有实现）
  - 优化了存储，仅通过索引进行操作（snowNLP借鉴)

- VISAP 空间作图  

** 最大的优缺点
优点：学习能力
缺点：急性子，想到什么就去做什么。解决办法：emacs 时间管理，包括任务类型，开始时间，结束时间，完成状态，优先级
** 读过哪些开源项目

- snowNLP
- costcla
- sklearn


** 有没有关注最前沿的机器学习动态
微软mmdnn 多深度学习平台切换

* 技巧
- 写代码时：先写一个很快能实现的答案，再和面试官口上说更好的答案。一方面不让面试官等，二来可以和面试官有的聊。

* 专业问题

** 机器学习
*** LASSO怎么求解
1. 坐标下降 O(mn)，针对可微的凸优化
2. 最小角回归 
*** 决策树剪枝
1. 前剪枝（设置参数）
2. 后剪枝：
   1. 误差降低剪枝，原始根节点和去掉一个节点后根节点在测试集上的误判数量对比，如果去掉后误判减少了，则实现剪枝
   2. 悲观剪枝，不需要测试集，二项分布渐进正态，连续修正因子，均值、方差为np、np(1-p)，当子树错误率大于等于叶子节点的错误率+一个标准差后，进行剪枝
*** 现在研究
金融复杂网络+空间计量
*** 想了解什么
工作状况、业务场景
*** SVM 和 LR 区别 与 联系
区别：
- 损失函数，LR：logistic loss， SVM：hinge loss
- 优化方法
- 适用范围：LR线性，考虑所有样本点，对异常值敏感；SVM只受支持向量影响
- LR适合大量数据，可用随机梯度或小批量梯度下降解决；SVM数据量大时训练慢，要找KKT条件
联系：
- LR类似不带核的支持向量机

*** 常用的统计量
- p值
- t值
- 

*** 如何判断凸优化
海森矩阵半定

*** libsvm和liblinear有什么区别
      libsvm主要解决通用典型分类问题
      liblinear 解决大规模是数据线性模型设计
*** 过拟合、欠拟合的特征、如何判断、如何处理
一个高方差、一个高偏差
学习曲线，交叉验证
LR：加正则
DT：剪枝
SVM：调整软间隔参数
NN：dropout
*** 随机森林 GBDT的区别
*** 损失函数类型
- hinge loss
- logistic loss
*** SVM 对偶问题的意义
凸二次规划对偶问题更好求解，根据KKT条件，只要计算少数几个支持向量的距离即可

*** Softmax回归是什么
V_i为V中i元素个数，softmax = e^{V_i} / \sum_j e^{V_j}
logistic回归是softmax在二分类情况下的退化
*** stacking 优缺点
优点： 提升效果，操作简单，训练可并行
缺点：容易过拟合
*** 是否了解mutual infomation、chi-square、LR前后向、树模型等特征选择方式
*** 机器学习算法调试（梯度检验）
根据极限的定义，损失函数的参数增加一个很小的量，用产生的delta除以该量，得到梯度，和实际计算梯度对比
*** 常用优化算法
- SGD + momentum（梯度为F，动量为摩擦力，学习率为速度，F=ma）
- （不会就别说）adaDelta：每个参数计算相应的学习率，加入动量momentum，防止学习率衰减或梯度消失
*** 梯度下降和牛顿法区别
一个是平面逼近，一个是曲面逼近
一个是一阶偏导，一个是二阶偏导矩阵
*** SVM 原理
距离超平面最近的不同类别的点几何间隔最大化
*** SVM原问题与对偶问题关系
原问题是在最大化alpha的前提下最小化w，对偶问题时再最小化w的前提下最大化alpha
*** KKT条件应用
互补对偶条件 α_i (y_i (w^T x + b) - 1) = 0
1. α = 0
2. α != 0
*** TF-IDF 算法
TF：词在该文本中出现频率
IDF： log(总文本数 / 包含该文本的数目)
缺点：忽略上下文，改进：word2vec doc2vec

*** LSTM GRU区别
LSTM：输入门、输出门、遗忘门
GRU：更加简单实现，更新门（前一时刻状态信息被带入当前状态）重置门（忽略前一时刻状态信息的程度）
*** EM 与 kmeans 的关系
https://www.cnblogs.com/rong86/p/3517573.html
*** kmeans优化、效果评估、k值确定，初始点确定，优缺点
效果评估：
- 轮廓系数
- 组间组内距离

K值确定：
- 迭代，根据效果取最好的
- 层次聚类

初始点确定：
选择第一个初始点的第一个值后，找离他最远的，作为第二个值，再找离这两个点中心最远的，作为第三个，依次类推

优缺点：
优点：原理简单，容易理解
缺点：算法每次迭代需要计算每个点到每一个中心距离，复杂度高；K值，初始值难以确定

优化：
- kmeans++ (初始点如上方法确定）
- mini-batch kmeans 每次选取一部分数据进行优化
- KD树，类似KNN中找最近邻的问题，用kd树解决

*** lasso 在 0处不可导怎么办
通过坐标下降法求解
*** SVD SVD++
SVD: 矩阵分解+baseline mode,考虑每个样本的偏移项
SVD++: SVD基础上引入隐式反馈,比如用户的浏览数据,历史评分数据
*** LR分布式代码

*** GBDT 正则化
1. 类似adaboost，添加步长(学习率)，较小的步长需要较大的迭代次数。
2. 只选择一部分样本，无放回抽样，比例一般在50%-80%
3. CART树剪枝

** 数据库
*** 四大属性
ACID，原子性，一致性，持久性，隔离性

*** 优化
https://www.zhihu.com/question/19719997
https://www.cnblogs.com/downey/p/5302088.html

*** Hive mysql 区别

1. Hive基于hadoop上，存储在HDFS中
2. Hive不支持对数据的修改和添加
3. Hive没有索引，通过分布式暴力扫描，因此访问延迟较高，不适合在线数据查询
4. 由于建立在集群之上，所以支持超大规模数据

** 计算机

*** python装饰器
#+BEGIN_SRC python
from functools import wraps

def timeit(function):
    @wraps(function)
    def func_time(*args, **kwargs):
        t0 = time.time()
        result = function(*args, **kwargs)
        t1 = time.time()
        print(t1 - t0)
    return result
return func_time

@timeit
#+END_SRC

*** python垃圾回收
引用计数，当计数为0时，进行回收

导致引用计数+1的情况：
- 对象被创建，例如a=23
- 对象被引用，例如b=a
- 对象被作为参数，传入到一个函数中，例如func(a)
- 对象作为一个元素，存储在容器中，例如list1=[a,a]

导致引用计数-1的情况：
- 对象的别名被显式销毁，例如del a
- 对象的别名被赋予新的对象，例如a=24
- 一个对象离开它的作用域，例如f函数执行完毕时，func函数中的局部变量（全局变量不会）
- 对象所在的容器被销毁，或从容器中删除对象

*** python多重继承

python3: 深度优先+从左到右
*** 继承、封装、多态

*** 接口、抽象类

接口实现动作，抽象类告诉这个是什么
*** 进程、线程区别
1. 进程类似工厂，线程类似里面每一个生产线
2. IO密集型用多线程、CPU密集型用多进程
** 算法
*** 长为n的数组前K个最大的数
维护一个长为K的数组，排序（快排），或者是用二叉树存，开始读n-k个数，每次数来了跟最小的比，如果大了，则插入，如果小了，继续
*** 长为m和长为n的两个字符串，找最长公共子串
用 mxn 的矩阵存储在某一位置是否匹配，且如果左上角非0，则该元素在左上角基础上加1，找到矩阵中最大的元素，以及位置
*** 数轴上从左到右有n个点a[0],a[1]…,a[n-1]，给定一根长度为L的绳子，求绳子最多能覆盖其中的几个点。要求算法复杂度为o(n)

#+BEGIN_SRC python

def cover(a: list, l: int):
    begin = 0
    end = 1
    max_cover = 1
    while end < len(a):
        if a[end] - a[begin] > l:
            print(max_cover)
            max_cover = max(end-begin, max_cover)
            begin += 1
        else:
            end += 1
    return max_cover

a = cover([0,1,3,6,8,10], 8)
a
#+END_SRC

#+RESULTS:
: None

*** 已知二叉树前序和中序遍历结果，求后序遍历结果
关键点：
1. 根据前序和后序确定根节点，前序是第一个，后序是最后一个
2. 在中序中按根节点分割，左侧再在前序或后序中找根节点，如此递归
*** 统计出现次数最多的K个数字
先hash统计词频，在找前K个最大的

*** 单链表 反转
通过三个指针存储相邻的三个节点


** 智力题
*** 最佳停时问题
37% 可用蒙特卡洛模拟做
