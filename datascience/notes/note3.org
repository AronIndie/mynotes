#+LATEX_HEADER: \newenvironment{lequation}{\begin{equation}\Large}{\end{equation}}
#+ATTR_LATEX: :width 5cm :options angle=90
#+TITLE: Algorithm when DataMining3
#+AUTHOR: 杨 睿
#+EMAIL: yangruipis@163.com
#+KEYWORDS: Machine Learning
#+OPTIONS: H:4 toc:t 

* 神经网络与深度学习

** 神经网络与深度学习简介

- 神经网络无需赘述
- "深度学习"是为了让层数较多的多层神经网络可以训练，能够work而演化出来的一系列的 新的结构和新的方法。

新的网络结构中最著名的就是CNN，它解决了传统较深的网络参数太多，很难训练的问题，使用了“局部感受野”和“权植共享”的概念，大大减少了网络参数的数量

- 原来多层神经网络做的步骤是：特征映射到值。特征是人工挑选。
- 深度学习做的步骤是 信号->特征->值。 特征是由网络自己选择。

** 感知机学习

感知机是一个模仿神经元的模型

[此处有图(自己想象)]

接受多个输入（x1，x2，x3...），产生一个输出（output），超参数为阈值，待估参数为每个x的权重

当加权和大于阈值时，信号激活，输出1，否则输出0


** 一般神经网络
实际决策中，模型要复杂得多，由多个感知机组成，可能是多层的结构，甚至有多个输出。

神经网络需要在给定输入和输出下，估计出每个神经元最优的权重向量w和阈值b(-threshold)

但是，如果每个神经元输出结果是0或者1，将会使结果过于敏感，因此要通过sigmoid函数将其转为连续输出

*** 激活函数

1. sigmoid
2. tanh
3. 修正线性单元(Rectified linear unit，ReLU）

ReLU(x) = 
- 0, if x <= 0
- x, if x > 0

~起到单侧抑制的作用~
~由于非负区间的梯度为常数，因此不存在梯度消失问题(Vanishing Gradient Problem)~

** BP反向传播网络

ref : http://blog.csdn.net/u014303046/article/details/78200010

=损失函数和代价函数区别= 

- 损失函数主要指的是对于单个样本的损失或误差； 
- 代价函数表示多样本同时输入模型的时候总体的误差——每个样本误差的和然后取平均值。

=什么是反向传播网络=
- 后项传播（正向），估计出神经元误差
- 前向传播（反向），估计出参数

=优点和不足=
残差传播到最前面的层已经变得很小，会出现梯度扩散，影响精度

*** 推导
~牢记四个公式以及其推导过程~


=单样本情况下：=

定义l为神经网络层编号，j为神经元编号，z为线性值，a为激活值，sigmoid(z)=a

那么，如果需要为第l层的第j个神经元的线性值添加一个扰动 $\Delta z_j^{[l]}$ ，需要使得最后的损失函数尽可能的变小，那么需要在其负梯度上进行，我们定义这个梯度为其误差 :

\begin{eqnarray}
\nonumber
\delta_j^{[l]} = \frac{\partial L(a, y)}{\partial z_j^{[l]}}
\end{eqnarray}

**** 公式一：输出层误差

\begin{eqnarray}
\nonumber
\delta_j^{[l]} = \frac{\partial L }{\partial a_j^{[l]}} Sigmoid^{-1} (z_j^{[l]})
\end{eqnarray}

证明：

[[file:pics/bp_equ_1.png]]

**** 公式二：隐含层误差

\begin{eqnarray}
\nonumber
\delta_j^{[l]} = \sum_k w_{kj}^{[l+1]} \delta_k^{[l+1]} Sigmoid^{-1} (z_j^{[l]})
\end{eqnarray}

w_{kj}^{[l]} 表示第l-1层的第j个神经元指向第l层的第k个神经元的权重

证明：
[[file:pics/bp_equ_2.png]]

**** 公式三：参数变化率，即w和b的梯度

\begin{eqnarray}
\nonumber
\frac{\partial L}{\partial b_j^{[l]}} &=& \delta _j^{[l]} \\
\nonumber
\frac{\partial L}{\partial w_{jk}^{[l]}} &=& a_k^{[l-1]} \delta _j^{[l]}
\end{eqnarray}

证明：
[[file:pics/bp_equ_3.png]]

**** 公式四：参数更新规则

以 α 为步长，负梯度为方向迭代，公式略

=多样本情况下:=

n表示第l层神经元个数，m为样本数
- 每一层的误差不再是一个n维的向量，而是一个nxm的矩阵
- 更新b的时候要对每一层的误差矩阵求行均值
- 得到的w依然是 nxn 的矩阵

*** 实现


** 卷积神经网络

ref: http://blog.csdn.net/aws3217150/article/details/46405095


要点：
1. 主要用在图像识别问题上
2. 自带正则化功能，大大减少了参数数目，因此减少了过拟合的程度

*** 卷积

卷积通过核矩阵，将一个较大的矩阵进行缩减

[[file:pics/convolved.png]]

- 如果原始矩阵不仅有长度、宽度，还有深度，则每一深度进行累加（比如：彩图深度为3）
- 如果有多个核函数，每个核函数分别计算
- 每个核函数总能得到一个 NxN 的卷积特征

**** 卷积中的超参数：

1. 补充0的长度P，一个是为了使图片的形状更方便我们进行卷积，另一个是因为它可以提高识别表现(详细原因请参考cs231n的课程)，比如5x5的图，P=2时得到7x7的图
2. 核函数的大小和数量
3. 步长 Stride，卷积中默认卷积核一次移动一个单位，其实可以移动Stride单位

假设图片宽度为W， 卷积核宽度为F， 步长为S，补0参数P，输出卷积特征的宽度为H，则有：

\begin{eqnarray}
\nonumber
H = (W - F + 2P) / S + 1
\end{eqnarray}


*** 池化(Pooling)

pooling是一个采样过程，一般采取max-pooling

[[file:pics/pooling.png]]



*** 全连接

和一般人工神经网络一样

*** 卷积层参数的确定

=共享权重=

同一个卷积核，核上每个元素的权重都一样，即如果是5x5的卷积核，则一个核一共只需估计5x5+1=26个参数（1位bias项）

例：
卷积特征 96x96x10（10个核）
-> 经过2x2，步长为2的池化后，得到48x48x10个特征
-> 再经过 5x5，步长为1，16个卷积核进行卷积，得到(5x5x10+1)x16=4016个参数，输出特征为44x44x16
-> 再pooling，得到22x22x16个输出特征
-> 此时，全连接到100个神经元的隐含层，需要的参数为：(22x22x16+1)*100=774500

最后得到的参数=774500 + 4016 + 隐含层到输出层的参数

** 循环神经网络(Recurrent Neural Network)

ref：
    https://zybuluo.com/hanbingtao/note/541458
    https://zhuanlan.zhihu.com/p/24720659

~以层的概念理解神经网络结构，而不是节点~

*** 单向循环神经网络

[[file:pics/rnn_1.png]]

中间的隐含层为循环层，循环层每一个节点的值不仅受与其连接的x的影响，还和上一个循环层节点的影响。

\begin{eqnarray}
\nonumber
o_t = g(V_{s_t}) \\
\nonumber
s_t = f(Ux_t + W _{t-1}) 
\end{eqnarray}

其中每个循环层神经元的 W, V, U 都是完全一样的，这是循环神经网络的 =共享权重= 特征，是递归网络相对于前馈网络而言最为突出的优势。

=时间结构共享是递归网络的核心中的核心。=

*** 双向循环神经网络

上述神经网络方向是 S_{t-1} 到 S_t ，而我们还可以加入一个反向的循环层

[[file:pics/rnn_2.png]]

此时需要同时保存两个层：

\begin{eqnarray}
\nonumber
o_t = g(V_{t} + V_{t}^') \\
\end{eqnarray}

*** 训练方法：BPTT

见：https://zybuluo.com/hanbingtao/note/541458


*** softmax 层

ref： https://www.zhihu.com/question/23765351

\begin{eqnarray}
\nonumber
S_i = \frac{e^{V_i}}{\sum_j e^{v_j}}
\end{eqnarray}

其中V_i表示V中第i个元素，Softmax即是该元素的指数，和所有元素指数和的比值

[[file:pics/rnn_5.png]]


*** 优缺点

优点：
1. 解决时序问题
2. 共享权重

缺点：
1. 时序过长时出现梯度爆炸和梯度消失问题

=什么是梯度爆炸和梯度消失=

[[file:pics/run_3.png]]

当t-k很大时，误差将极快的收敛到0或者无穷大

如何解决：长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU）





*** 输入与输出

=方式：=

[[file:pics/rnn_4.png]]

- many to one：常用在情感分析中，将一句话关联到一个情感向量上去。
- many to many：第一个many to many在DNN-HMM语音识别框架中常有用到
- many to many(variable length)：第二个many to many常用在机器翻译两个不同语言时。

=类型=

与其他前馈网络不同，该网络必须包含时间参数
输入张量形状：(time_steps, n_samples, dim_input)
输出张量形状：(time_steps, n_samples, dim_output)


** 递归神经网络(Recursive Neural Network)                              :了解:

ref: https://zybuluo.com/hanbingtao/note/626300

循环神经网络将句子看成一个序列，然而很多时候句子是有结构的（语法树：树状结构）

需要更多人工标注的语料

** LSTM

为了解决循环神经网络（以后默认RNN为循环神经网络）存在的梯度爆炸和梯度消失的问题，LSTM(Long Short Term Memory Network, LSTM)被提出来

LSTM在循环层中不仅存储原来的状态 h_t 还存储一个长期的状态 c_t (单元状态)，如下图所示

[[file:pics/LSTM_0.png]]

其中每一个状态都是向量。

~LSTM的关键，就是怎样控制长期状态c~

为此，LSTM提出门(gate)的概念，类似阀门，可以控制只让一部分的状态进来，门事实上是一个全连接层，输入一个向量，输出一个0到1之间的实数等长向量，用我们要控制的向量乘以门得到的结果向量，就可以达到控制的目的。

LSTM有三个门：

- 遗忘门：它决定了上一时刻的单元状态 c_{t-1} 有多少保留到当前时刻单元状态 c_t
- 输入门：它决定了当前时刻网络的输入 x_t 有多少保存到单元状态 c_t
- 输出门：它决定了当前时刻的单元状态 c_t 有多少输出到循环层节点的输出值 h_t

总体流程如下图：（非常关键）

[[file:pics/LSTM_1.png]]

上图解释如下：

1. 首先，输入 x_t 经过变换：

\begin{eqnarray}
\nonumber
f_t = \sigma (W_f [h_{t-1}, x_t] + b_f)
\end{eqnarray}

得到遗忘门向量，作用于 c_{t-1}上（直接按元素乘以 c_{t-1})

2. 再将 x_t 经过输入门的变化（公式类似），得到输入门向量 i_t ， ~输入门是作用在当前的单元状态 c'_t 上的~

3. 计算用于描绘当前输入的单元状态 c'_t ，根据上一次的输出 h_{t-1} 和这次的输入 x_t，得到结果后乘以输入门，达到控制效果，然后加到经过遗忘门后的上一期单元状态中，得到更新后当期单元状态 c_t ，可输出为 c_t 。由于遗忘门的控制，它可以保存很久很久之前的信息，由于输入门的控制，它又可以避免当前无关紧要的内容进入记忆。

4. 计算 h_t 的输出门 O_t ，它控制了长期记忆对当前输出的影响。
5. 得到LSTM最终输出，它是由输出门和单元状态共同确定的：

\begin{eqnarray}
\nonumber
h_t = O_t \odot tanh(c_t)
\end{eqnarray}


** 实现

*** 方案1. win10 tensorflow 安装
ref:
http://blog.csdn.net/weixin_36368407/article/details/54177380

**** Cuda & cudnn

**** TensorFlow

*** 方案2. 基于theano的Keras win10 安装

http://blog.csdn.net/circle2015/article/details/54235127

1. 安装 mingw

#+BEGIN_SRC bash
conda install mingw libpython
#+END_SRC

2. pip install theano
3. pip install keras
4. 主文件夹中找到 .keras的配置文件，修改默认后台为theano
5. 配置theano

c://Users//ray//.theanorc.txt 文件，里面加入theano的配置项
#+BEGIN_SRC 
[global]
floatX=float32
device=cpu
[blas]
ldflags=-LC:\\Users\\yangr\\Documents\\OpenBLAS\\bin -LC:\\Users\\yangr\\Documents\\OpenBLAS\\lib -lopenblas
[gcc]  
cxxflags=-IC:\\Users\\yangr\\Anaconda3\\MinGW
#+END_SRC

其中openBlas加速库要提前下载：
1. 下载openblas库http://sourceforge.net/projects/openblas/files/v0.2.14
2. 下载mingw64 http://sourceforge.net/projects/openblas/files/v0.2.14/mingw64_dll.zip/download ，并将其添加到openblas/bin/
3. 路径输入至.theanorc.txt

=如果遇到问题=

#+BEGIN_SRC python
File "E:\Things_Installed_Here\Anaconda_Py\envs\Py341\lib\site-packages\theano-0.7.0-py3.4.egg\theano\gof\cmodule.py", line 331, in dlimport
    rval = __import__(module_name, {}, {}, [module_name])
ImportError: DLL load failed: The specified module could not be found.
#+END_SRC

则如下操作：
#+BEGIN_SRC bash
$ conda remove mingw
$ conda install m2w64-toolchain
#+END_SRC
*** Keras Tips

**** 自定义 metrics 性能评估函数
#+BEGIN_SRC python
import keras.backend as K
def f1_score(y_true, y_pred):

    # Count positive samples.
    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))
    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))

    # If there are no true samples, fix the F1 score at 0.
    if c3 == 0:
        return 0

    # How many selected items are relevant?
    precision = c1 / c2

    # How many relevant items are selected?
    recall = c1 / c3

    # Calculate f1_score
    f1_score = 2 * (precision * recall) / (precision + recall)
    return f1_score
#+END_SRC

*** Experiment 1. 信贷违约预测模型

*** Experiment 2. 一个简单的缺失语言自动补全


*** Experiment 3. 手写数字识别
* 推荐算法
* 机器学习算法调试
** 梯度检查
* 数据库

