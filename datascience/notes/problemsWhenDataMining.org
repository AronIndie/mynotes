#+LATEX_HEADER: \usepackage{xeCJK} 
#+LATEX_HEADER: \setmainfont{"微软雅黑"}
#+ATTR_LATEX: :width 5cm :options angle=90
#+TITLE: Problems When Data Mining
#+AUTHOR: Ray Yang
#+EMAIL: yangruipis@163.com
#+KEYWORDS: Data Mining
#+OPTIONS: H:4 toc:t 


* TODO 数据预处理时遇到的问题[0/1]
- State "TODO"       from "TODO"       [2017-12-23 周六 14:18]
- State "TODO"       from "TODO"       [2017-12-23 周六 14:18]
- State "TODO"       from "DONE"       [2017-12-23 周六 14:18]
- State "DONE"       from ""           [2017-12-23 周六 14:18]

** 离散变量与连续变量的识别
  规则1) 遇到文本型变量，先排除其为唯一id或是编号或是日期的可能，然后将其归为离散变量（一般是名义变量）
  规则2) 当变量为整型时，value_counts，根据结果长度判断是不是
      - 如果变量唯一值较多，比如日期，则判断其为连续变量
      - 如果变量唯一值较少，且分布较平衡则为离散变量（一般是序次变量）
  规则3) 当变量为浮点型时，同样需要进行value_counts，当唯一值较少时，确定其为离散变量，否则为连续变量
** 离散特征分布不平衡时如何识别（imbalance variable）
依次对变量进行value_counts()，提取最大的计数，如果最大的计数占总数80%以上，则视为非平衡变量，如果此时变量数目非常多，则可以直接放弃该变量


#+BEGIN_SRC python :results output
  # 删除df_all里面所有值都相同的列
  def dropSameCol(df):
      result_list = []
      for col in df.columns:
          count_values = df[col].value_counts()
          if count_values.shape[0] == 1:
              result_list.append(False)
          else:
              if count_values.shape[0] <= 5 and max(count_values) >= 4.0*sum(count_values)/5:
                  result_list.append(False)
              else:
                  result_list.append(True)
      return df.loc[:, result_list]
#+END_SRC

#+RESULTS:

** 哪些变量需要做oneHotEncoding

只有名义变量需要做one Hot Encoding
注意虚拟变量陷阱，即k个类型只需要k-1个变量（防止矩阵非满秩，变量数大于样本数，无法取逆）
|            | 浮点型   | 整型     | 字符串                       |
|------------+----------+----------+------------------------------|
| 大量唯一值 | 连续变量 | 连续变量 | 可能为id，date，建议直接去掉 |
| 少量唯一值 | 有序变量 | 有序变量 | 名义变量                     |

** 什么情况下需要归一化
只有基于*距离*的模型和基于*参数*的模型需要归一化，基于树的模型无需归一化。
** TODO 面对回归问题而不是分类问题时，特征选择方法有哪些
- State "TODO"       from ""           [2017-12-23 周六 10:46]

** lasso回归遇到所有系数均为0的情况该怎么办
- 降低alpha值
- 增加最大迭代次数
- 减小tolerance
** 交叉验证与标准化的先后问题
- 在交叉验证时是否会用训练集数据的参数对测试集进行标准化？
** 数据分割与标准化的先后问题
先分割数据，再进行标准化，用分割出的训练集的均值和方差参数对测试集进行标准化

** 特征选择方法以及适用情况

#+BEGIN_SRC python :results output
  import matplotlib.pyplot as plt
  %matplotlib inline
  # 查看选出了哪几个 feature, 黑色是选出来的 （当总特征数较少的时候使用）
  mask = select.get_support()
  print(mask.reshape(1, -1)[:10])
  # visualize the mask. black is True, white is False
  plt.matshow(mask.reshape(1, -1), cmap='gray_r');
#+END_SRC


- filter方法（根据各个自变量和因变量之间的关系）
  - 相关系数：适用于连续变量（pearson, spearman, kendel)或者是离散变量(spearman, jaccard)
  - 卡方检验：只适用于离散变量

    #+BEGIN_SRC python :results output
      # 以 chi2 结合SelectKbest 为例
      # 要求：X必须非负，所以我们事先对X_train做一次MaxMin归一化
      from sklearn.feature_selection import chi2
      from sklearn.feature_selection import SelectKBest

      select = SelectKBest(chi2, k=10) # 选出前10个
      y_train = y_train.astype(float)
      X_uni_selected = select.fit_transform(X_train_norm, y_train)
      X_uni_selected_test = select.transform(X_test_norm)
      print(X_train_norm.shape)
      print(X_uni_selected.shape)
    #+END_SRC
  - 互信息
    - 评价一个事件出现对另一个事出现所贡献的信息量，用于离散的定性变量
    - 最大信息系数基于互信息被提出，用以处理定量数据[[http://blog.csdn.net/qtlyx/article/details/50780400][MIC]]

      #+BEGIN_SRC python :results output
        from sklearn.feature_selection import SelectKBest
        from minepy import MINE
        #由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
        def mic(x, y):
            m = MINE()
            m.compute_score(x, y)
            return (m.mic(), 0.5)

        #选择K个最好的特征，返回特征选择后的数据
        SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
      #+END_SRC

  - 信息增益
    这个不多说，暂且用于离散数据吧
- wrapper方法（通过预测效果，或者说是目标函数，来判断是否加入该变量，往往需要迭代）
  - 递归特征消除法

    #+BEGIN_SRC python :results output
      from sklearn.feature_selection import RFE
      from sklearn.linear_model import LogisticRegression

      #递归特征消除法，返回特征选择后的数据
      #参数estimator为基模型
      #参数n_features_to_select为选择的特征个数
      RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
    #+END_SRC

- embedded方法（通过学习器自动选择特征）
  - lasso回归(L1惩罚项)

    #+BEGIN_SRC python :results output
      from sklearn.feature_selection import SelectFromModel
      from sklearn.linear_model import LogisticRegression

      #带L1惩罚项的逻辑回归作为基模型的特征选择
      SelectFromModel(LogisticRegression(penalty="l1", C=0.1)).fit_transform(iris.data, iris.target)
    #+END_SRC

  - lasso回归结合岭回归(L1 + L2惩罚项)
    L1只是从若干相关的变量中选一个，没选到不代表无关，正确做法是，若一个特征F_0在L1中系数为1，L2中系数位a，选择L2中系数同样接近于a且在L1中系数为0的特征F_i，并且让F_i评分F_0在L1中的系数（来自[[https://www.zhihu.com/question/29316149][知乎]]）

- 基于树的特征选择

  #+BEGIN_SRC python :results output
    from sklearn.feature_selection import SelectFromModel
    from sklearn.ensemble import GradientBoostingClassifier

    #GBDT作为基模型的特征选择
    SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)
  #+END_SRC

** 测试集中的分类变量取值，在训练集中没有
1. 样本量少，且较多测试样本存在该现象，那么剔除改变量，重新训练模型
2. 样本量多，模型无法重新训练时，找到与其最相近的、存在训练集中的变量取值进行替换，可以考虑用KNN，把该变量作为Y，其余特征作为X

3. 如果发现变量可以往上规约，则尽量往上规约，比如城市虚拟变量往往无法匹配，可以在训练集和测试集中均维规约为省份虚拟变量


* 数据不平衡问题
** 简介

*** 什么是数据不平衡问题(imbalance dataset)
样本标签，或者说是预测目标，取值不平衡，比如为0的非常多，为1的非常少，导致分类器容易将所有样本均预测为0，带来的准确率却很高

*** 为什么类不平衡是不好的
- 从模型的训练过程来看：少量样本提供的信息过少，是的训练容易受误差干扰
- 从模型的预测过程来看： *当预测几率大于观测几率时* ，样本被判为正类，比如先验的观测几率是0.5，而少量样本容易扭曲观测几率

** 文献与方法综述

最详细的资料：https://pypi.python.org/pypi/imbalanced-learn#id31
以及用户手册：http://contrib.scikit-learn.org/imbalanced-learn/stable/

*** 抽样方法
**** Under Sampling (欠采样法、向下采样法)，减少多数类样本
***** Edited Nearest Neighbor (ENN)
对每个多数类的样本， 如果他的大部分K近邻样本是少数类，那么将该点删去

***** Repeated Edited Nearest Neighbor
重复ENN直至样本不发生改变

***** Tomek Link Removal
*REF* : "Two Modifications of CNN", 1976
如果样本点A和样本点B的最近邻（即K=1近邻）都是对方，且A与B分别属于少数类与多数类，则将该点删去

***** Ensemble 模型融合法
*REF* : "Exploratory undersampling for class-imbalance learning", 2009

*思想* :
    多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果

***** BalanceCascade 增强训练法
*REF* : "Exploratory undersampling for class-imbalance learning", 2009

*思想* :
    先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果

**** Over Sampling (过采样法，向上采样法），增加少数类样本
***** 随机有放回抽少数类样本到总样本中，并加入随机扰动

*** 合成方法
**** SMOTE (Synthetic Minority Oversampling TEchnique)
*REF* : 
    - "DATA MINING FOR IMBALANCED DATASETS:AN OVERVIEW"
    - "SMOTE: Synthetic Minority Over-sampling Technique", 2002

*步骤* ：
    1. 对于少数类样本集{x_i, i=1,...,n}，找到每个x_i的K个近邻样本点
    2. 对每个x_i，随机抽取K个近邻点中的一个，记为x_i^{(k)}
    3. 生成新的样本点 x_{i,1} = x_i + \eta_1 · (x_i^{(k)} - x_i)，其中\eta_1位0-1之间的随机数
    4. 将步骤3执行N次，最终得到N倍于原少数类样本的点

*改进* ：
    该方法的缺点是，增加了类之间重叠的可能性，并且有可能生成一些无意义的样本，因此有如下改进方法
    - Borderline-SMOTE
    - ADASYN

**** Borderline-SMOTE

*主要思想* ：
- 如果少数类样本点附近全是多数类的点，那么改点很明显为噪声，不做处理或者是剔除
- 如果少数类样本点附近有较多的多数类样本，那么说明该样本刚好处于分类的边界，具有较大的信息。
- 如果少数类样本点附近有较少的多数类样本点，那么该样本点很安全，如果强行合成新样本点，则会模糊分类的边界，因此不做处理

实际操作中，如果K/2以上的K近邻点都为多数类，那么就进行合成，否则不合成

**** SMOTE + ENN
*REF* : "A study of the behavior of several methods for balancing machine learning training data" Batista et al 2004

**** SMOTE + Tomek
*REF* : "A study of the behavior of several methods for balancing machine learning training data" Batista et al 2004

**** SMOTEBoost
*REF* : "SMOTEBoost: Improving Prediction of the Minority Class in Boosting", 2003

结合了SMOTE和AdaBoost算法，不断更新样本的分布
*** 加权方法

给与不同错误损失不同的权重，视情况而定

*** 一分类方法

当正负样本相差特别悬殊时，把他看成一分类或者是异常检测问题，此时重点不在于捕捉类间的差别，而是为其中一类进行建模，经典的工作包括One-class SVM等。


*** 方法选择
来自博客：http://blog.csdn.net/lujiandong1/article/details/52658675

1. 在正负样本都非常之少的情况下，应该采用数据合成的方式；
2. 在负样本足够多，正样本非常之少且比例及其悬殊的情况下，应该考虑一分类方法；
3. 在正负样本都足够多且比例不是特别悬殊的情况下，应该考虑采样或者加权的方法。
4. 采样和加权在数学上是等价的，但实际应用中效果却有差别。尤其是采样了诸如Random Forest等分类方法，训练过程会对训练集进行随机采样。在这种情况下，如果计算资源允许上采样往往要比加权好一些。
5. 另外，虽然上采样和下采样都可以使数据集变得平衡，并且在数据足够多的情况下等价，但两者也是有区别的。实际应用中，我的经验是如果计算资源足够且小众类样本足够多的情况下使用上采样，否则使用下采样，因为上采样会增加训练集的大小进而增加训练时间，同时小的训练集非常容易产生过拟合。
6. 对于下采样，如果计算资源相对较多且有良好的并行环境，应该选择Ensemble方法。


** 实现

python imbalance-learn 包
