<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-06-08 五 17:17 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Algorithm when DataMining</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="杨 睿" />
<meta name="keywords" content="Machine Learning" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js", "[Contrib]/siunitx/siunitx.js", "[Contrib]/mhchem/mhchem.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        TeX: {extensions: ["AMSmath.js","AMSsymbols.js",  "[Contrib]/siunitx/siunitx.js", "[Contrib]/mhchem/mhchem.js"]},
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Algorithm when DataMining</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org4a73d46">1. <span class="done DONE">DONE</span> 数据预处理</a>
<ul>
<li><a href="#orgb710db2">1.1. 认识数据</a></li>
<li><a href="#org1f97897">1.2. 变量类型判断</a></li>
<li><a href="#org6e3190b">1.3. 异常值、错误值处理</a></li>
<li><a href="#org3c0309c">1.4. 缺失值处理</a>
<ul>
<li><a href="#org44a4f18">1.4.1. 缺失机制</a></li>
<li><a href="#org59b1533">1.4.2. 脱敏与非脱敏数据</a></li>
<li><a href="#org1da2301">1.4.3. 不同机制的处理方法</a>
<ul>
<li><a href="#org002cea9">1.4.3.1. MAR, MCAR:</a></li>
<li><a href="#orgaf94f6e">1.4.3.2. MANR</a></li>
</ul>
</li>
<li><a href="#orgfbdc74f">1.4.4. sklearn的方法</a></li>
</ul>
</li>
<li><a href="#orgcde5f76">1.5. 新变量生成</a>
<ul>
<li><a href="#org6faa9f8">1.5.1. 根据变量含义生成新变量</a></li>
<li><a href="#orgb7920a3">1.5.2. 数据的特征构造新特征</a></li>
<li><a href="#org2a9dcf4">1.5.3. 多项式特征</a></li>
</ul>
</li>
<li><a href="#orgdbe375f">1.6. 连续特征标准化与区间缩放</a>
<ul>
<li><a href="#org35f202e">1.6.1. 注意</a></li>
<li><a href="#org3fbd21d">1.6.2. 标准化(standardization)</a></li>
<li><a href="#orgfda0703">1.6.3. 区间缩放</a>
<ul>
<li><a href="#org5563746">1.6.3.1. Rescaling(归一化)</a></li>
<li><a href="#org104f3f3">1.6.3.2. Mean normalization</a></li>
<li><a href="#org9dec4f2">1.6.3.3. Scaling to unit length</a></li>
</ul>
</li>
<li><a href="#org2e8cb7b">1.6.4. 特殊情况：稀疏点的区间缩放</a></li>
<li><a href="#org646405b">1.6.5. 特殊情况：离群点的区间缩放</a></li>
</ul>
</li>
<li><a href="#org4605408">1.7. 离散特征 独热编码</a>
<ul>
<li><a href="#org4a753bd">1.7.1. 方法一</a></li>
<li><a href="#org76a079d">1.7.2. 方法二：</a></li>
<li><a href="#org5b94866">1.7.3. 方法三(最好)</a></li>
</ul>
</li>
<li><a href="#org8891964">1.8. 数据不平衡方法</a>
<ul>
<li><a href="#org225f7d5">1.8.1. 简介</a>
<ul>
<li><a href="#orgb20aef3">1.8.1.1. 什么是数据不平衡问题(imbalance dataset)</a></li>
<li><a href="#org9c4c47d">1.8.1.2. 为什么类不平衡是不好的</a></li>
<li><a href="#orgf776bee">1.8.1.3. 什么样的模型需要处理非平衡数据:</a></li>
</ul>
</li>
<li><a href="#orgfd41752">1.8.2. 文献与方法综述</a></li>
<li><a href="#org6c6cb3f">1.8.3. 抽样方法</a>
<ul>
<li><a href="#org343179a">1.8.3.1. Under Sampling (欠采样法、向下采样法)，减少多数类样本</a></li>
<li><a href="#orgd66c5bb">1.8.3.2. Over Sampling (过采样法，向上采样法），增加少数类样本</a></li>
</ul>
</li>
<li><a href="#org58d68ca">1.8.4. 合成方法</a>
<ul>
<li><a href="#org2e64bac">1.8.4.1. SMOTE (Synthetic Minority Oversampling TEchnique)</a></li>
<li><a href="#org9889f53">1.8.4.2. Borderline-SMOTE</a></li>
<li><a href="#org1666cb0">1.8.4.3. SMOTE + ENN</a></li>
<li><a href="#org879619e">1.8.4.4. SMOTE + Tomek</a></li>
<li><a href="#org334ed2f">1.8.4.5. SMOTEBoost</a></li>
</ul>
</li>
<li><a href="#org3dee17d">1.8.5. 加权方法</a></li>
<li><a href="#org94ba0dd">1.8.6. 一分类方法</a></li>
<li><a href="#org578148f">1.8.7. 方法选择</a></li>
<li><a href="#org224b089">1.8.8. 实现</a></li>
</ul>
</li>
<li><a href="#org3b00182">1.9. 管道pipline</a>
<ul>
<li><a href="#orgccb79a8">1.9.1. FunctionTransformer自定义一个转化器,并且可以在Pipeline中使用</a></li>
<li><a href="#orgdceaca6">1.9.2. 运行流程</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org11189f4">2. <span class="done DONE">DONE</span> 特征预处理(特征工程)</a>
<ul>
<li><a href="#org531fbfc">2.1. <span class="done DONE">DONE</span> 特征选择</a>
<ul>
<li><a href="#org087327d">2.1.1. Filter</a>
<ul>
<li><a href="#org8065957">2.1.1.1. 方差法</a></li>
<li><a href="#org8ebe8ee">2.1.1.2. 相关系数</a></li>
<li><a href="#org05001c6">2.1.1.3. 卡方检验</a></li>
<li><a href="#org28e905c">2.1.1.4. 互信息</a></li>
</ul>
</li>
<li><a href="#orgc485a5f">2.1.2. Wrapper</a></li>
<li><a href="#orge53dc0e">2.1.3. Embedded</a>
<ul>
<li><a href="#org5ceb524">2.1.3.1. 基于正则项的特征选择</a></li>
<li><a href="#orgca8fe57">2.1.3.2. 基于树的特征选择</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org1cd3146">2.2. 降维</a>
<ul>
<li><a href="#org2e0ef25">2.2.1. PCA 主成分分析和EVD分解</a>
<ul>
<li><a href="#org698ce28">2.2.1.1. 主成分分析要求</a></li>
<li><a href="#org57fe999">2.2.1.2. 特征值和特征向量</a></li>
<li><a href="#org1469ce8">2.2.1.3. 特征分解(EVD)和主成分的关系</a></li>
</ul>
</li>
<li><a href="#org7a16cd9">2.2.2. SVD分解</a>
<ul>
<li><a href="#org97cf88a">2.2.2.1. 正交向量和正交矩阵</a></li>
<li><a href="#org72e8b2d">2.2.2.2. SVD介绍</a></li>
<li><a href="#org3735210">2.2.2.3. SVD性质</a></li>
</ul>
</li>
<li><a href="#orgbcdbd70">2.2.3. 线性判别分析</a>
<ul>
<li><a href="#orgdf5c8e0">2.2.3.1. LDA</a></li>
<li><a href="#orgff1dfe9">2.2.3.2. GDA</a></li>
<li><a href="#org0356355">2.2.3.3. 二次判别分析QDA</a></li>
</ul>
</li>
<li><a href="#orge83f554">2.2.4. LASSO</a></li>
<li><a href="#orgf28020f">2.2.5. 小波分析</a></li>
<li><a href="#org5eb7cb0">2.2.6. 深度学习SparseAutoEncoder</a></li>
<li><a href="#orge6323d3">2.2.7. 拉普拉斯映射(流形学习)</a></li>
<li><a href="#orga779b1f">2.2.8. 低维线性嵌入(流形学习)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgbd96d00">3. <span class="done DONE">DONE</span> 模型选择<code>[3/3]</code></a>
<ul>
<li><a href="#org9316fa4">3.1. <span class="done DONE">DONE</span> 模型评价<code>[4/4]</code></a>
<ul>
<li><a href="#orgac2bef5">3.1.1. <span class="done DONE">DONE</span> 二分类模型</a></li>
<li><a href="#orge6cf51d">3.1.2. <span class="done DONE">DONE</span> 多分类模型</a></li>
<li><a href="#org89fadd1">3.1.3. <span class="done DONE">DONE</span> 回归模型</a></li>
<li><a href="#org92d4001">3.1.4. <span class="done CANCELED">CANCELED</span> 聚类模型</a></li>
</ul>
</li>
<li><a href="#org559e58d">3.2. <span class="done DONE">DONE</span> 交叉验证</a></li>
<li><a href="#org196c7bd">3.3. <span class="done CANCELED">CANCELED</span> 网格搜索</a></li>
<li><a href="#orgb08a380">3.4. 如何检验过拟合</a></li>
</ul>
</li>
<li><a href="#org09e5ed9">4. <span class="done DONE">DONE</span> 基于树的算法<code>[2/2]</code></a>
<ul>
<li><a href="#orgf1f74c7">4.1. <span class="done DONE">DONE</span> 决策树</a>
<ul>
<li><a href="#org5d3931e">4.1.1. 分类树</a></li>
<li><a href="#org45c82e3">4.1.2. 回归树</a></li>
<li><a href="#org2684af4">4.1.3. <span class="todo TODO">TODO</span> 剪枝</a></li>
</ul>
</li>
<li><a href="#orgbb2619c">4.2. <span class="done DONE">DONE</span> 随机森林</a>
<ul>
<li><a href="#org4e6220a">4.2.1. 概述</a></li>
<li><a href="#orga380e3b">4.2.2. 优缺点</a>
<ul>
<li><a href="#org7ce5ca2">4.2.2.1. 优点</a></li>
<li><a href="#org7c5f0a9">4.2.2.2. 缺点</a></li>
<li><a href="#org2384555">4.2.2.3. 为什么随机森林不存在过拟合问题</a></li>
</ul>
</li>
<li><a href="#orgf4748d3">4.2.3. 实现</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org9c1f411">5. <span class="done DONE">DONE</span> KNN相关算法<code>[2/2]</code></a>
<ul>
<li><a href="#org1c9bbf5">5.1. <span class="done DONE">DONE</span> KNN</a>
<ul>
<li><a href="#org580ede7">5.1.1. 模型</a>
<ul>
<li><a href="#orge14325e">5.1.1.1. 三要素</a></li>
<li><a href="#org63199ab">5.1.1.2. 距离度量</a></li>
<li><a href="#orgbbe4ed2">5.1.1.3. K值的选择</a></li>
</ul>
</li>
<li><a href="#orgfaaf1ac">5.1.2. 分类的规则</a></li>
<li><a href="#org9ec34b3">5.1.3. 代码实现</a></li>
</ul>
</li>
<li><a href="#org2fc2a14">5.2. <span class="done DONE">DONE</span> KD树</a></li>
</ul>
</li>
<li><a href="#org510605d">6. <span class="done DONE">DONE</span> Logistic<code>[3/3]</code></a>
<ul>
<li><a href="#orgcb4a5e1">6.1. <span class="done DONE">DONE</span> 理论</a>
<ul>
<li><a href="#org805d37d">6.1.1. sigmoid函数</a></li>
<li><a href="#orgc20c802">6.1.2. 估计方法</a>
<ul>
<li><a href="#orgc5a492c">6.1.2.1. 最小二乘估计</a></li>
<li><a href="#org37ce269">6.1.2.2. 极大似然估计</a></li>
</ul>
</li>
<li><a href="#orgb6f55a2">6.1.3. 求解</a>
<ul>
<li><a href="#orgbab8db4">6.1.3.1. 梯度下降 （gradient descent）</a></li>
<li><a href="#orgca2fbe1">6.1.3.2. 随机梯度下降（stochastic gradient descent）</a></li>
<li><a href="#org605fab5">6.1.3.3. 小批量梯度下降（mini-batch gradient descent）</a></li>
<li><a href="#orgb187789">6.1.3.4. 学习率更新</a></li>
<li><a href="#org5be9f8b">6.1.3.5. 拟牛顿法</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgba3c42a">6.2. <span class="done DONE">DONE</span> 实现</a></li>
<li><a href="#org62f2a3c">6.3. <span class="done DONE">DONE</span> 多元logistic情况（Multinormal）</a></li>
</ul>
</li>
<li><a href="#orgbbd492e">7. <span class="done DONE">DONE</span> 基于贝叶斯的算法</a>
<ul>
<li><a href="#org9b837ce">7.1. 朴素贝叶斯</a>
<ul>
<li><a href="#org9b69b18">7.1.1. 贝叶斯模型简介</a></li>
<li><a href="#orgaa76ecf">7.1.2. 朴素贝叶斯模型的提出</a></li>
<li><a href="#org8ce8b82">7.1.3. 拉普拉斯平滑</a></li>
</ul>
</li>
<li><a href="#org82e5e6d">7.2. 最小错误率贝叶斯</a></li>
<li><a href="#org8eb3c60">7.3. 最小风险贝叶斯</a></li>
<li><a href="#org82a3dc1">7.4. 半朴素贝叶斯</a></li>
<li><a href="#orga871bdd">7.5. 贝叶斯网络</a></li>
<li><a href="#org539f2bf">7.6. 高斯过程回归</a></li>
<li><a href="#orge64fb0e">7.7. 贝叶斯优化</a>
<ul>
<li><a href="#orge024dd5">7.7.1. 简介</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgcd2e482">8. <span class="done DONE">DONE</span> EM算法</a>
<ul>
<li><a href="#org8760387">8.1. 简介</a></li>
<li><a href="#orgae1d71f">8.2. EM算法和K-means聚类</a></li>
</ul>
</li>
<li><a href="#org7edb53c">9. <span class="done DONE">DONE</span> 集成学习</a>
<ul>
<li><a href="#orgf3a08c6">9.1. 理论</a>
<ul>
<li><a href="#org7ea7ac5">9.1.1. Boosting方法</a>
<ul>
<li><a href="#org5c02660">9.1.1.1. Adaboost算法</a></li>
<li><a href="#orgbc2a576">9.1.1.2. GBDT(Gradient Boosting Decision Tree)</a></li>
</ul>
</li>
<li><a href="#orgcdff84c">9.1.2. bagging</a></li>
<li><a href="#orgcc81602">9.1.3. 为什么说bagging减少variance，而boosting减少bias</a></li>
</ul>
</li>
<li><a href="#orgf1e69f0">9.2. 相关包学习</a>
<ul>
<li><a href="#org5d6f00a">9.2.1. GBDT</a></li>
<li><a href="#org0381ce3">9.2.2. XGBoost</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org3745fd7">10. <span class="done DONE">DONE</span> HMM、条件随机场、混合高斯</a>
<ul>
<li><a href="#orgf683091">10.1. <span class="done DONE">DONE</span> HMM</a></li>
<li><a href="#orgef5a757">10.2. 条件随机场CRF</a></li>
<li><a href="#org7c04630">10.3. 混合高斯模型GMM</a></li>
</ul>
</li>
<li><a href="#org54eca1a">11. <span class="todo TODO">TODO</span> 支持向量机<code>[4/6]</code></a>
<ul>
<li><a href="#org89000e4">11.1. 前言</a>
<ul>
<li><a href="#org4a1c457">11.1.1. 函数间隔与几何间隔</a></li>
</ul>
</li>
<li><a href="#org6c91a9c">11.2. <span class="done DONE">DONE</span> 线性可分支持向量机与对偶方法</a>
<ul>
<li><a href="#org6150c79">11.2.1. 对偶问题</a></li>
<li><a href="#org047a7f4">11.2.2. KKT条件(Karush-Kuhn-Tucker, 库恩塔克条件)</a></li>
<li><a href="#org369417a">11.2.3. SMO算法(Sequential Minimal Optimization， 序列最小化）</a></li>
</ul>
</li>
<li><a href="#orgdca9974">11.3. <span class="done DONE">DONE</span> 线性不可分支持向量机</a>
<ul>
<li><a href="#org0d0decb">11.3.1. 核函数</a>
<ul>
<li><a href="#org1c8c780">11.3.1.1. 核函数的优势</a></li>
<li><a href="#org2e020fd">11.3.1.2. 哪些通用核函数</a></li>
<li><a href="#org4bf3cdb">11.3.1.3. 如何选择核函数</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgac533f8">11.4. <span class="done DONE">DONE</span> 线性支持向量机(软间隔与正则化)</a>
<ul>
<li><a href="#org1fc9aec">11.4.1. 软间隔</a></li>
<li><a href="#org56f445c">11.4.2. 损失函数</a></li>
<li><a href="#orgc64fcd6">11.4.3. 加入松弛变量</a>
<ul>
<li><a href="#org2b2e3f0">11.4.3.1. KKT条件：</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org008bbbe">11.5. <span class="todo SOMEDAY">SOMEDAY</span> 支持向量回归</a></li>
<li><a href="#org51f8a39">11.6. <span class="todo TODO">TODO</span> 推导过程</a>
<ul>
<li><a href="#org0e7a114">11.6.1. 几何间隔推导出优化方程一般形式</a></li>
<li><a href="#org02dbf52">11.6.2. 拉格朗日对偶问题推导</a></li>
<li><a href="#orge848e6f">11.6.3. KKT条件推导</a></li>
<li><a href="#org4b4ff63">11.6.4. SMO算法推导</a></li>
<li><a href="#org0822bae">11.6.5. 核函数推导</a></li>
<li><a href="#org829b139">11.6.6. 软间隔推导</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgd9b54ee">12. <span class="done DONE">DONE</span> 文本挖掘</a>
<ul>
<li><a href="#orgcadeb03">12.1. word2vec</a>
<ul>
<li><a href="#org6f51d36">12.1.1. 模型</a>
<ul>
<li><a href="#org409a17b">12.1.1.1. CBOW模型</a></li>
<li><a href="#org21395d8">12.1.1.2. Skip-Gram模型</a></li>
<li><a href="#orgd277c35">12.1.1.3. word2vec的改进</a></li>
</ul>
</li>
<li><a href="#orgfb4d211">12.1.2. 超参数与调参</a></li>
</ul>
</li>
<li><a href="#org86e9aab">12.2. doc2vec</a></li>
</ul>
</li>
<li><a href="#org321cb63">13. <span class="done DONE">DONE</span> 聚类算法<code>[3/3]</code></a>
<ul>
<li><a href="#orgfd750a7">13.1. <span class="done DONE">DONE</span> Kmeans</a></li>
<li><a href="#org9e04321">13.2. <span class="done DONE">DONE</span> 层次聚类</a></li>
<li><a href="#orge0732b2">13.3. <span class="done DONE">DONE</span> DBSCAN聚类</a></li>
</ul>
</li>
<li><a href="#org42b745f">14. <span class="done DONE">DONE</span> 数值优化专题</a>
<ul>
<li><a href="#orgfbc2c84">14.1. 预备知识</a>
<ul>
<li><a href="#org5f16995">14.1.1. 损失函数</a>
<ul>
<li><a href="#orge42e261">14.1.1.1. 0-1损失(Binary Loss)</a></li>
<li><a href="#org1d960d8">14.1.1.2. 感知损失（Perceptron Loss）</a></li>
<li><a href="#org6853dca">14.1.1.3. Hinge Loss</a></li>
<li><a href="#orgbd8bd4e">14.1.1.4. 对数损失</a></li>
<li><a href="#orgd0f559b">14.1.1.5. 平方损失</a></li>
<li><a href="#orga5a91f9">14.1.1.6. 绝对损失(Absolute Loss)</a></li>
<li><a href="#org03b877e">14.1.1.7. 指数损失</a></li>
</ul>
</li>
<li><a href="#org66e1a22">14.1.2. 函数几个重要的点</a>
<ul>
<li><a href="#org0e8aa64">14.1.2.1. 拐点</a></li>
<li><a href="#org4e32e38">14.1.2.2. 极值点</a></li>
<li><a href="#org461012d">14.1.2.3. 驻点</a></li>
<li><a href="#org29ba8e5">14.1.2.4. 鞍点（saddle point）</a></li>
</ul>
</li>
<li><a href="#org2871ac2">14.1.3. 梯度和海塞矩阵</a></li>
</ul>
</li>
<li><a href="#orgceedb86">14.2. 优化方法</a>
<ul>
<li><a href="#orgeba8a8e">14.2.1. 优化问题划分：</a>
<ul>
<li><a href="#org9fa40e5">14.2.1.1. 凸优化</a></li>
<li><a href="#org7b952f6">14.2.1.2. 无约束最优化</a></li>
<li><a href="#org226d727">14.2.1.3. 约束最优化</a></li>
<li><a href="#orgb77c61a">14.2.1.4. 局部最优化</a></li>
</ul>
</li>
<li><a href="#org8d55957">14.2.2. 详细的优化方法：</a>
<ul>
<li><a href="#org2a16318">14.2.2.1. 坐标下降</a></li>
<li><a href="#orga6df1e9">14.2.2.2. 梯度下降</a></li>
<li><a href="#org0fab132">14.2.2.3. 随机梯度下降</a></li>
<li><a href="#org82e96ac">14.2.2.4. 动量梯度下降法</a></li>
<li><a href="#org2f82317">14.2.2.5. GradDelta &amp; AdaDelta</a></li>
<li><a href="#org30c091a">14.2.2.6. 共轭梯度法</a></li>
<li><a href="#orgf9a61d6">14.2.2.7. 牛顿法</a></li>
<li><a href="#org8a121d9">14.2.2.8. 拟牛顿法</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orge26dd43">14.3. 程序编写</a></li>
</ul>
</li>
<li><a href="#org05494f8">15. <span class="todo TODO">TODO</span> 调参专题</a>
<ul>
<li><a href="#orgdbdb1d9">15.1. 网格搜索</a></li>
<li><a href="#orgee8b987">15.2. 随机搜索</a></li>
<li><a href="#org0817372">15.3. 基于梯度的优化</a></li>
<li><a href="#org2e3a115">15.4. 坐标下降</a>
<ul>
<li><a href="#orge5dbcda">15.4.1. 调整过程影响类参数</a></li>
<li><a href="#org175b52b">15.4.2. 调整子模型影响类参数</a></li>
</ul>
</li>
<li><a href="#orgf418393">15.5. 贝叶斯优化</a></li>
</ul>
</li>
<li><a href="#orgd78e5e8">16. 神经网络与深度学习</a>
<ul>
<li><a href="#org08f578f">16.1. 神经网络与深度学习简介</a></li>
<li><a href="#org97392e3">16.2. 感知机学习</a></li>
<li><a href="#org060c179">16.3. 一般神经网络</a>
<ul>
<li><a href="#org644a772">16.3.1. 激活函数</a></li>
</ul>
</li>
<li><a href="#orgf99cf82">16.4. BP反向传播网络</a>
<ul>
<li><a href="#orgbd4bc8f">16.4.1. 推导</a>
<ul>
<li><a href="#org582fa47">16.4.1.1. 公式一：输出层误差</a></li>
<li><a href="#org94f169c">16.4.1.2. 公式二：隐含层误差</a></li>
<li><a href="#orgfda8f24">16.4.1.3. 公式三：参数变化率，即w和b的梯度</a></li>
<li><a href="#org8efcf72">16.4.1.4. 公式四：参数更新规则</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org9ace37b">16.5. 卷积神经网络</a>
<ul>
<li><a href="#org6273d02">16.5.1. 卷积</a>
<ul>
<li><a href="#org4a38da4">16.5.1.1. 卷积中的超参数：</a></li>
</ul>
</li>
<li><a href="#org888c59b">16.5.2. 池化(Pooling)</a></li>
<li><a href="#org5a93130">16.5.3. 全连接</a></li>
<li><a href="#org5f8c401">16.5.4. 卷积层参数的确定</a></li>
</ul>
</li>
<li><a href="#org9f67f12">16.6. 循环神经网络(Recurrent Neural Network)</a>
<ul>
<li><a href="#org9db7915">16.6.1. 单向循环神经网络</a></li>
<li><a href="#orgaabb95a">16.6.2. 双向循环神经网络</a></li>
<li><a href="#org326e759">16.6.3. 训练方法：BPTT</a></li>
<li><a href="#orgd32c648">16.6.4. softmax 层</a></li>
<li><a href="#org204d67f">16.6.5. 优缺点</a></li>
<li><a href="#org605cc11">16.6.6. 输入与输出</a></li>
</ul>
</li>
<li><a href="#orga2ea9ef">16.7. 递归神经网络(Recursive Neural Network)&#xa0;&#xa0;&#xa0;<span class="tag"><span class="__">了解</span></span></a></li>
<li><a href="#orgcfd45aa">16.8. LSTM</a></li>
<li><a href="#org199f626">16.9. 实现</a>
<ul>
<li><a href="#org7d568cb">16.9.1. 方案1. win10 tensorflow 安装</a>
<ul>
<li><a href="#org9b411af">16.9.1.1. Cuda &amp; cudnn</a></li>
<li><a href="#org9c4e79b">16.9.1.2. TensorFlow</a></li>
</ul>
</li>
<li><a href="#orgcedb8be">16.9.2. 方案2. 基于theano的Keras win10 安装</a></li>
<li><a href="#orgcd3f193">16.9.3. Keras Tips</a>
<ul>
<li><a href="#org33b6708">16.9.3.1. 自定义 metrics 性能评估函数</a></li>
</ul>
</li>
<li><a href="#orgd86036d">16.9.4. Experiment 1. 信贷违约预测模型</a></li>
<li><a href="#org56da443">16.9.5. Experiment 2. 一个简单的缺失语言自动补全</a></li>
<li><a href="#org92458aa">16.9.6. Experiment 3. 手写数字识别</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgb9090e7">17. 工具</a></li>
<li><a href="#org36ee947">18. 推荐算法 / 推荐系统</a>
<ul>
<li><a href="#org07cd1d4">18.1. 前言</a>
<ul>
<li><a href="#org6ea490e">18.1.1. 隐式反馈和显式反馈</a></li>
</ul>
</li>
<li><a href="#org48091a6">18.2. 推荐算法类别</a>
<ul>
<li><a href="#org328aac2">18.2.1. 基于内容的推荐</a></li>
<li><a href="#orgce7b409">18.2.2. 基于协同过滤的推荐</a>
<ul>
<li><a href="#org2ddd632">18.2.2.1. 基于内存的</a></li>
<li><a href="#orgdd5712c">18.2.2.2. 基于模型的</a></li>
</ul>
</li>
<li><a href="#org9dd4fcf">18.2.3. 基于知识的推荐 / 混合推荐</a></li>
</ul>
</li>
<li><a href="#org8f918c7">18.3. 基于内容的推荐</a></li>
<li><a href="#org15f81a7">18.4. 基于内存的协同过滤推荐</a>
<ul>
<li><a href="#orgcf56311">18.4.1. 基于用户的</a></li>
<li><a href="#orgefd4e96">18.4.2. 基于项目的</a></li>
<li><a href="#org81ea6ad">18.4.3. 存在的问题</a></li>
</ul>
</li>
<li><a href="#org7ab9dcf">18.5. 基于模型的协同过滤推荐</a>
<ul>
<li><a href="#org5300881">18.5.1. 矩阵分解</a>
<ul>
<li><a href="#orgd6c3689">18.5.1.1. 构建目标函数</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org3983cb1">19. 机器学习算法调试</a>
<ul>
<li><a href="#org95252ea">19.1. 梯度检查</a></li>
</ul>
</li>
<li><a href="#orgaae19bd">20. 数据库</a></li>
</ul>
</div>
</div>


<div id="outline-container-org4a73d46" class="outline-2">
<h2 id="org4a73d46"><span class="section-number-2">1</span> <span class="done DONE">DONE</span> 数据预处理</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>State "DONE"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-05-25 五 11:05]</span></span></li>
<li>State "TODO"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2018-05-21 一 09:54]</span></span></li>
</ul>

<p>
<code>这里主要是拿到机器学习比赛数据后的处理方式</code>
</p>



<div class="figure">
<p><img src="pics/data_process.png" alt="data_process.png" />
</p>
</div>
</div>

<div id="outline-container-orgb710db2" class="outline-3">
<h3 id="orgb710db2"><span class="section-number-3">1.1</span> 认识数据</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>观察特征的分布情况（是否正态、是否偏锋、是否厚尾、是否非平衡）</li>
<li>观察特征之间的关系
<ul class="org-ul">
<li>脱敏数据：相关系数矩阵，热力图等</li>
<li>非脱敏数据：通过变量名称推断其关系，并且进行验证</li>
</ul></li>
</ul>

<p>
<code>注意：所有数据预处理的参数均是由训练数据得到，在交叉验证中切记切记</code>
<code>处理方式：通过管道，得到一个完整的分类器（输入+输出），将其带入交叉验证函数中</code>
</p>
</div>
</div>

<div id="outline-container-org1f97897" class="outline-3">
<h3 id="org1f97897"><span class="section-number-3">1.2</span> 变量类型判断</h3>
<div class="outline-text-3" id="text-1-2">
<p>
<code>在一开始即可完成</code>
</p>

<ul class="org-ul">
<li>连续变量</li>
<li>二分类变量</li>
<li>多分类变量
<ul class="org-ul">
<li>名义变量</li>
<li>有序变量</li>
</ul></li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">def</span> <span style="color: #bc6ec5; font-weight: bold;">get_type</span><span style="color: #4f97d7;">(</span>arr<span style="color: #4f97d7;">)</span>:
    <span style="color: #2aa1ae;">"""</span>
<span style="color: #2aa1ae;">    &#33719;&#21462;&#21464;&#37327;&#31867;&#22411;&#25968;&#32452;&#65292;&#21253;&#25324;&#20102;binary&#65292; multi_class&#65292;continuous</span>
<span style="color: #2aa1ae;">    :param arr:   np.ndarray</span>
<span style="color: #2aa1ae;">    :return:      list[LabelType]</span>
<span style="color: #2aa1ae;">    """</span>
    <span style="color: #4f97d7; font-weight: bold;">if</span> <span style="color: #4f97d7;">len</span><span style="color: #4f97d7;">(</span>arr.shape<span style="color: #4f97d7;">)</span> == <span style="color: #a45bad;">2</span>:
        <span style="color: #7590db;">res</span> = <span style="color: #4f97d7;">[]</span>
        <span style="color: #4f97d7; font-weight: bold;">for</span> feature <span style="color: #4f97d7; font-weight: bold;">in</span> arr.T:
            <span style="color: #7590db;">count</span> = np.unique<span style="color: #4f97d7;">(</span><span style="color: #bc6ec5;">[</span>i <span style="color: #4f97d7; font-weight: bold;">for</span> i <span style="color: #4f97d7; font-weight: bold;">in</span> feature <span style="color: #4f97d7; font-weight: bold;">if</span> <span style="color: #4f97d7; font-weight: bold;">not</span> np.isnan<span style="color: #2d9574;">(</span>i<span style="color: #2d9574;">)</span><span style="color: #bc6ec5;">]</span><span style="color: #4f97d7;">)</span>
            <span style="color: #7590db;">is_continuous</span> = <span style="color: #a45bad;">False</span>
            <span style="color: #4f97d7; font-weight: bold;">for</span> i <span style="color: #4f97d7; font-weight: bold;">in</span> count:
                <span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">&#24403;&#23384;&#22312;&#28014;&#28857;&#22411;&#65292;&#19988;&#23567;&#25968;&#28857;&#21518;&#26377;&#25968;&#23383;&#26102;&#65292;&#26159;&#36830;&#32493;&#20540;</span>
                <span style="color: #4f97d7; font-weight: bold;">if</span> <span style="color: #4f97d7;">int</span><span style="color: #4f97d7;">(</span>i<span style="color: #4f97d7;">)</span> != i:
                    <span style="color: #7590db;">is_continuous</span> = <span style="color: #a45bad;">True</span>

            <span style="color: #4f97d7; font-weight: bold;">if</span> np.<span style="color: #4f97d7;">max</span><span style="color: #4f97d7;">(</span>feature<span style="color: #4f97d7;">)</span> - np.<span style="color: #4f97d7;">min</span><span style="color: #4f97d7;">(</span>feature<span style="color: #4f97d7;">)</span> &gt; <span style="color: #a45bad;">10</span>:
                <span style="color: #7590db;">is_continuous</span> = <span style="color: #a45bad;">True</span>
            <span style="color: #4f97d7; font-weight: bold;">if</span> <span style="color: #4f97d7;">len</span><span style="color: #4f97d7;">(</span>count<span style="color: #4f97d7;">)</span> == <span style="color: #a45bad;">2</span>:
                res.append<span style="color: #4f97d7;">(</span>LabelType.binary<span style="color: #4f97d7;">)</span>
            <span style="color: #4f97d7; font-weight: bold;">elif</span> is_continuous:
                res.append<span style="color: #4f97d7;">(</span>LabelType.continuous<span style="color: #4f97d7;">)</span>
            <span style="color: #4f97d7; font-weight: bold;">else</span>:
                res.append<span style="color: #4f97d7;">(</span>LabelType.multi_class<span style="color: #4f97d7;">)</span>
        <span style="color: #4f97d7; font-weight: bold;">return</span> res
    <span style="color: #4f97d7; font-weight: bold;">elif</span> <span style="color: #4f97d7;">len</span><span style="color: #4f97d7;">(</span>arr.shape<span style="color: #4f97d7;">)</span> == <span style="color: #a45bad;">1</span>:
        <span style="color: #7590db;">count</span> = np.unique<span style="color: #4f97d7;">(</span><span style="color: #bc6ec5;">[</span>i <span style="color: #4f97d7; font-weight: bold;">for</span> i <span style="color: #4f97d7; font-weight: bold;">in</span> arr <span style="color: #4f97d7; font-weight: bold;">if</span> <span style="color: #4f97d7; font-weight: bold;">not</span> np.isnan<span style="color: #2d9574;">(</span>i<span style="color: #2d9574;">)</span><span style="color: #bc6ec5;">]</span><span style="color: #4f97d7;">)</span>
        <span style="color: #7590db;">is_continuous</span> = <span style="color: #a45bad;">False</span>
        <span style="color: #4f97d7; font-weight: bold;">for</span> i <span style="color: #4f97d7; font-weight: bold;">in</span> count:
            <span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">&#24403;&#23384;&#22312;&#28014;&#28857;&#22411;&#65292;&#19988;&#23567;&#25968;&#28857;&#21518;&#26377;&#25968;&#23383;&#26102;&#65292;&#26159;&#36830;&#32493;&#20540;</span>
            <span style="color: #4f97d7; font-weight: bold;">if</span> <span style="color: #4f97d7;">int</span><span style="color: #4f97d7;">(</span>i<span style="color: #4f97d7;">)</span> != i:
                <span style="color: #7590db;">is_continuous</span> = <span style="color: #a45bad;">True</span>
                <span style="color: #4f97d7; font-weight: bold;">break</span>

        <span style="color: #4f97d7; font-weight: bold;">if</span> np.<span style="color: #4f97d7;">max</span><span style="color: #4f97d7;">(</span>arr<span style="color: #4f97d7;">)</span> - np.<span style="color: #4f97d7;">min</span><span style="color: #4f97d7;">(</span>arr<span style="color: #4f97d7;">)</span> &gt; <span style="color: #a45bad;">10</span>:
            <span style="color: #7590db;">is_continuous</span> = <span style="color: #a45bad;">True</span>
        <span style="color: #4f97d7; font-weight: bold;">if</span> <span style="color: #4f97d7;">len</span><span style="color: #4f97d7;">(</span>count<span style="color: #4f97d7;">)</span> == <span style="color: #a45bad;">2</span> <span style="color: #4f97d7; font-weight: bold;">or</span> <span style="color: #4f97d7;">len</span><span style="color: #4f97d7;">(</span>count<span style="color: #4f97d7;">)</span> == <span style="color: #a45bad;">1</span>:
            <span style="color: #4f97d7; font-weight: bold;">return</span> LabelType.binary
        <span style="color: #4f97d7; font-weight: bold;">elif</span> is_continuous:
            <span style="color: #4f97d7; font-weight: bold;">return</span> LabelType.continuous
        <span style="color: #4f97d7; font-weight: bold;">else</span>:
            <span style="color: #4f97d7; font-weight: bold;">return</span> LabelType.multi_class

</pre>
</div>
</div>
</div>

<div id="outline-container-org6e3190b" class="outline-3">
<h3 id="org6e3190b"><span class="section-number-3">1.3</span> 异常值、错误值处理</h3>
<div class="outline-text-3" id="text-1-3">
<p>
<code>在一开始即可完成</code>
</p>

<p>
<code>异常值一般只针对连续数据</code>
</p>

<ol class="org-ol">
<li>通过分布图的方式发现异常值</li>
<li>分情况处理：
<ul class="org-ul">
<li>如果分布存在较明显的双峰异常，则添加虚拟变量，作为新的特征</li>
<li>如果只是个别的特大或特小值，则用winsorize变换进行处理或者直接剔除</li>
</ul></li>
</ol>
</div>
</div>

<div id="outline-container-org3c0309c" class="outline-3">
<h3 id="org3c0309c"><span class="section-number-3">1.4</span> 缺失值处理</h3>
<div class="outline-text-3" id="text-1-4">
<p>
<code>每次交叉验证都需要进行</code>
</p>

<p>
ref:
</p>
<ul class="org-ul">
<li><a href="http://blog.sina.com.cn/s/blog_7fb03f7d01012j6p.html">http://blog.sina.com.cn/s/blog_7fb03f7d01012j6p.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22677693">https://zhuanlan.zhihu.com/p/22677693</a></li>
<li><a href="https://blog.csdn.net/lujiandong1/article/details/52654703">https://blog.csdn.net/lujiandong1/article/details/52654703</a></li>
</ul>
</div>


<div id="outline-container-org44a4f18" class="outline-4">
<h4 id="org44a4f18"><span class="section-number-4">1.4.1</span> 缺失机制</h4>
<div class="outline-text-4" id="text-1-4-1">
<ol class="org-ol">
<li>随机缺失(MAR) ： 缺失的可能性与其他完全变量（不含缺失值的变量）有关，比如后续的调查只针对IQ大于100的人，缺失与IQ有关，则是随机缺失</li>
<li>完全随机缺失(MCAR)：缺失不依赖于任何不完全变量或完全变量，比如资料遗失、机器损坏等</li>
<li>非随机缺失(MANR)：数据的缺失依赖于不完全变量自身，比如收入较低的人不愿意填写收入栏</li>
</ol>

<p>
数据是否是完全随机缺失可以采用单变量t检验和Little (1988)提出的多元t检验
</p>

<p>
研究者推荐使用包含辅助变量（Auxiliary Variables，与缺失值相关的因素）的方法减少估计偏差并提高满足MAR假设的可能
</p>
</div>
</div>

<div id="outline-container-org59b1533" class="outline-4">
<h4 id="org59b1533"><span class="section-number-4">1.4.2</span> 脱敏与非脱敏数据</h4>
<div class="outline-text-4" id="text-1-4-2">
<ul class="org-ul">
<li>如果是脱敏数据，则所有缺失情况均可作为随机缺失或完全随机缺失处理</li>
<li>如果是非脱敏数据，则需要通过变量名称进行推断</li>
</ul>
</div>
</div>

<div id="outline-container-org1da2301" class="outline-4">
<h4 id="org1da2301"><span class="section-number-4">1.4.3</span> 不同机制的处理方法</h4>
<div class="outline-text-4" id="text-1-4-3">
<p>
无论什么情况，缺失数据过多时均可考虑舍弃该特征
</p>
</div>

<div id="outline-container-org002cea9" class="outline-5">
<h5 id="org002cea9"><span class="section-number-5">1.4.3.1</span> MAR, MCAR:</h5>
<div class="outline-text-5" id="text-1-4-3-1">
<ul class="org-ul">
<li>行删除： <code>你可以删除包含空值的对象用完整的数据集来进行训练，但预测时你却不能忽略包含空值的对象</code></li>
<li>均值、中位数、众数填补</li>
<li>模型补全
<ul class="org-ul">
<li>回归：连续数据、相对均值更好，缺点：线性，共线性</li>
<li>KNN：针对离散数据，缺点：大数据集下效率低</li>
<li>EM算法：对似然函数进行EM估计，缺点：收敛慢，复杂</li>
<li>多重填补：</li>
</ul></li>
<li>独热编码，将缺失值看成虚拟变量，对离散数据有效，对连续数据建议先离散化后再做</li>
<li>不处理
<ul class="org-ul">
<li>C4.5 将特征A上的缺失样本同时划入子节点中，同时根据子节点的标签类别比例调整样本权重：<a href="https://blog.csdn.net/u012328159/article/details/79413610">https://blog.csdn.net/u012328159/article/details/79413610</a></li>
<li>GBDT(XGBoost)：xgboost把缺失值当做稀疏矩阵来对待，本身的在节点分裂时不考虑的缺失值的数值。缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个。如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树</li>
<li>贝叶斯(网络)：？<a href="https://datascience.stackexchange.com/questions/3711/how-does-the-naive-bayes-classifier-handle-missing-data-in-training?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa">https://datascience.stackexchange.com/questions/3711/how-does-the-naive-bayes-classifier-handle-missing-data-in-training?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa</a></li>
<li>人工神经网络：？</li>
</ul></li>
</ul>
</div>
</div>


<div id="outline-container-orgaf94f6e" class="outline-5">
<h5 id="orgaf94f6e"><span class="section-number-5">1.4.3.2</span> MANR</h5>
<div class="outline-text-5" id="text-1-4-3-2">
<p>
由于缺失情况和该变量本身有关，因此直接将其转为虚拟变量即可：
</p>

<ul class="org-ul">
<li>针对离散变量：“男”、“女”、nan， 通过两个虚拟变量进行替换</li>
<li>针对连续变量：
<ul class="org-ul">
<li>值非常稀疏的变量：年净收入 0~100k、nan，先进行离散化，再转为虚拟变量</li>
<li>值不是非常稀疏的变量：年龄 20~50、nan，直接进行离散化</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgfbdc74f" class="outline-4">
<h4 id="orgfbdc74f"><span class="section-number-4">1.4.4</span> sklearn的方法</h4>
<div class="outline-text-4" id="text-1-4-4">
<p>
Imputer方法可以对数据进行插补，更重要的是， <code>它支持稀疏数据</code>
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">from</span> sklearn.preprossing <span style="color: #4f97d7; font-weight: bold;">import</span> Imputer
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgcde5f76" class="outline-3">
<h3 id="orgcde5f76"><span class="section-number-3">1.5</span> 新变量生成</h3>
<div class="outline-text-3" id="text-1-5">
<p>
<code>在一开始完成即可</code>
</p>
</div>

<div id="outline-container-org6faa9f8" class="outline-4">
<h4 id="org6faa9f8"><span class="section-number-4">1.5.1</span> 根据变量含义生成新变量</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
比如： 根据身份证号码生成性别、居住省份、居住城市、星座（玄学）等
</p>

<p>
<code>需要基于对变量的理解</code>
</p>
</div>
</div>

<div id="outline-container-orgb7920a3" class="outline-4">
<h4 id="orgb7920a3"><span class="section-number-4">1.5.2</span> 数据的特征构造新特征</h4>
<div class="outline-text-4" id="text-1-5-2">
<ul class="org-ul">
<li>缺失值特征</li>
<li>异常值特征</li>
<li>非众数特征</li>
</ul>
</div>
</div>

<div id="outline-container-org2a9dcf4" class="outline-4">
<h4 id="org2a9dcf4"><span class="section-number-4">1.5.3</span> 多项式特征</h4>
<div class="outline-text-4" id="text-1-5-3">
<ul class="org-ul">
<li>直接加入多项式特征，再通过特征选择筛选</li>
<li>适用于关键特征较少的情况</li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">from</span> sklearn.preprocessing <span style="color: #4f97d7; font-weight: bold;">import</span> PolynomialFeatures

<span style="color: #7590db;">poly</span> = PolynomialFeatures<span style="color: #4f97d7;">(</span>degree=<span style="color: #a45bad;">10</span><span style="color: #4f97d7;">)</span>
poly.fit<span style="color: #4f97d7;">(</span>X<span style="color: #4f97d7;">)</span>
<span style="color: #7590db;">X_poly</span> = poly.transform<span style="color: #4f97d7;">(</span>X<span style="color: #4f97d7;">)</span>
</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-orgdbe375f" class="outline-3">
<h3 id="orgdbe375f"><span class="section-number-3">1.6</span> 连续特征标准化与区间缩放</h3>
<div class="outline-text-3" id="text-1-6">
<p>
<code>每次交叉验证都需要进行</code>
</p>

<p>
各种方法的对比： 
<a href="http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py">http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py</a>
</p>
</div>

<div id="outline-container-org35f202e" class="outline-4">
<h4 id="org35f202e"><span class="section-number-4">1.6.1</span> 注意</h4>
<div class="outline-text-4" id="text-1-6-1">
<ul class="org-ul">
<li>线性模型需要用标准化的数据建模,而树类模型不需要标准化的数据</li>
<li>处理标准化的时候,注意将测试集的数据transform到test集上</li>
</ul>
</div>
</div>

<div id="outline-container-org3fbd21d" class="outline-4">
<h4 id="org3fbd21d"><span class="section-number-4">1.6.2</span> 标准化(standardization)</h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
使不同度量的变量具有可比性，同时 <code>不改变原始数据的分布</code> 
</p>

<p>
好处：
</p>
<ul class="org-ul">
<li>使具有可比性</li>
<li>不改变分布</li>
</ul>

\begin{eqnarray}
\nonumber
x' = \frac{x - \bar{x}}{\sigma}
\end{eqnarray}


<p>
<code>注意</code>
什么样的模型需要进行标准化：基于距离的（LR，Kmeans，SVM）
</p>
</div>
</div>

<div id="outline-container-orgfda0703" class="outline-4">
<h4 id="orgfda0703"><span class="section-number-4">1.6.3</span> 区间缩放</h4>
<div class="outline-text-4" id="text-1-6-3">
<p>
使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类圆形，但是 <code>改变了数据的原始分布</code>
</p>

<p>
好处：
</p>
<ul class="org-ul">
<li>提高迭代求解的速度</li>
<li>提高迭代求解的精度</li>
</ul>
</div>


<div id="outline-container-org5563746" class="outline-5">
<h5 id="org5563746"><span class="section-number-5">1.6.3.1</span> Rescaling(归一化)</h5>
<div class="outline-text-5" id="text-1-6-3-1">
\begin{eqnarray}
\nonumber
x' = \frac{x - min(x)}{max(x) - min(x)}
\end{eqnarray}
</div>
</div>

<div id="outline-container-org104f3f3" class="outline-5">
<h5 id="org104f3f3"><span class="section-number-5">1.6.3.2</span> Mean normalization</h5>
<div class="outline-text-5" id="text-1-6-3-2">
\begin{eqnarray}
\nonumber
x' = \frac{x - mean(x)}{max(x) - min(x)}
\end{eqnarray}
</div>
</div>

<div id="outline-container-org9dec4f2" class="outline-5">
<h5 id="org9dec4f2"><span class="section-number-5">1.6.3.3</span> Scaling to unit length</h5>
<div class="outline-text-5" id="text-1-6-3-3">
\begin{eqnarray}
\nonumber
x' = \frac{x}{||x||}
\end{eqnarray}
</div>
</div>
</div>



<div id="outline-container-org2e8cb7b" class="outline-4">
<h4 id="org2e8cb7b"><span class="section-number-4">1.6.4</span> 特殊情况：稀疏点的区间缩放</h4>
<div class="outline-text-4" id="text-1-6-4">
<p>
常规的Scale容易破坏数据的稀疏性结构，而MaxAbsScaler不会shift or center the data，因此不会改变稀疏结构
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">import</span> sklearn.preprossing.MaxAbsScaler
</pre>
</div>
</div>
</div>


<div id="outline-container-org646405b" class="outline-4">
<h4 id="org646405b"><span class="section-number-4">1.6.5</span> 特殊情况：离群点的区间缩放</h4>
<div class="outline-text-4" id="text-1-6-5">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">import</span> sklearn.preprocessing.RobustScaler 
</pre>
</div>

<p>
通过分位数进行区间缩放，因此更加稳健
</p>
</div>
</div>
</div>

<div id="outline-container-org4605408" class="outline-3">
<h3 id="org4605408"><span class="section-number-3">1.7</span> 离散特征 独热编码</h3>
<div class="outline-text-3" id="text-1-7">
<p>
<code>在一开始完成即可</code>
</p>

<p>
<code>注意：如果某虚拟变量在测试集中存在而训练集中不存在，怎么办？</code>
</p>
</div>

<div id="outline-container-org4a753bd" class="outline-4">
<h4 id="org4a753bd"><span class="section-number-4">1.7.1</span> 方法一</h4>
<div class="outline-text-4" id="text-1-7-1">
<ul class="org-ul">
<li>如果在train中存在而test中不存在： 为test中添加0列</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">Get missing columns in the training test</span>
<span style="color: #7590db;">missing_cols</span> = <span style="color: #4f97d7;">set</span><span style="color: #4f97d7;">(</span> train.columns <span style="color: #4f97d7;">)</span> - <span style="color: #4f97d7;">set</span><span style="color: #4f97d7;">(</span> test.columns <span style="color: #4f97d7;">)</span>
<span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">Add a missing column in test set with default value equal to 0</span>
<span style="color: #4f97d7; font-weight: bold;">for</span> c <span style="color: #4f97d7; font-weight: bold;">in</span> missing_cols:
    <span style="color: #7590db;">test</span><span style="color: #4f97d7;">[</span>c<span style="color: #4f97d7;">]</span> = <span style="color: #a45bad;">0</span>
<span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">Ensure the order of column in the test set is in the same order than in train set</span>
<span style="color: #7590db;">test</span> = test<span style="color: #4f97d7;">[</span>train.columns<span style="color: #4f97d7;">]</span>
</pre>
</div>

<ul class="org-ul">
<li>如果在test中存在而在train中不存在：剔除test中的对应列</li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">Get missing columns in the training test</span>
<span style="color: #7590db;">missing_cols</span> = <span style="color: #4f97d7;">set</span><span style="color: #4f97d7;">(</span> test.columns <span style="color: #4f97d7;">)</span> - <span style="color: #4f97d7;">set</span><span style="color: #4f97d7;">(</span> train.columns <span style="color: #4f97d7;">)</span>
<span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">Add a missing column in test set with default value equal to 0</span>
<span style="color: #4f97d7; font-weight: bold;">for</span> c <span style="color: #4f97d7; font-weight: bold;">in</span> missing_cols:
    <span style="color: #4f97d7; font-weight: bold;">del</span> test<span style="color: #4f97d7;">[</span>c<span style="color: #4f97d7;">]</span>
</pre>
</div>

<ul class="org-ul">
<li>如果上述两种情况交杂，则各自去差集处理</li>
</ul>
</div>
</div>

<div id="outline-container-org76a079d" class="outline-4">
<h4 id="org76a079d"><span class="section-number-4">1.7.2</span> 方法二：</h4>
<div class="outline-text-4" id="text-1-7-2">
<p>
在独热编码时就同时加入训练集和测试集
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">import</span> pandas <span style="color: #4f97d7; font-weight: bold;">as</span> pd

<span style="color: #7590db;">train_objs_num</span> = <span style="color: #4f97d7;">len</span><span style="color: #4f97d7;">(</span>train<span style="color: #4f97d7;">)</span>
<span style="color: #7590db;">dataset</span> = pd.concat<span style="color: #4f97d7;">(</span>objs=<span style="color: #bc6ec5;">[</span>train, test<span style="color: #bc6ec5;">]</span>, axis=<span style="color: #a45bad;">0</span><span style="color: #4f97d7;">)</span>
<span style="color: #7590db;">dataset_preprocessed</span> = pd.get_dummies<span style="color: #4f97d7;">(</span>dataset<span style="color: #4f97d7;">)</span>
<span style="color: #7590db;">train_preprocessed</span> = dataset_preprocessed<span style="color: #4f97d7;">[</span>:train_objs_num<span style="color: #4f97d7;">]</span>
<span style="color: #7590db;">test_preprocessed</span> = dataset_preprocessed<span style="color: #4f97d7;">[</span>train_objs_num:<span style="color: #4f97d7;">]</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org5b94866" class="outline-4">
<h4 id="org5b94866"><span class="section-number-4">1.7.3</span> 方法三(最好)</h4>
<div class="outline-text-4" id="text-1-7-3">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">import</span> pandas <span style="color: #4f97d7; font-weight: bold;">as</span> pd
<span style="color: #7590db;">train</span> = pd.DataFrame<span style="color: #4f97d7;">(</span>data = <span style="color: #bc6ec5;">[</span><span style="color: #2d9574;">[</span><span style="color: #2d9574;">'a'</span>, <span style="color: #a45bad;">123</span>, <span style="color: #2d9574;">'ab'</span><span style="color: #2d9574;">]</span>, <span style="color: #2d9574;">[</span><span style="color: #2d9574;">'b'</span>, <span style="color: #a45bad;">234</span>, <span style="color: #2d9574;">'bc'</span><span style="color: #2d9574;">]</span><span style="color: #bc6ec5;">]</span>,
                     columns=<span style="color: #bc6ec5;">[</span><span style="color: #2d9574;">'col1'</span>, <span style="color: #2d9574;">'col2'</span>, <span style="color: #2d9574;">'col3'</span><span style="color: #bc6ec5;">]</span><span style="color: #4f97d7;">)</span>
<span style="color: #7590db;">test</span> = pd.DataFrame<span style="color: #4f97d7;">(</span>data = <span style="color: #bc6ec5;">[</span><span style="color: #2d9574;">[</span><span style="color: #2d9574;">'c'</span>, <span style="color: #a45bad;">345</span>, <span style="color: #2d9574;">'ab'</span><span style="color: #2d9574;">]</span>, <span style="color: #2d9574;">[</span><span style="color: #2d9574;">'b'</span>, <span style="color: #a45bad;">456</span>, <span style="color: #2d9574;">'ab'</span><span style="color: #2d9574;">]</span><span style="color: #bc6ec5;">]</span>,
                     columns=<span style="color: #bc6ec5;">[</span><span style="color: #2d9574;">'col1'</span>, <span style="color: #2d9574;">'col2'</span>, <span style="color: #2d9574;">'col3'</span><span style="color: #bc6ec5;">]</span><span style="color: #4f97d7;">)</span>

<span style="color: #7590db;">train1</span> = pd.get_dummies<span style="color: #4f97d7;">(</span>train<span style="color: #4f97d7;">)</span>
<span style="color: #7590db;">test1</span> = pd.get_dummies<span style="color: #4f97d7;">(</span>test<span style="color: #4f97d7;">)</span>  

<span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">&#29983;&#25104;&#21015;&#21517;&#19968;&#33268;&#30340;&#20004;&#24352;&#34920;&#65292;&#22806;&#36830;&#25509;&#65292;&#32570;&#22833;&#20540;&#29992;0&#22635;&#20805;</span>
<span style="color: #7590db;">new_train</span>, <span style="color: #7590db;">new_test</span> = train1.align<span style="color: #4f97d7;">(</span>test1,join=<span style="color: #2d9574;">'outer'</span>, axis=<span style="color: #a45bad;">1</span>, fill_value=<span style="color: #a45bad;">0</span><span style="color: #4f97d7;">)</span>
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org8891964" class="outline-3">
<h3 id="org8891964"><span class="section-number-3">1.8</span> 数据不平衡方法</h3>
<div class="outline-text-3" id="text-1-8">
</div>
<div id="outline-container-org225f7d5" class="outline-4">
<h4 id="org225f7d5"><span class="section-number-4">1.8.1</span> 简介</h4>
<div class="outline-text-4" id="text-1-8-1">
</div>
<div id="outline-container-orgb20aef3" class="outline-5">
<h5 id="orgb20aef3"><span class="section-number-5">1.8.1.1</span> 什么是数据不平衡问题(imbalance dataset)</h5>
<div class="outline-text-5" id="text-1-8-1-1">
<p>
样本标签，或者说是预测目标，取值不平衡，比如为0的非常多，为1的非常少，导致分类器容易将所有样本均预测为0，带来的准确率却很高
</p>
</div>
</div>

<div id="outline-container-org9c4c47d" class="outline-5">
<h5 id="org9c4c47d"><span class="section-number-5">1.8.1.2</span> 为什么类不平衡是不好的</h5>
<div class="outline-text-5" id="text-1-8-1-2">
<ul class="org-ul">
<li>从模型的训练过程来看：少量样本提供的信息过少，是的训练容易受误差干扰</li>
<li>从模型的预测过程来看： <b>当预测几率大于观测几率时</b> ，样本被判为正类，比如先验的观测几率是0.5，而少量样本容易扭曲观测几率</li>
</ul>
</div>
</div>

<div id="outline-container-orgf776bee" class="outline-5">
<h5 id="orgf776bee"><span class="section-number-5">1.8.1.3</span> 什么样的模型需要处理非平衡数据:</h5>
<div class="outline-text-5" id="text-1-8-1-3">
<ul class="org-ul">
<li>基于贝叶斯的模型无需处理</li>
<li>基于树的模型一定要处理</li>
<li>不包含样本先验的模型(logistic, knn)要处理</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgfd41752" class="outline-4">
<h4 id="orgfd41752"><span class="section-number-4">1.8.2</span> 文献与方法综述</h4>
<div class="outline-text-4" id="text-1-8-2">
<p>
最详细的资料：<a href="https://pypi.python.org/pypi/imbalanced-learn#id31">https://pypi.python.org/pypi/imbalanced-learn#id31</a>
以及用户手册：<a href="http://contrib.scikit-learn.org/imbalanced-learn/stable/">http://contrib.scikit-learn.org/imbalanced-learn/stable/</a>
</p>
</div>
</div>

<div id="outline-container-org6c6cb3f" class="outline-4">
<h4 id="org6c6cb3f"><span class="section-number-4">1.8.3</span> 抽样方法</h4>
<div class="outline-text-4" id="text-1-8-3">
</div>
<div id="outline-container-org343179a" class="outline-5">
<h5 id="org343179a"><span class="section-number-5">1.8.3.1</span> Under Sampling (欠采样法、向下采样法)，减少多数类样本</h5>
<div class="outline-text-5" id="text-1-8-3-1">
</div>
<ol class="org-ol">
<li><a id="org4652c5d"></a>Edited Nearest Neighbor (ENN)<br />
<div class="outline-text-6" id="text-1-8-3-1-1">
<p>
对每个多数类的样本， 如果他的大部分K近邻样本是少数类，那么将该点删去
</p>
</div>
</li>

<li><a id="org311ca31"></a>Repeated Edited Nearest Neighbor<br />
<div class="outline-text-6" id="text-1-8-3-1-2">
<p>
重复ENN直至样本不发生改变
</p>
</div>
</li>

<li><a id="org815e16d"></a>Tomek Link Removal<br />
<div class="outline-text-6" id="text-1-8-3-1-3">
<p>
<b>REF</b> : "Two Modifications of CNN", 1976
如果样本点A和样本点B的最近邻（即K=1近邻）都是对方，且A与B分别属于少数类与多数类，则将该点删去
</p>
</div>
</li>

<li><a id="orgf5f830d"></a>Ensemble 模型融合法<br />
<div class="outline-text-6" id="text-1-8-3-1-4">
<p>
<b>REF</b> : "Exploratory undersampling for class-imbalance learning", 2009
</p>

<p>
<b>思想</b> :
    多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果
</p>
</div>
</li>

<li><a id="orgca6abac"></a>BalanceCascade 增强训练法<br />
<div class="outline-text-6" id="text-1-8-3-1-5">
<p>
<b>REF</b> : "Exploratory undersampling for class-imbalance learning", 2009
</p>

<p>
<b>思想</b> :
    先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgd66c5bb" class="outline-5">
<h5 id="orgd66c5bb"><span class="section-number-5">1.8.3.2</span> Over Sampling (过采样法，向上采样法），增加少数类样本</h5>
<div class="outline-text-5" id="text-1-8-3-2">
</div>
<ol class="org-ol">
<li><a id="org6d82276"></a>随机有放回抽少数类样本到总样本中，并加入随机扰动<br /></li>
</ol>
</div>
</div>

<div id="outline-container-org58d68ca" class="outline-4">
<h4 id="org58d68ca"><span class="section-number-4">1.8.4</span> 合成方法</h4>
<div class="outline-text-4" id="text-1-8-4">
</div>
<div id="outline-container-org2e64bac" class="outline-5">
<h5 id="org2e64bac"><span class="section-number-5">1.8.4.1</span> SMOTE (Synthetic Minority Oversampling TEchnique)</h5>
<div class="outline-text-5" id="text-1-8-4-1">
<p>
<b>REF</b> : 
</p>
<ul class="org-ul">
<li>"DATA MINING FOR IMBALANCED DATASETS:AN OVERVIEW"</li>
<li>"SMOTE: Synthetic Minority Over-sampling Technique", 2002</li>
</ul>

<p>
<b>步骤</b> ：
</p>
<ol class="org-ol">
<li>对于少数类样本集{x<sub>i</sub>, i=1,&#x2026;,n}，找到每个x<sub>i的K个同为少数类的近邻样本点</sub></li>
<li>对每个x<sub>i</sub>，随机抽取K个近邻点中的一个，记为x<sub>i</sub><sup>(k)</sup></li>
<li>生成新的样本点 x<sub>i,1</sub> = x<sub>i</sub> + &eta;<sub>1</sub> · (x<sub>i</sub><sup>(k)</sup> - x<sub>i</sub>)，其中&eta;<sub>1位0</sub>-1之间的随机数</li>
<li>将步骤3执行N次，最终得到N倍于原少数类样本的点</li>
</ol>

<p>
<b>改进</b> ：
    该方法的缺点是，增加了类之间重叠的可能性，并且有可能生成一些无意义的样本，因此有如下改进方法
</p>
<ul class="org-ul">
<li>Borderline-SMOTE</li>
<li>ADASYN</li>
</ul>
</div>
</div>

<div id="outline-container-org9889f53" class="outline-5">
<h5 id="org9889f53"><span class="section-number-5">1.8.4.2</span> Borderline-SMOTE</h5>
<div class="outline-text-5" id="text-1-8-4-2">
<p>
<b>主要思想</b> ：
</p>
<ul class="org-ul">
<li>如果少数类样本点附近全是多数类的点，那么改点很明显为噪声，不做处理或者是剔除</li>
<li>如果少数类样本点附近有较多的多数类样本，那么说明该样本刚好处于分类的边界，具有较大的信息。</li>
<li>如果少数类样本点附近有较少的多数类样本点，那么该样本点很安全，如果强行合成新样本点，则会模糊分类的边界，因此不做处理</li>
</ul>

<p>
实际操作中，如果K/2以上的K近邻点都为多数类，那么就进行合成，否则不合成
</p>
</div>
</div>

<div id="outline-container-org1666cb0" class="outline-5">
<h5 id="org1666cb0"><span class="section-number-5">1.8.4.3</span> SMOTE + ENN</h5>
<div class="outline-text-5" id="text-1-8-4-3">
<p>
<b>REF</b> : "A study of the behavior of several methods for balancing machine learning training data" Batista et al 2004
</p>
</div>
</div>

<div id="outline-container-org879619e" class="outline-5">
<h5 id="org879619e"><span class="section-number-5">1.8.4.4</span> SMOTE + Tomek</h5>
<div class="outline-text-5" id="text-1-8-4-4">
<p>
<b>REF</b> : "A study of the behavior of several methods for balancing machine learning training data" Batista et al 2004
</p>
</div>
</div>

<div id="outline-container-org334ed2f" class="outline-5">
<h5 id="org334ed2f"><span class="section-number-5">1.8.4.5</span> SMOTEBoost</h5>
<div class="outline-text-5" id="text-1-8-4-5">
<p>
<b>REF</b> : "SMOTEBoost: Improving Prediction of the Minority Class in Boosting", 2003
</p>

<p>
结合了SMOTE和AdaBoost算法，不断更新样本的分布
</p>
</div>
</div>
</div>
<div id="outline-container-org3dee17d" class="outline-4">
<h4 id="org3dee17d"><span class="section-number-4">1.8.5</span> 加权方法</h4>
<div class="outline-text-4" id="text-1-8-5">
<p>
给与不同错误损失不同的权重，视情况而定
</p>
</div>
</div>

<div id="outline-container-org94ba0dd" class="outline-4">
<h4 id="org94ba0dd"><span class="section-number-4">1.8.6</span> 一分类方法</h4>
<div class="outline-text-4" id="text-1-8-6">
<p>
当正负样本相差特别悬殊时，把他看成一分类或者是异常检测问题，此时重点不在于捕捉类间的差别，而是为其中一类进行建模，经典的工作包括One-class SVM等。
</p>
</div>
</div>

<div id="outline-container-org578148f" class="outline-4">
<h4 id="org578148f"><span class="section-number-4">1.8.7</span> 方法选择</h4>
<div class="outline-text-4" id="text-1-8-7">
<p>
来自博客：<a href="http://blog.csdn.net/lujiandong1/article/details/52658675">http://blog.csdn.net/lujiandong1/article/details/52658675</a>
</p>

<ol class="org-ol">
<li>在正负样本都非常之少的情况下，应该采用数据合成的方式；</li>
<li>在负样本足够多，正样本非常之少且比例及其悬殊的情况下，应该考虑一分类方法；</li>
<li>在正负样本都足够多且比例不是特别悬殊的情况下，应该考虑采样或者加权的方法。</li>
<li>采样和加权在数学上是等价的，但实际应用中效果却有差别。尤其是采样了诸如Random Forest等分类方法，训练过程会对训练集进行随机采样。在这种情况下，如果计算资源允许上采样往往要比加权好一些。</li>
<li>另外，虽然上采样和下采样都可以使数据集变得平衡，并且在数据足够多的情况下等价，但两者也是有区别的。实际应用中，我的经验是如果计算资源足够且小众类样本足够多的情况下使用上采样，否则使用下采样，因为上采样会增加训练集的大小进而增加训练时间，同时小的训练集非常容易产生过拟合。</li>
<li>对于下采样，如果计算资源相对较多且有良好的并行环境，应该选择Ensemble方法。</li>
</ol>
</div>
</div>

<div id="outline-container-org224b089" class="outline-4">
<h4 id="org224b089"><span class="section-number-4">1.8.8</span> 实现</h4>
<div class="outline-text-4" id="text-1-8-8">
<p>
python imbalance-learn 包
</p>
</div>
</div>
</div>


<div id="outline-container-org3b00182" class="outline-3">
<h3 id="org3b00182"><span class="section-number-3">1.9</span> 管道pipline</h3>
<div class="outline-text-3" id="text-1-9">
</div>
<div id="outline-container-orgccb79a8" class="outline-4">
<h4 id="orgccb79a8"><span class="section-number-4">1.9.1</span> FunctionTransformer自定义一个转化器,并且可以在Pipeline中使用</h4>
<div class="outline-text-4" id="text-1-9-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">import</span> numpy <span style="color: #4f97d7; font-weight: bold;">as</span> np
<span style="color: #4f97d7; font-weight: bold;">from</span> sklearn.preprocessing <span style="color: #4f97d7; font-weight: bold;">import</span> FunctionTransformer
<span style="color: #7590db;">transformer</span> = FunctionTransformer<span style="color: #4f97d7;">(</span>np.log1p<span style="color: #4f97d7;">)</span><span style="color: #2aa1ae; background-color: #292e34;">#</span><span style="color: #2aa1ae; background-color: #292e34;">&#25324;&#21495;&#20869;&#30340;&#23601;&#26159;&#33258;&#23450;&#20041;&#20989;&#25968;</span>
<span style="color: #7590db;">X</span> = np.array<span style="color: #4f97d7;">(</span><span style="color: #bc6ec5;">[</span><span style="color: #2d9574;">[</span><span style="color: #a45bad;">0</span>, <span style="color: #a45bad;">1</span><span style="color: #2d9574;">]</span>, <span style="color: #2d9574;">[</span><span style="color: #a45bad;">2</span>, <span style="color: #a45bad;">3</span><span style="color: #2d9574;">]</span><span style="color: #bc6ec5;">]</span><span style="color: #4f97d7;">)</span>
transformer.transform<span style="color: #4f97d7;">(</span>X<span style="color: #4f97d7;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgdceaca6" class="outline-4">
<h4 id="orgdceaca6"><span class="section-number-4">1.9.2</span> 运行流程</h4>
<div class="outline-text-4" id="text-1-9-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">from</span> sklearn.pipeline <span style="color: #4f97d7; font-weight: bold;">import</span> Pipeline

<span style="color: #7590db;">pipe_lr</span> = Pipeline<span style="color: #4f97d7;">(</span><span style="color: #bc6ec5;">[</span><span style="color: #2d9574;">(</span><span style="color: #2d9574;">'sc'</span>, StandardScaler<span style="color: #67b11d;">()</span><span style="color: #2d9574;">)</span>,
                    <span style="color: #2d9574;">(</span><span style="color: #2d9574;">'pca'</span>, PCA<span style="color: #67b11d;">(</span>n_components=<span style="color: #a45bad;">2</span><span style="color: #67b11d;">)</span><span style="color: #2d9574;">)</span>,
                    <span style="color: #2d9574;">(</span><span style="color: #2d9574;">'clf'</span>, LogisticRegression<span style="color: #67b11d;">(</span>random_state=<span style="color: #a45bad;">1</span><span style="color: #67b11d;">)</span><span style="color: #2d9574;">)</span>
                    <span style="color: #bc6ec5;">]</span><span style="color: #4f97d7;">)</span>
pipe_lr.fit<span style="color: #4f97d7;">(</span>X_train, y_train<span style="color: #4f97d7;">)</span>
<span style="color: #4f97d7; font-weight: bold;">print</span><span style="color: #4f97d7;">(</span><span style="color: #2d9574;">'Test accuracy: %.3f'</span> % pipe_lr.score<span style="color: #bc6ec5;">(</span>X_test, y_test<span style="color: #bc6ec5;">)</span><span style="color: #4f97d7;">)</span>
</pre>
</div>

<p>
之前的每一个管道方法必须继承 <code>fit</code> 和 <code>transform</code> 的属性，最后一层方法继承 <code>fit</code> <code>score</code> <code>predict</code>
</p>


<div class="figure">
<p><img src="pics/pipline.png" alt="pipline.png" />
</p>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org11189f4" class="outline-2">
<h2 id="org11189f4"><span class="section-number-2">2</span> <span class="done DONE">DONE</span> 特征预处理(特征工程)</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org531fbfc" class="outline-3">
<h3 id="org531fbfc"><span class="section-number-3">2.1</span> <span class="done DONE">DONE</span> 特征选择</h3>
<div class="outline-text-3" id="text-2-1">
<p>
<code>在一开始完成</code>
</p>
</div>

<div id="outline-container-org087327d" class="outline-4">
<h4 id="org087327d"><span class="section-number-4">2.1.1</span> Filter</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
自变量和目标变量之间的关联
</p>

<p>
<code>使用前提</code>
</p>
<ul class="org-ul">
<li>自变量只有两种情况：连续 或 0-1</li>
<li>因变量有三种情况：连续  0-1 或 多分类</li>
</ul>

<p>
<code>注意</code>
Filter方法通常需要给定 <code>K(需要的特征数目)</code> 
</p>
</div>

<div id="outline-container-org8065957" class="outline-5">
<h5 id="org8065957"><span class="section-number-5">2.1.1.1</span> 方差法</h5>
<div class="outline-text-5" id="text-2-1-1-1">
<p>
<code>适用于：</code>
</p>
<ul class="org-ul">
<li>任何自变量</li>
<li>任何因变量</li>
</ul>


<p>
<code>思路：</code>
变量方差越大则反应的信息越大
</p>
</div>
</div>

<div id="outline-container-org8ebe8ee" class="outline-5">
<h5 id="org8ebe8ee"><span class="section-number-5">2.1.1.2</span> 相关系数</h5>
<div class="outline-text-5" id="text-2-1-1-2">
<p>
<code>适用于：</code>
</p>
<ul class="org-ul">
<li>任何自变量</li>
<li>连续或 0-1 因变量</li>
</ul>

<p>
<code>思想：</code>
如果特征和标签相关性较大，则认为特征更有效
</p>

<p>
<code>不同情况的计算方法</code>
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">特征</th>
<th scope="col" class="org-left">标签</th>
<th scope="col" class="org-left">方法</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">连续</td>
<td class="org-left">连续</td>
<td class="org-left">pearson相关系数、spearman秩相关系数、Kendall相关系数</td>
</tr>

<tr>
<td class="org-left">连续</td>
<td class="org-left">离散</td>
<td class="org-left">离散转为二值，计算相关系数，如果是多值，则计算多个相关系数取平均</td>
</tr>

<tr>
<td class="org-left">离散</td>
<td class="org-left">连续</td>
<td class="org-left">同上</td>
</tr>

<tr>
<td class="org-left">离散</td>
<td class="org-left">离散</td>
<td class="org-left">jarcard距离</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org05001c6" class="outline-5">
<h5 id="org05001c6"><span class="section-number-5">2.1.1.3</span> 卡方检验</h5>
<div class="outline-text-5" id="text-2-1-1-3">
<p>
<code>适用:</code>
</p>
<ul class="org-ul">
<li>分类变量</li>
<li>0-1或多分类因变量</li>
</ul>

<p>
<code>卡方检验被用来:</code>
</p>
<ol class="org-ol">
<li>检验连续变量的分布是否与某种理论分布相一致</li>
<li>检验某个分类变量各类出现的概率是否等于指定概率</li>
<li><code>检验两个分类变量是否两两独立</code></li>
</ol>

<p>
<code>做法:</code>
</p>
<ul class="org-ul">
<li>原始数据中得到实际值</li>
<li>根据实际值,假设两变量独立,计算得到理论值</li>
<li>卡方检验</li>
</ul>

<p>
例:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-right">抽烟</td>
<td class="org-right">不抽烟</td>
</tr>

<tr>
<td class="org-left">肺癌</td>
<td class="org-right">10</td>
<td class="org-right">20</td>
</tr>

<tr>
<td class="org-left">未肺癌</td>
<td class="org-right">90</td>
<td class="org-right">80</td>
</tr>
</tbody>
</table>

<p>
以上是实际数据中得到的数据,那么理论数据则是假设抽烟和肺癌独立,求出每一个类别下的联合概率,再乘以总人数N=200,得到期望数E
</p>

<p>
P(抽烟,肺癌) = P(抽烟) * P(肺癌) = (10 + 90)*(10+20) / 200<sup>2</sup>
E(抽烟,肺癌) = P(抽烟,肺癌) * 200
</p>

<p>
最后带入公式,
</p>

\begin{eqnarray}
\nonumber
\chi ^2  = \sum \frac{(N(.,.) - E(.,.))^2}{E(.,.)}
\end{eqnarray}

<p>
进行卡方检验,自由度为 (列数-1)*(行数-1)
</p>
</div>
</div>

<div id="outline-container-org28e905c" class="outline-5">
<h5 id="org28e905c"><span class="section-number-5">2.1.1.4</span> 互信息</h5>
<div class="outline-text-5" id="text-2-1-1-4">
<ul class="org-ul">
<li>任何自变量</li>
<li>任何因变量（最大信息系数法用以处理定量数据）</li>
</ul>


<div class="figure">
<p><img src="pics/entropy.jpg" alt="entropy.jpg" />
</p>
</div>
</div>

<ol class="org-ol">
<li><a id="orgc560d8c"></a>信息熵<br />
<div class="outline-text-6" id="text-2-1-1-4-1">
<p>
代表了一个分布的信息量,或者编码的平均长度
</p>

\begin{eqnarray}
\nonumber
H(p) = \sum_x p(x) log_2 \frac{1}{p(x)} = -\sum_x p(x) log_2 p(x) \in \ [0, log_2 \frac{1}{n}]
\end{eqnarray}

<p>
其中n为类别数目, <code>信息熵越大,信息越混乱,越无序</code>
</p>

<ul class="org-ul">
<li>log<sub>2</sub> 比特</li>
<li>log<sub>10</sub> 哈特</li>
<li>log<sub>e</sub> 奈特</li>
</ul>

<p>
如何转化: 换底公式 \(log_a b = \frac{log_c b}{log_c a}\)
</p>
</div>
</li>


<li><a id="org378193b"></a>交叉熵<br />
<div class="outline-text-6" id="text-2-1-1-4-2">
<p>
用一个猜测的分布编码方式去编码一个真实的分布,可以用来判断分布的差异
</p>

\begin{eqnarray}
\nonumber
H_p(q) = \sum_x q(x) log \frac{1}{p(x)}
\end{eqnarray}

<p>
<code>交叉熵损失与机器学习</code>
Logistic回归的损失函数即为 交叉熵损失,优点:是凸的
</p>
</div>
</li>

<li><a id="org5bdb61f"></a>KL散度<br />
<div class="outline-text-6" id="text-2-1-1-4-3">
<p>
衡量两个分布之间的距离
</p>

\begin{eqnarray}
\nonumber
D_q(p) = H_q(p) - H(p) = \sum_x q(x) log_2 (\frac{q(x)}{p(x)})
\end{eqnarray}

<p>
<code>非负性</code>
证明: <a href="https://blog.csdn.net/haolexiao/article/details/70142571">https://blog.csdn.net/haolexiao/article/details/70142571</a>
</p>
</div>
</li>

<li><a id="orgd3fd2b1"></a>联合熵<br />
<div class="outline-text-6" id="text-2-1-1-4-4">
\begin{eqnarray}
\nonumber
H(X, Y) = \sum_{x,y} p(x,y) log_2 \frac{1}{p(x, y)}
\end{eqnarray}
</div>
</li>

<li><a id="org02d64ae"></a>条件熵<br />
<div class="outline-text-6" id="text-2-1-1-4-5">
\begin{eqnarray}
\nonumber

\end{eqnarray}
\begin{eqnarray}
\nonumber
H(X|Y) = \sum_{y} p(y) \sum_{x} p(x|y) log_2 \frac{1}{p(x| y)} = \sum_{x, y} p(x, y) log_2 \frac{1}{p(x|y)}
\end{eqnarray}

<p>
<code>联合熵和条件熵的关系</code>
当已知X时, Y的不确定性减少了,H(X,Y)剩余的信息量就是条件熵:
</p>

\begin{eqnarray}
\nonumber
H(Y|X) = H(X, Y) - H(X)
\end{eqnarray}

<p>
如果X和Y独立,则:
</p>
\begin{eqnarray}
\nonumber
H(Y|X) = H(Y) = H(X, Y) - H(X)
\end{eqnarray}
</div>
</li>


<li><a id="org4e1beb0"></a>互信息(信息增益)<br />
<div class="outline-text-6" id="text-2-1-1-4-6">
<p>
互信息就是一个联合分布中的两个信息的纠缠程度/或者叫相互影响那部分的信息量
</p>

\begin{eqnarray}
\nonumber
I(X, Y) = H(X) + H(Y) - H(X, Y) = H(Y) - H(Y|X)
\end{eqnarray}

<ul class="org-ul">
<li>如果X,Y独立,则互信息为0</li>
<li>互信息越大说明两者关系越强</li>
</ul>

<p>
<code>非负性</code> 证明见:<a href="https://blog.csdn.net/haolexiao/article/details/70142571">https://blog.csdn.net/haolexiao/article/details/70142571</a>
</p>
</div>
</li>
</ol>
</div>
</div>



<div id="outline-container-orgc485a5f" class="outline-4">
<h4 id="orgc485a5f"><span class="section-number-4">2.1.2</span> Wrapper</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
<code>通过目标函数来决定是否加入一个变量</code>
<code>无需给定K(需要的特征数目)</code>
</p>


<p>
不断地迭代，产生特征子集，评价：
</p>
<ul class="org-ul">
<li>完全搜索</li>
<li>随机搜索（模拟退火、粒子群、遗传算法）</li>
<li>启发式搜索
<ul class="org-ul">
<li>爬山法: 不断的添加能使效果变好的特征,直到不能更好为止 (前向贪心算法)</li>
<li>后向贪心算法:不断减少特征直至最好</li>
<li>逐步回归法:前后向均有</li>
</ul></li>
</ul>
</div>
</div>


<div id="outline-container-orge53dc0e" class="outline-4">
<h4 id="orge53dc0e"><span class="section-number-4">2.1.3</span> Embedded</h4>
<div class="outline-text-4" id="text-2-1-3">
</div>
<div id="outline-container-org5ceb524" class="outline-5">
<h5 id="org5ceb524"><span class="section-number-5">2.1.3.1</span> 基于正则项的特征选择</h5>
<div class="outline-text-5" id="text-2-1-3-1">
<p>
<code>为什么正则项(Regularization)可以防止过拟合？</code>
</p>

<p>
ref: <a href="http://blog.csdn.net/jackie_zhu/article/details/52134592">http://blog.csdn.net/jackie_zhu/article/details/52134592</a>
ref: <a href="https://www.zhihu.com/question/20700829">https://www.zhihu.com/question/20700829</a>
</p>

<p>
模型过拟合的原因往往是模型过于复杂，拟合了不需要的参数
</p>

<p>
简单的说，正则项通过损失函数中的惩罚项，对参数施加限制，使其对噪声和异常值敏感程度较小
</p>

<ul class="org-ul">
<li>L1正则</li>
</ul>

\begin{eqnarray}
\nonumber
J = \frac{1}{N}\sum_{i=1}^N (f(x_i) - y_i)^2 + \lambda \sum_{i=1}^N ||w||
\end{eqnarray}

<ul class="org-ul">
<li>L2正则</li>
</ul>

\begin{eqnarray}
\nonumber
J = \frac{1}{N}\sum_{i=1}^N (f(x_i) - y_i)^2 + \lambda \sum_{i=1}^N ||w||^2
\end{eqnarray}


<p>
这里的 lambda 越大，表示对 w 的限制越强， w越接近0，（对应图中的区域越小） 模型复杂度越低，越不容易过拟合，模型方差越小
</p>

<p>
<code>过拟合：高方差</code>
<code>欠拟合：高偏差</code>
</p>

<p>
<code>概率论角度解释</code>
</p>

<p>
比如L2正则，相当于施加了一个0均值，α<sup>-1</sup> 为方差的正态分布约束，将其加入到极大似然里去，求对数，去掉常数项，即是后面的形式
</p>

<ul class="org-ul">
<li>当 α=0 时，即高斯分布方差趋向于无穷大，为无信息先验，即没有加上约束</li>
<li>当 α 增大时，表明先验的方差越小，模型越稳定，相对的variance越小，越不容易过拟合</li>
</ul>


<div class="figure">
<p><img src="pics/ridge.png" alt="ridge.png" />
</p>
</div>


<p>
<code>为什么L1正则可做特征选择？</code>
</p>

<ol class="org-ol">
<li>图形角度</li>
</ol>

<div class="figure">
<p><img src="pics/lasso.png" alt="lasso.png" />
</p>
</div>
<ol class="org-ol">
<li>概率论角度</li>

<li>L1加入了拉普拉斯先验，尖峰，在0处概率非常高</li>
<li>L2加入了高斯先验，钟形，在0处概率和0附近概率差不多</li>
</ol>
</div>
</div>





<div id="outline-container-orgca8fe57" class="outline-5">
<h5 id="orgca8fe57"><span class="section-number-5">2.1.3.2</span> 基于树的特征选择</h5>
<div class="outline-text-5" id="text-2-1-3-2">
<p>
以GBDT为例，特征j的全局重要程度通过特征j在单颗树中的重要程度平均值来衡量
</p>

\begin{eqnarray}
\nonumber
J_j^2 (T) = \sum_{t=1}^{L-1} i_t^2 1(v_t =j)
\end{eqnarray}

<p>
其中T为第T颗树，L为叶子节点数目，对于一颗满二叉树，非叶子节点数目等于L-1，1(v<sub>t</sub> = j)表示如果该非叶子节点的分裂特征是j，那么就是1，否则为0；i<sub>t</sub><sup>2表示该节点安装j分裂后带来的平方损失的减少值</sup>。
</p>

<p>
除了GBDT，普通的决策树、随机森林都可以进行特征选择
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org1cd3146" class="outline-3">
<h3 id="org1cd3146"><span class="section-number-3">2.2</span> 降维</h3>
<div class="outline-text-3" id="text-2-2">
</div>
<div id="outline-container-org2e0ef25" class="outline-4">
<h4 id="org2e0ef25"><span class="section-number-4">2.2.1</span> PCA 主成分分析和EVD分解</h4>
<div class="outline-text-4" id="text-2-2-1">
</div>
<div id="outline-container-org698ce28" class="outline-5">
<h5 id="org698ce28"><span class="section-number-5">2.2.1.1</span> 主成分分析要求</h5>
<div class="outline-text-5" id="text-2-2-1-1">
<ul class="org-ul">
<li>新维度特征之间相关性尽可能小</li>
<li>参数空间 \(\theta\) 有界</li>
<li>方差尽可能大,且每个主成分方差递减</li>
</ul>
</div>
</div>

<div id="outline-container-org57fe999" class="outline-5">
<h5 id="org57fe999"><span class="section-number-5">2.2.1.2</span> 特征值和特征向量</h5>
<div class="outline-text-5" id="text-2-2-1-2">
<p>
\(\lambda\) 和 \(\nu\) 是方阵A的特征值和特征向量,当且仅当:
</p>

\begin{eqnarray}
\nonumber
A \nu = \lambda \nu
\end{eqnarray}

<p>
从这个公式可以看出，特征值所对应的特征向量描绘了此变换的方向，而特征值描绘了此变换的大小，或者说此变换方向对整体方向的贡献值。特征分解满足：
</p>

\begin{eqnarray}
\nonumber
A = Q \Sigma Q^{-1}
\end{eqnarray}

<p>
其中A为方阵，Q为特征向量矩阵，每一列均为一个特征向量，Σ为特征值为对角元素的对角阵，与特征向量一一对应。特征值求法如下：
</p>

\begin{eqnarray}
\nonumber
|\lambda E -A | &=& 0 \\
\nonumber
(\lambda E -A)\nu &=& E
\end{eqnarray}
</div>
</div>

<div id="outline-container-org1469ce8" class="outline-5">
<h5 id="org1469ce8"><span class="section-number-5">2.2.1.3</span> 特征分解(EVD)和主成分的关系</h5>
<div class="outline-text-5" id="text-2-2-1-3">
<p>
主成分分析事实上是一个等式约束优化问题:
</p>

  \begin{eqnarray}
\nonumber
\max_{\lambda} {Var(\theta_1 X)}} \\
\nonumber
s.t. \ \ \theta_1 \theta_1^T = 1
\end{eqnarray}

<p>
其中约束表示了参数 θ 的有界性
</p>

<p>
利用拉格朗日乘子法,得到:
</p>

\begin{eqnarray}
\nonumber
\phi(\theta) = \theta_1 \Sigma \theta_1^T - \lambda (\theta_1 \theta_1^T - 1)
\end{eqnarray}

<p>
对 θ 求导可以得到 \(2(\Sigma - \lambda E) \theta_1 = 0\)  由于 θ 不等于0,因此转换为 \(|\Sigma - \lambda E| = 0\) 即特征值问题
</p>
</div>
</div>
</div>

<div id="outline-container-org7a16cd9" class="outline-4">
<h4 id="org7a16cd9"><span class="section-number-4">2.2.2</span> SVD分解</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
ref:
</p>
<ul class="org-ul">
<li><a href="https://blog.csdn.net/zhongkejingwang/article/details/43053513">https://blog.csdn.net/zhongkejingwang/article/details/43053513</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/6251584.html">https://www.cnblogs.com/pinard/p/6251584.html</a></li>
</ul>
</div>

<div id="outline-container-org97cf88a" class="outline-5">
<h5 id="org97cf88a"><span class="section-number-5">2.2.2.1</span> 正交向量和正交矩阵</h5>
<div class="outline-text-5" id="text-2-2-2-1">
<p>
正交向量即点积为0的两个向量 
</p>

<ul class="org-ul">
<li>正交矩阵必须是方阵</li>
<li>正交矩阵的转置等于其逆矩阵</li>
<li>正交矩阵的行和列都是两两正交的向量</li>
<li>正交矩阵对应的变换是正交变换</li>
<li>两种表现: 旋转和反射</li>
</ul>
</div>
</div>

<div id="outline-container-org72e8b2d" class="outline-5">
<h5 id="org72e8b2d"><span class="section-number-5">2.2.2.2</span> SVD介绍</h5>
<div class="outline-text-5" id="text-2-2-2-2">
<p>
EVD分解将方阵的一组正交基映射到了另一组正交基,而对于任意矩阵,同样也是可以的,这就是SVD分解
</p>

<p>
定义:
</p>

\begin{eqnarray}
\nonumber
A_{m \times n} = U_{m \times m} \Sigma_{m \times n} V^T_{n \times n}
\end{eqnarray}

<p>
其中 U 和 V 都是正交矩阵
</p>

<p>
<code>如何计算:</code>
</p>

<ol class="org-ol">
<li>对 \(A^T A\) 求n个特征值对应的特征向量, 张成一个 n x n 的矩阵,就是我们的 \(V\) (右奇异矩阵)</li>
<li>对 \(AA^T\) 求 m个特征值对应的特征向量, 张成一个 m x m的矩阵,就是我们的 \(U\) (左奇异矩阵)</li>
<li><p>
我们注意到:
</p>
\begin{eqnarray}
\nonumber
A = U \Sigma V^T \Rightarrow AV = U \Sigma V^T V \Rightarrow A v_i = \sigma_i u_i \Rightarrow \sigma_i = \frac{A v_i}{u_i}
\end{eqnarray}
<p>
其中, \(v_i, u_i\) 分别是 V 和 U 的第i个特征向量(第i列), v<sub>i维度为</sub> n x 1, u<sub>i的</sub>
维度为 m x 1
</p></li>
</ol>


<p>
<code>特征值等于奇异值矩阵的平方</code> 证明见 ref 2.
</p>
</div>
</div>

<div id="outline-container-org3735210" class="outline-5">
<h5 id="org3735210"><span class="section-number-5">2.2.2.3</span> SVD性质</h5>
<div class="outline-text-5" id="text-2-2-2-3">
<p>
<code>可以用最大的k个奇异值描述整个向量(类似EVD分解)</code>
</p>

<p>
因此可以用来降维, 数据压缩和去噪, 推荐算法
</p>

<p>
<code>优点:</code>
</p>
<ul class="org-ul">
<li>可并行</li>
<li>原理简单</li>
<li>可针对任意矩阵</li>
</ul>
<p>
<code>缺点:</code>
</p>
<ul class="org-ul">
<li>分解矩阵解释性不强,类似黑盒子</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgbcdbd70" class="outline-4">
<h4 id="orgbcdbd70"><span class="section-number-4">2.2.3</span> 线性判别分析</h4>
<div class="outline-text-4" id="text-2-2-3">
<p>
ref: <a href="https://blog.csdn.net/daunxx/article/details/51881956">https://blog.csdn.net/daunxx/article/details/51881956</a>
</p>

<p>
根据假设的条件分布P(x|y)寻找决策面，我们知道：
</p>

\begin{eqnarray}
\nonumber
P(y=i|x) = \frac{f(x|y=i) \pi_{y=i}}{\sum_j f(x|y=j) \pi_{y=j}}
\end{eqnarray}

<p>
分类器的差异表现在 f(x|y=i)的分布函数假定不同
</p>

<p>
<code>上述公式只针对生成式模型</code>
</p>
</div>

<div id="outline-container-orgdf5c8e0" class="outline-5">
<h5 id="orgdf5c8e0"><span class="section-number-5">2.2.3.1</span> LDA</h5>
<div class="outline-text-5" id="text-2-2-3-1">
<p>
又叫Fisher判别，其假设f(x|y=i)为均值不同，方差相同的正态分布，可以用来降维
是一个： <code>有监督的降维或是分类方法</code>
</p>

<p>
<code>有监督学习</code>
</p>

<p>
<code>目的:</code>
使得降维后的点尽可能的容易被区分, 但是不保证投影后正交
</p>


<p>
<code>实现方法：</code>
</p>
<ul class="org-ul">
<li>通过瑞利熵</li>
<li>最大化类间距</li>
<li>最小化类内聚</li>
</ul>
</div>
</div>
<div id="outline-container-orgff1dfe9" class="outline-5">
<h5 id="orgff1dfe9"><span class="section-number-5">2.2.3.2</span> GDA</h5>
<div class="outline-text-5" id="text-2-2-3-2">
<p>
高斯判别分析GDA是LDA的核变换后的版本
</p>
</div>
</div>

<div id="outline-container-org0356355" class="outline-5">
<h5 id="org0356355"><span class="section-number-5">2.2.3.3</span> 二次判别分析QDA</h5>
<div class="outline-text-5" id="text-2-2-3-3">
<p>
假设f(x|y=i)服从均值不同，方差也不同的正态分布
</p>
</div>
</div>
</div>

<div id="outline-container-orge83f554" class="outline-4">
<h4 id="orge83f554"><span class="section-number-4">2.2.4</span> LASSO</h4>
<div class="outline-text-4" id="text-2-2-4">
<p>
见 Logistic 章节
</p>
</div>
</div>
<div id="outline-container-orgf28020f" class="outline-4">
<h4 id="orgf28020f"><span class="section-number-4">2.2.5</span> 小波分析</h4>
</div>
<div id="outline-container-org5eb7cb0" class="outline-4">
<h4 id="org5eb7cb0"><span class="section-number-4">2.2.6</span> 深度学习SparseAutoEncoder</h4>
<div class="outline-text-4" id="text-2-2-6">
<p>
用少于输入层神经元数量的隐含层神经元去学习表征输入层的特征，相当于把输入层的特征压缩了，所以是特征降维。 
</p>
</div>
</div>

<div id="outline-container-orge6323d3" class="outline-4">
<h4 id="orge6323d3"><span class="section-number-4">2.2.7</span> 拉普拉斯映射(流形学习)</h4>
<div class="outline-text-4" id="text-2-2-7">
<p>
ref:
</p>
<ul class="org-ul">
<li><a href="https://blog.csdn.net/yujianmin1990/article/details/48420483">https://blog.csdn.net/yujianmin1990/article/details/48420483</a></li>
</ul>

<p>
拉普拉斯映射就是直接在低维下找到样本，使得所有样本保持原来的相似度。
</p>
</div>
</div>

<div id="outline-container-orga779b1f" class="outline-4">
<h4 id="orga779b1f"><span class="section-number-4">2.2.8</span> 低维线性嵌入(流形学习)</h4>
<div class="outline-text-4" id="text-2-2-8">
<p>
假设数据中每个点可以由其近邻的几个点重构出来。降到低维，使样本仍能保持原来的重构关系，且重构系数也一样。
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgbd96d00" class="outline-2">
<h2 id="orgbd96d00"><span class="section-number-2">3</span> <span class="done DONE">DONE</span> 模型选择<code>[3/3]</code></h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>State "DONE"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-21 日 15:46]</span></span></li>
<li>State "TODO"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-21 日 15:26]</span></span></li>
<li>State "TODO"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-21 日 15:18]</span></span></li>
<li>State "TODO"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-13 六 15:34]</span></span></li>
<li>State "TODO"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-13 六 15:34]</span></span></li>
<li>State "TODO"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-13 六 15:34]</span></span></li>
<li>State "TODO"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-07 日 14:13]</span></span></li>
</ul>
</div>

<div id="outline-container-org9316fa4" class="outline-3">
<h3 id="org9316fa4"><span class="section-number-3">3.1</span> <span class="done DONE">DONE</span> 模型评价<code>[4/4]</code></h3>
<div class="outline-text-3" id="text-3-1">
</div>
<div id="outline-container-orgac2bef5" class="outline-4">
<h4 id="orgac2bef5"><span class="section-number-4">3.1.1</span> <span class="done DONE">DONE</span> 二分类模型</h4>
<div class="outline-text-4" id="text-3-1-1">
<ul class="org-ul">
<li>accuracy</li>
<li>precision</li>
<li>recall</li>
<li>f1</li>
<li>auc<sub>roc</sub>
<ul class="org-ul">
<li>只在输出为概率时有用，如logistic回归</li>
<li>auc 位roc曲线的下面积，其物理意义为任取一对正负样本对，正样本的score大于负样本的概率</li>
<li>计算: 
<ol class="org-ol">
<li>给定(score, label)元组, 包括M个正样本和N个负样本,</li>
<li>我们先将score排序, 找到最大得分的正样本的序号(rank<sub>1</sub>), 那么得分比他小的正样本数目为 M-1,比分比他小的负样本数目为 (rand<sub>1</sub> - 1) - (M-1)</li>
<li>再找到第二大得分的正样本序号(rank<sub>2</sub>) , 得分比他小的正样本数目为 M-2, 得分比他小的负样本数目为 (rank<sub>2</sub> - 1) - (M-2)</li>
<li><p>
我们要计算一对样本正的得分大于负的的概率,因此需要将所有得分比正样本小的负样本数目相加,除以正负配对总数,即 
</p>
\begin{eqnarray}
\nonumber
AUC = \frac{\sum_i^M (rand\_i - 1 - (M-i))}{MN} = \frac{\sum_i^M rand\_i - M(M+1)/2}{MN}
\end{eqnarray}</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>


<div id="outline-container-orge6cf51d" class="outline-4">
<h4 id="orge6cf51d"><span class="section-number-4">3.1.2</span> <span class="done DONE">DONE</span> 多分类模型</h4>
<div class="outline-text-4" id="text-3-1-2">
<ul class="org-ul">
<li>f1<sub>micro</sub></li>
<li>f1<sub>macro</sub></li>
<li>f1<sub>weight</sub></li>
</ul>
</div>
</div>
<div id="outline-container-org89fadd1" class="outline-4">
<h4 id="org89fadd1"><span class="section-number-4">3.1.3</span> <span class="done DONE">DONE</span> 回归模型</h4>
<div class="outline-text-4" id="text-3-1-3">
<ul class="org-ul">
<li>explained<sub>variance</sub></li>
<li>absolute<sub>error</sub></li>
<li>squared<sub>error</sub></li>
<li>RMSE(root mean squared error)</li>
<li>RMSLE(root mean squared log error, in case of the abnormal value)</li>
<li>r2</li>
<li>median<sub>absolute</sub><sub>error</sub></li>
</ul>
</div>
</div>
<div id="outline-container-org92d4001" class="outline-4">
<h4 id="org92d4001"><span class="section-number-4">3.1.4</span> <span class="done CANCELED">CANCELED</span> 聚类模型</h4>
<div class="outline-text-4" id="text-3-1-4">
<p>
<code>了解即可</code>
</p>
<ul class="org-ul">
<li>互信息</li>
<li>rand系数</li>
<li>轮廓系数</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org559e58d" class="outline-3">
<h3 id="org559e58d"><span class="section-number-3">3.2</span> <span class="done DONE">DONE</span> 交叉验证</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>留出法：选出两个互斥子集分别作为训练集和测试集</li>
<li>K折交叉：分成K个互斥子集，对每一个子集作为测试集，其他的作为训练集，进行K次检验（K=样本数时，为留一法）</li>
<li>自助法：</li>
</ul>
<p>
从训练集D中有放回的抽样，得到D'，如果抽的次数足够多，则始终没被抽到的概率将近三分之一：
</p>

\begin{eqnarray}
\nonumber
\lim_{m\rightarrow \infty} ( 1- \frac{1}{m}) ^ m \rightarrow \frac{1}{e} = 0.368
\end{eqnarray}

<p>
注意：该公式在随机森林抽取变量时也同样用掉了，证明三分之一这个概率
</p>

<p>
此时将D'作为训练集，D/D'(没被抽到的)作为测试集，进行验证。
</p>

<p>
<code>自助法在数据集较小，难以有效划分训练集和测试集时非常有用</code> ，但是由于改变了初始数据的分布，因此会引入估计偏差，所以前两种用的比较多一点
</p>

<p>
<code>对于包含时序的数据，必须使用之前的数据进行交叉验证，来预测之后的数据</code>
</p>
</div>
</div>

<div id="outline-container-org196c7bd" class="outline-3">
<h3 id="org196c7bd"><span class="section-number-3">3.3</span> <span class="done CANCELED">CANCELED</span> 网格搜索</h3>
</div>

<div id="outline-container-orgb08a380" class="outline-3">
<h3 id="orgb08a380"><span class="section-number-3">3.4</span> 如何检验过拟合</h3>
<div class="outline-text-3" id="text-3-4">
<p>
<a href="https://www.cnblogs.com/yan2015/p/5052393.html">https://www.cnblogs.com/yan2015/p/5052393.html</a>
学习曲线，通过做出随样本变化与训练集、测试集的误差变化曲线，观察方差与偏差是否偏大
</p>

<ul class="org-ul">
<li>如果纵坐标是Error,则下方为训练集,上方为测试集</li>
<li>如果纵坐标是效果指标,则上方是训练集,下方是测试集</li>
</ul>

<p>
理论 样本量增加必然减少过拟合问题,而过拟合减少的速度决定了模型的好坏
</p>

<p>
解决方法
</p>
<ul class="org-ul">
<li>增加样本</li>
<li>降低模型复杂度</li>
<li>正则项</li>
<li>dropout层(神经网络)</li>
<li>松弛变量(SVM)</li>
<li>剪枝(决策树)</li>
<li>增加K值(KNN)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org09e5ed9" class="outline-2">
<h2 id="org09e5ed9"><span class="section-number-2">4</span> <span class="done DONE">DONE</span> 基于树的算法<code>[2/2]</code></h2>
<div class="outline-text-2" id="text-4">
<ul class="org-ul">
<li>State "DONE"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-16 二 11:25]</span></span></li>
<li>State "TODO"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-15 一 14:38]</span></span></li>
<li>State "TODO"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2018-01-13 六 15:38]</span></span></li>
</ul>
</div>
<div id="outline-container-orgf1f74c7" class="outline-3">
<h3 id="orgf1f74c7"><span class="section-number-3">4.1</span> <span class="done DONE">DONE</span> 决策树</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>State "TODO"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2017-12-23 六 16:10]</span></span></li>
</ul>

<div class="figure">
<p><img src="pics/decision_tree.png" alt="decision_tree.png" />
</p>
</div>
</div>


<div id="outline-container-org5d3931e" class="outline-4">
<h4 id="org5d3931e"><span class="section-number-4">4.1.1</span> 分类树</h4>
<div class="outline-text-4" id="text-4-1-1">

<div class="figure">
<p><img src="pics/tree2.png" alt="tree2.png" />
</p>
</div>

<ol class="org-ol">
<li>ID3 
<ul class="org-ul">
<li>划分依据：最大信息熵增益</li>
<li>多叉树</li>
<li>只针对分类变量</li>
</ul></li>
<li>C4.5 
<ul class="org-ul">
<li>划分依据：信息增益比率（使用分裂信息来惩罚取值较多的Feature，防止取值较多的feature由于其信息增益较大而被优先选中）</li>
<li>多叉树</li>
<li>分类变量或连续变量</li>
</ul></li>
<li>CART
<ul class="org-ul">
<li>根据基尼系数划分</li>
<li>二叉树</li>
<li>分类变量或连续变量</li>
</ul></li>
</ol>

<p>
<code>损失函数</code>
</p>

\begin{eqnarray}
\nonumber
J(\alpha, T) = \sum_{t=1}^{|T|}  N_t H_t + \alpha |T| \\
\nonumber
H_t = - \sum_k \frac{N_{tk}}{{N_t}} log \frac{N_{tk}}{{N_t}}
\end{eqnarray}

<p>
其中|T|为所有叶节点数目，N<sub>t表示叶节点中样本数</sub>，H<sub>t为该叶节点的信息熵</sub>，N<sub>tk</sub>表示叶节点中的类别k，alpha为惩罚项参数
</p>

<p>
前半部分表示了模型整体的效果，后半部分表示了惩罚项，叶节点越多越复杂
</p>
</div>
</div>



<div id="outline-container-org45c82e3" class="outline-4">
<h4 id="org45c82e3"><span class="section-number-4">4.1.2</span> 回归树</h4>
<div class="outline-text-4" id="text-4-1-2">
<p>
回归树本质上与分类树类似，只不过每一个分支节点和叶节点，都会得到一个因变量的预测值，并通过该预测值得到估计的均方误差，用来判断分类的结果，作为划分依据
</p>
</div>
</div>

<div id="outline-container-org2684af4" class="outline-4">
<h4 id="org2684af4"><span class="section-number-4">4.1.3</span> <span class="todo TODO">TODO</span> 剪枝</h4>
<div class="outline-text-4" id="text-4-1-3">
<ul class="org-ul">
<li>State "TODO"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2018-01-15 一 14:38]</span></span></li>

<li>前剪枝（设置参数）</li>
<li>后剪枝：
<ol class="org-ol">
<li>误差降低剪枝，原始根节点和去掉一个节点后根节点在测试集上的误判数量对比，如果去掉后误判减少了，则实现剪枝(需要测试集)</li>
<li>悲观剪枝，不需要测试集，二项分布渐进正态，连续修正因子，均值、方差为np、np(1-p)，当子树错误率大于等于叶子节点的错误率+一个标准差后，进行剪枝</li>
</ol></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgbb2619c" class="outline-3">
<h3 id="orgbb2619c"><span class="section-number-3">4.2</span> <span class="done DONE">DONE</span> 随机森林</h3>
<div class="outline-text-3" id="text-4-2">
</div>
<div id="outline-container-org4e6220a" class="outline-4">
<h4 id="org4e6220a"><span class="section-number-4">4.2.1</span> 概述</h4>
<div class="outline-text-4" id="text-4-2-1">
<p>
什么是随机森林：
</p>

<ul class="org-ul">
<li>森林：根据集成学习(Ensemble Learning)的思想，通过多个决策树进行分类，最终结果由多个决策树结果投票得到</li>
<li>随机：决策树的训练样本是从原始训练集中随机得到的：
<ul class="org-ul">
<li>原始训练集的总样本数为N，而每棵树的随机训练集的样本数也为N，但是是从原始样本中有放回抽N次得到的(bootstrap)</li>
<li>原始训练集的总特征数为M，而每棵树的随机训练集的特征数为m(m&lt;=M)，从原始样本的M个特征中随机无放回的抽取，m为随机森林唯一的超参数</li>
</ul></li>
</ul>

<p>
<code>为什么抽取样本时是有放回的</code> 如果不是有放回抽样，则每颗树的训练样本都是一样的（如果抽N个）、或者是高度相关的（如果抽n(n&lt;N)个样本，此时至少有(2*n-N)个样本是一样的） 
</p>

<p>
<code>袋外误差</code>
数据中总有1/3的样本未抽到，这个是袋外数据(out of bag, oob)，用训练好的模型估算袋外数据的误差，可以证明该误差是测试数据的无偏估计。
</p>

<p>
<code>随机森林的错误率和两个因素有关：</code>
</p>
<ol class="org-ol">
<li>两颗树样本的相关性越大，错误率越大</li>
<li>每个树的分类能力越强，整个森林的错误率越小</li>
</ol>

<p>
参数m的增加将导致树之间的相关性和树的分类能力同时增加，而m的减小也会导致两者同时减小，因此 <code>如何确定m非常关键</code>
</p>
</div>
</div>


<div id="outline-container-orga380e3b" class="outline-4">
<h4 id="orga380e3b"><span class="section-number-4">4.2.2</span> 优缺点</h4>
<div class="outline-text-4" id="text-4-2-2">
</div>
<div id="outline-container-org7ce5ca2" class="outline-5">
<h5 id="org7ce5ca2"><span class="section-number-5">4.2.2.1</span> 优点</h5>
<div class="outline-text-5" id="text-4-2-2-1">
<ol class="org-ol">
<li>在当前所有算法中，具有极好的准确率</li>
<li>能够有效地运行在大数据集上</li>
<li>能够处理具有高维特征的输入样本，而且不需要降维</li>
<li>能够评估各个特征在分类问题上的重要性</li>
<li>在生成过程中，能够获取到内部生成误差的一种无偏估计</li>
<li>对于缺省值问题也能够获得很好得结果</li>
<li>需要调的参数非常少</li>
<li>几乎不会有过拟合的问题，因为它相当于已经在内部进行了交叉验证（Breiman，2001），然而这点尚有争议（Elith and</li>
</ol>
<p>
Graham，2009）。
</p>
<ol class="org-ol">
<li>不需要顾忌多重共线性</li>
</ol>
</div>
</div>

<div id="outline-container-org7c5f0a9" class="outline-5">
<h5 id="org7c5f0a9"><span class="section-number-5">4.2.2.2</span> 缺点</h5>
<div class="outline-text-5" id="text-4-2-2-2">
<ol class="org-ol">
<li>对于回归问题表现不好，无法给出连续的预测，并且只能在训练集因变量的范围内进行预测</li>
<li>对于许多统计建模者来说，随机森林给人的感觉像是一个 <code>黑盒子</code></li>
<li>对于非平衡数据集效果不好，倾向于类别较多的值</li>
</ol>
</div>
</div>

<div id="outline-container-org2384555" class="outline-5">
<h5 id="org2384555"><span class="section-number-5">4.2.2.3</span> 为什么随机森林不存在过拟合问题</h5>
<div class="outline-text-5" id="text-4-2-2-3">
<ol class="org-ol">
<li>随机的样本和随机的特征使得模型不易陷入过拟合，具有较强的抗噪能力</li>
<li>无需通过交叉验证对其误差进行估计，它可以在内部进行评估，通过oob估计得到误差的无偏估计：
<ol class="org-ol">
<li>对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）；</li>
<li>然后以简单多数投票作为该样本的分类结果；</li>
</ol></li>
</ol>
<p>
　3) 最后用误分个数占样本总数的比率作为随机森林的oob误分率。
<code>oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。</code>
</p>
</div>
</div>
</div>

<div id="outline-container-orgf4748d3" class="outline-4">
<h4 id="orgf4748d3"><span class="section-number-4">4.2.3</span> 实现</h4>
</div>
</div>
</div>

<div id="outline-container-org9c1f411" class="outline-2">
<h2 id="org9c1f411"><span class="section-number-2">5</span> <span class="done DONE">DONE</span> KNN相关算法<code>[2/2]</code></h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li>State "DONE"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-07 日 20:10]</span></span></li>
<li>State "TODO"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-07 日 20:10]</span></span></li>
<li>State "TODO"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-04 四 14:31]</span></span></li>
<li>State "TODO"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2018-01-04 四 14:31]</span></span></li>
</ul>
</div>
<div id="outline-container-org1c9bbf5" class="outline-3">
<h3 id="org1c9bbf5"><span class="section-number-3">5.1</span> <span class="done DONE">DONE</span> KNN</h3>
<div class="outline-text-3" id="text-5-1">
</div>
<div id="outline-container-org580ede7" class="outline-4">
<h4 id="org580ede7"><span class="section-number-4">5.1.1</span> 模型</h4>
<div class="outline-text-4" id="text-5-1-1">
<p>
简述： 根据离待分类点距离最近的K个点的label，确定待分类点的label。
</p>

<p>
<code>注意：</code> knn当样本量越大时效果越好，但是带来计算量的上升
</p>
</div>
<div id="outline-container-orge14325e" class="outline-5">
<h5 id="orge14325e"><span class="section-number-5">5.1.1.1</span> 三要素</h5>
<div class="outline-text-5" id="text-5-1-1-1">
<ul class="org-ul">
<li>训练集</li>
<li>距离度量</li>
<li>K值</li>
</ul>

<p>
当三要素确定后，分类结果可以唯一确定。
</p>
</div>
</div>

<div id="outline-container-org63199ab" class="outline-5">
<h5 id="org63199ab"><span class="section-number-5">5.1.1.2</span> 距离度量</h5>
<div class="outline-text-5" id="text-5-1-1-2">
<ul class="org-ul">
<li>明可夫斯基距离 \(\sqrt[p]{\sum_{l=1}^n |x_i^{(l)} - x_j^{(l)}|^p}\) (p范数)</li>
<li>欧式距离 p = 2</li>
<li>曼哈顿距离 p = 1</li>
<li>最大值距离, p = inf, \(\max_{l} |x_i^{(l)} - x_j^{(l)}|\)</li>
<li>最小值距离，p = -inf, \(\min_{l} |x_i^{(l)} - x_j^{(l)}|\)</li>
</ul>
</div>
</div>
<div id="outline-container-orgbbe4ed2" class="outline-5">
<h5 id="orgbbe4ed2"><span class="section-number-5">5.1.1.3</span> K值的选择</h5>
<div class="outline-text-5" id="text-5-1-1-3">
<ul class="org-ul">
<li>K值较小：
<ul class="org-ul">
<li>学习的近似误差(approximation error)减小，只有相近的点才会起到作用</li>
<li>学习的估计误差(estimation error)增大，对近邻的点过于敏感，容易过拟合</li>
</ul></li>
<li>K值增大：
<ul class="org-ul">
<li>与上面刚好相反，意味着模型变简单，容易欠拟合</li>
</ul></li>
</ul>

<p>
在实际应用中，K一般取一个较小的值，然后通过交叉验证法来取最佳K值
</p>
</div>
</div>
</div>

<div id="outline-container-orgfaaf1ac" class="outline-4">
<h4 id="orgfaaf1ac"><span class="section-number-4">5.1.2</span> 分类的规则</h4>
<div class="outline-text-4" id="text-5-1-2">
<p>
KNN算法中的分类决策规则往往是多数表决
</p>

<p>
<b>多数表决等价于经验风险最小化</b> 《统计学习方法》(P40)
</p>
</div>
</div>
<div id="outline-container-org9ec34b3" class="outline-4">
<h4 id="org9ec34b3"><span class="section-number-4">5.1.3</span> 代码实现</h4>
</div>
</div>


<div id="outline-container-org2fc2a14" class="outline-3">
<h3 id="org2fc2a14"><span class="section-number-3">5.2</span> <span class="done DONE">DONE</span> KD树</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>用原始数据生成一颗平衡二叉树，对数据进行保存于索引</li>
<li>维度越接近样本数时，效率越低，越接近于KNN</li>
<li>找最近邻需要通过二叉搜索和 <b>回溯</b> 算法
<ol class="org-ol">
<li>从root节点开始，DFS搜索直到叶子节点，同时在stack中顺序存储已经访问的节点。</li>
<li>如果搜索到叶子节点，当前的叶子节点被设为最近邻节点。</li>
<li>然后通过stack回溯:</li>
<li>如果当前点的距离比最近邻点距离近，更新最近邻节点.</li>
<li>然后检查以最近距离为半径的圆是否和父节点的超平面相交.</li>
<li>如果相交，则必须到父节点的另外一侧，用同样的DFS搜索法，开始检查最近邻节点。</li>
<li>如果不相交，则继续往上回溯，而父节点的另一侧子节点都被淘汰，不再考虑的范围中.</li>
<li>当搜索回到root节点时，搜索完成，得到最近邻节点。</li>
</ol></li>
<li>算法复杂度分析：</li>
</ul>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Algorithm</th>
<th scope="col" class="org-left">Average</th>
<th scope="col" class="org-left">Worst</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Space</td>
<td class="org-left">O(n)</td>
<td class="org-left">O(n)</td>
</tr>

<tr>
<td class="org-left">Search</td>
<td class="org-left">O(logn)</td>
<td class="org-left">O(n)</td>
</tr>

<tr>
<td class="org-left">Insert</td>
<td class="org-left">O(logn)</td>
<td class="org-left">O(n)</td>
</tr>

<tr>
<td class="org-left">Delete</td>
<td class="org-left">O(logn)</td>
<td class="org-left">O(n)</td>
</tr>
</tbody>
</table>
<ul class="org-ul">
<li>当考虑K近邻时，可以维护一个近邻的优先队列（见<a href="https://en.wikipedia.org/wiki/K-d_tree">wiki<sub>KDTree</sub></a>)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org510605d" class="outline-2">
<h2 id="org510605d"><span class="section-number-2">6</span> <span class="done DONE">DONE</span> Logistic<code>[3/3]</code></h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>State "DONE"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-16 二 11:29]</span></span></li>
<li>State "TODO"       from "DONE"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-13 六 15:40]</span></span></li>
<li>State "DONE"       from "DONE"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-13 六 15:40]</span></span></li>
<li>State "DONE"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2018-01-13 六 15:40]</span></span></li>
</ul>
</div>
<div id="outline-container-orgcb4a5e1" class="outline-3">
<h3 id="orgcb4a5e1"><span class="section-number-3">6.1</span> <span class="done DONE">DONE</span> 理论</h3>
<div class="outline-text-3" id="text-6-1">
<p>
ref : <a href="http://blog.csdn.net/zjuPeco/article/details/77165974">http://blog.csdn.net/zjuPeco/article/details/77165974</a>
</p>
</div>
<div id="outline-container-org805d37d" class="outline-4">
<h4 id="org805d37d"><span class="section-number-4">6.1.1</span> sigmoid函数</h4>
<div class="outline-text-4" id="text-6-1-1">
\begin{eqnarray}
\nonumber
f(x) = \frac{1}{1+e^{-x}} 
\end{eqnarray}

<p>
将(-inf, inf)定义域映射到(0,1)值域，与之类似的还有tan函数。
</p>

<p>
<code>为什么选择sigmoid函数:</code>
</p>

<p>
logistic回归由指数族分布推出
</p>

<p>
指数族分布: 
</p>

\begin{eqnarray}
\nonumber
p(y;\eta) = b(y) exp(\eta^T T(y) - a(\eta))
\end{eqnarray}

<p>
而logistic回归服从伯努利分布:
</p>

\begin{eqnarray}
\nonumber
p(y;p) = p^y (1-p)^{(1-y)} = exp(y \ln{p} + (1-y)\ln{(1-p)}) = exp(y \ln{\frac{p}{1-p}} + \ln{(1-p)})
\end{eqnarray}

<p>
其中 \(\eta = \ln{\frac{p}{1-p}}\) 可以推出 \(p = \frac{1}{1 + e^{-\eta}}\) ,而根据指数族分布的假设:
</p>
<ol class="org-ol">
<li>已知a, b, T(.)时, \(\eta\) 可以确定一个指数族分布</li>
<li>\(eta\) 是线性于 X ,即 \(eta = w^T x\)</li>
</ol>

<p>
所以得到sigmoid函数: \(f(x) = p = \frac{1}{1 + e^{-w^T x}}\)
</p>

<p>
<code>sigmoid的重要性质：</code>
</p>

<p>
\[
f'(x) = f(x)(1-f(x))
\]
</p>

<p>
对于logfistic回归模型，考虑 \(x=(1, x_1, x_2,...,x_n)\) ，设条件概率 \(P(y=1|x)=p\) ，则logistic回归模型为：
</p>
\begin{eqnarray}
\nonumber
P(y=1|x) = \frac{1}{1+e^{-g(x)}} 
\end{eqnarray}
<p>
其中：
</p>
\begin{eqnarray}
\nonumber
g(x) = w^T x 
\end{eqnarray}

<p>
那么相反，在x条件下不发生的概率为 \[ P(y=0|x)=1-p=1-P(y=1|x) \] ，所以，
</p>
\begin{eqnarray}
\nonumber
P(y=0|x) = 1 - \frac{1}{1+e^{-g(x)}} = \frac{1}{1+e^{g(x)}}
\end{eqnarray}

<p>
所以事件发生于不发生的概率比为：
</p>

\begin{eqnarray}
\nonumber
\frac{P(y=1|x)}{P(y=0|x)} = e^{g(x)}
\end{eqnarray}

<p>
两边取对数得到：
</p>


\begin{eqnarray}
\nonumber
log(\frac{p}{1-p}) = g(x) = w^T x
\end{eqnarray}
</div>
</div>

<div id="outline-container-orgc20c802" class="outline-4">
<h4 id="orgc20c802"><span class="section-number-4">6.1.2</span> 估计方法</h4>
<div class="outline-text-4" id="text-6-1-2">
<p>
首先当然我们想到的是最小二乘估计，模仿线性回归，令残差平方和作为损失函数：
</p>
</div>

<div id="outline-container-orgc5a492c" class="outline-5">
<h5 id="orgc5a492c"><span class="section-number-5">6.1.2.1</span> 最小二乘估计</h5>
<div class="outline-text-5" id="text-6-1-2-1">
<p>
损失函数为：
</p>

\begin{eqnarray}
\nonumber
j(w) = \sum_i \frac{1}{2} (\phi(g(x_i)) - y_i)^2
\end{eqnarray}

<p>
其中 \(\phi()\) 为sigmoid函数， 此时发现损失函数非凸，导致存在较多的局部最小值，难以求解
</p>
</div>
</div>

<div id="outline-container-org37ce269" class="outline-5">
<h5 id="org37ce269"><span class="section-number-5">6.1.2.2</span> 极大似然估计</h5>
<div class="outline-text-5" id="text-6-1-2-2">
<p>
将上文中的 \(P(y=i|x), i \in {0,1}\) 写成一般形式：
</p>

\begin{eqnarray}
\nonumber
P(y|x,w) = \phi(g(x))^y (1 - \phi(g(x)))^{(1-y)}
\end{eqnarray}

<p>
对于每一个样本，极大似然估计假设其独立同分布，则将每个样本概率相乘，可得其联合概率（似然值），为了方便计算，我们对似然值取对数，同时另z = g(x)：
</p>

\begin{eqnarray}
\nonumber
log(L(w)) = \sum_{i=1}^n (y^{(i)} ln(\phi(z^{(i)})) + (1-y^{(i)})(1 - ln(\phi(z^{(i)})))
\end{eqnarray}

<p>
此时要取似然函数的最大值，而为了与损失函数对应，因此我们在左右两侧加上负号，得到损失函数：
</p>

<p>
\[
J(w) = -log(L(w))
\]
</p>

<p>
<code>注意，这里就是为什么logistic回归要用对数损失而不是平方损失</code>
</p>

<p>
此时损失函数见下图，如果样本值为1，则sigmoid函数值越接近1，损失越小
</p>


<div class="figure">
<p><img src="pics/logistic_loss.png" alt="logistic_loss.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgb6f55a2" class="outline-4">
<h4 id="orgb6f55a2"><span class="section-number-4">6.1.3</span> 求解</h4>
<div class="outline-text-4" id="text-6-1-3">
</div>
<div id="outline-container-orgbab8db4" class="outline-5">
<h5 id="orgbab8db4"><span class="section-number-5">6.1.3.1</span> 梯度下降 （gradient descent）</h5>
<div class="outline-text-5" id="text-6-1-3-1">
<p>
梯度方向即函数变化最快的方向，沿着梯度方向寻找更容易找到函数的最大值，而沿着梯度想法的方向寻找更容易找到最小值
</p>

<p>
sigmoid函数有着如下优良的性质，因此求导非常容易
</p>
\begin{eqnarray}
\phi ' (z) = \phi (z) (1 - \phi(z))
\end{eqnarray}

<p>
对于梯度下降，我们需要求损失函数在参数向量一个分量上的偏导数，用以更新参数向量：
</p>


\begin{eqnarray}
\frac{\partial J(w)}{\partial w_j} = - \sum_{i=1}^n (y^{(i)} \frac{1}{\phi(z^{(i)})} - (1 - y^{(i)}) \frac{1}{1 - \phi(z^{(i)})}) \frac{\partial \phi(z^{(i)})} {w_i}
\end{eqnarray}

<p>
而根据sigmoid的性质，可得：
</p>

\begin{eqnarray}
\nonumber
\frac{\partial \phi(z^{(i)})}{w_i} = \phi'(z^{(i)}) \frac{\partial z^{(i)}}{\partial w_i}
\end{eqnarray}

<p>
综上带入，即可得到较为简化的梯度函数：
</p>

\begin{eqnarray}
\nonumber
w_j := w_j - \eta \frac{\partial J(w)}{\partial w_i} = w_j + \eta \sum_{i=1}^n (y^{(i)} - \phi(z^{(i)})) x_j^{(i)}
\end{eqnarray}
</div>
</div>

<div id="outline-container-orgca2fbe1" class="outline-5">
<h5 id="orgca2fbe1"><span class="section-number-5">6.1.3.2</span> 随机梯度下降（stochastic gradient descent）</h5>
<div class="outline-text-5" id="text-6-1-3-2">
<p>
梯度下降的公式中可以看出，在样本量非常大，即 n-&gt;inf 时，每次更新权重会非常耗时，随机梯度下降即是为了解决此问题提出的
</p>

<p>
随机梯度下降是指每次更新权重时随机选出一个样本进行，而不是之前的全样本计算然后加总
</p>

<p>
<code>随机梯度下降加速</code> 对梯度下降重新建模： \(w := m * w - \eta ()\) ，m表示动量（ <code>Momentum</code> ），物理意义为摩擦力，为了防止参数在谷底不能停止的情况，一般在一开始将m设为0.5，在一定的迭代次数后不断增加，最后到0.99。
</p>

<p>
在实践中，一般采取SGD + momentum的方式
</p>
</div>
</div>
<div id="outline-container-org605fab5" class="outline-5">
<h5 id="org605fab5"><span class="section-number-5">6.1.3.3</span> 小批量梯度下降（mini-batch gradient descent）</h5>
<div class="outline-text-5" id="text-6-1-3-3">
<p>
不使用全样本，而是每次抽取一定数量的样本
</p>
</div>
</div>

<div id="outline-container-orgb187789" class="outline-5">
<h5 id="orgb187789"><span class="section-number-5">6.1.3.4</span> 学习率更新</h5>
<div class="outline-text-5" id="text-6-1-3-4">
<ul class="org-ul">
<li>逐步降低（Step decay），即经过一定迭代次数后将学习率乘以一个小的衰减因子。典型的做法包括经过5次迭代（epoch）后学习率乘以0.5，或者20次迭代后乘以0.1。</li>
<li>指数衰减（Exponential decay），其数学表达式可以表示为：α=α0e−kt，其中，α0和k是需要设置的超参数，t是迭代次数。</li>
<li>倒数衰减（1/t decay），其数学表达式可以表示为：α=α0/(1+kt)，其中，α0和k是需要设置的超参数，t是迭代次数。</li>
</ul>

<p>
实践中发现逐步衰减的效果优于另外两种方法，一方面在于其需要设置的超参数数量少，另一方面其可解释性也强于另两种方法。
</p>
</div>
</div>
<div id="outline-container-org5be9f8b" class="outline-5">
<h5 id="org5be9f8b"><span class="section-number-5">6.1.3.5</span> 拟牛顿法</h5>
<div class="outline-text-5" id="text-6-1-3-5">
<p>
上述所有方法都是一阶更新方法，而加速的另外一种思路是利用二阶更新方法，包括牛顿法、拟牛顿法(<a href="http://blog.csdn.net/itplus/article/details/21897443">http://blog.csdn.net/itplus/article/details/21897443</a>)等等（这里要用到Hessian矩阵，对内存要求较高）。
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgba3c42a" class="outline-3">
<h3 id="orgba3c42a"><span class="section-number-3">6.2</span> <span class="done DONE">DONE</span> 实现</h3>
<div class="outline-text-3" id="text-6-2">
<p>
见mysimlpelogit.py
</p>
</div>
</div>
<div id="outline-container-org62f2a3c" class="outline-3">
<h3 id="org62f2a3c"><span class="section-number-3">6.3</span> <span class="done DONE">DONE</span> 多元logistic情况（Multinormal）</h3>
<div class="outline-text-3" id="text-6-3">
<p>
如果存在多个分类，那么可以训练多个分类器，一类一个，每一个训练样本都只属于下面两类：“是这类”和“不是这类”。训练的时候也是训练N套参数。
</p>

<p>
对于一个测试样本，带入每一个分类器计算一遍概率，以概率最大的分类有效。
</p>
</div>
</div>
</div>

<div id="outline-container-orgbbd492e" class="outline-2">
<h2 id="orgbbd492e"><span class="section-number-2">7</span> <span class="done DONE">DONE</span> 基于贝叶斯的算法</h2>
<div class="outline-text-2" id="text-7">
<ul class="org-ul">
<li>State "DONE"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2018-01-17 三 12:58]</span></span></li>
</ul>
</div>
<div id="outline-container-org9b837ce" class="outline-3">
<h3 id="org9b837ce"><span class="section-number-3">7.1</span> 朴素贝叶斯</h3>
<div class="outline-text-3" id="text-7-1">
<p>
ref: 西瓜书
</p>
</div>
<div id="outline-container-org9b69b18" class="outline-4">
<h4 id="org9b69b18"><span class="section-number-4">7.1.1</span> 贝叶斯模型简介</h4>
<div class="outline-text-4" id="text-7-1-1">
<ul class="org-ul">
<li>判别式模型(discriminative models)：直接对P(Y|X)建模，来预测Y，包括决策树，BP神经网络，支持向量机</li>
<li>生成式模型(generative models): 先对联合概率分布P(X, Y)建模，再由此获得P(Y|X)，包括贝叶斯模型</li>

<li>生成式：对p(x, y)建模：朴素贝叶斯，LDA，隐马尔科夫，混合高斯</li>
<li>判别式：对P(y|x)建模：LR,SVM,决策树,Boosting，条件随机场，区分度训练</li>
</ul>

<p>
贝叶斯公式为（ <code>此处贝叶斯公式的分母由全概率公式推导得到</code> ）:
</p>

\begin{eqnarray}
\nonumber
P(Y|X) = \frac{P(Y) P(X|Y)}{P(X)}
\end{eqnarray}

<p>
P(Y)为先验概率；P(X|Y)为样本对标记的条件概率，又称为似然；P(X)为用于归一化的“证据”(evidence)因子。因此估计P(Y|X)的问题变为如何估计P(Y)和P(X|Y)。
</p>

<ul class="org-ul">
<li>P(Y)的估计：根据大数定律，当训练集包含充足的独立同分布样本时，可以通过样本频率估计总体概率</li>
<li>P(X|Y)的估计：当训练集维度很高时，往往存在极多种可能，导致很多概率稀疏，因此有着较大的困难</li>
</ul>
</div>
</div>

<div id="outline-container-orgaa76ecf" class="outline-4">
<h4 id="orgaa76ecf"><span class="section-number-4">7.1.2</span> 朴素贝叶斯模型的提出</h4>
<div class="outline-text-4" id="text-7-1-2">
<p>
为了克服P(X|Y)在有限样本下估计困难的问题，提出“属性条件独立性假设”，即每个属性独立的对分类结果产生影响
</p>

<p>
<code>贝叶斯公式分母对于所有类别来说是常数</code>: 因为给定类别下只要比较正的概率和负的概率谁大即可，而正负概率的分母相等
</p>


<p>
由于对每个类别来说，P(X)是相同的，因此我们得到朴素贝叶斯判定准则：
</p>

\begin{eqnarray}
\nonumber
h_{nb}(x) = \max_{y \in Y} P(y) \prod\limits_{i=1}^{d} P(x_i|y)
\end{eqnarray}

<p>
其中，d为属性数，x<sub>i为第i个属性的取值</sub>，y为标签的类别，Y为标签的集合，此时x<sub>i的取值是我们要预测的测试样本的取值</sub>
</p>

<ul class="org-ul">
<li>标签的先验概率可以非常容易的得到：</li>
</ul>

\begin{eqnarray}
\nonumber
P(y) = \frac{|D_y|}{|D|}
\end{eqnarray}

<p>
其中|D<sub>y</sub>|为第y类样本的数目，|D|为全样本数目
</p>

<ul class="org-ul">
<li>条件概率P(x<sub>i</sub> | y)可以估计为：</li>
</ul>

\begin{eqnarray}
\nonumber
P(x_i|y) = \frac{|D_{y,x_i}|}{|D_y|}
\end{eqnarray}

<p>
其中|D<sub>y,x<sub>i</sub></sub>|表示在D<sub>y</sub> 中，第i个属性取值为x<sub>i的样本个数</sub>
</p>

<ul class="org-ul">
<li>对于连续属性，假定服从正态分布，利用样本可以估计出第y类样本该属性的均值和标准差，在根据该属性的取值和正态分布密度函数，得到其概率。</li>
</ul>
</div>
</div>

<div id="outline-container-org8ce8b82" class="outline-4">
<h4 id="org8ce8b82"><span class="section-number-4">7.1.3</span> 拉普拉斯平滑</h4>
<div class="outline-text-4" id="text-7-1-3">
<p>
<code>为何要平滑</code>
</p>

<p>
当某一类别下某属性的取值并没有观测到，这并不意味着其概率为0，但是会导致整个概率等于0，因此需要进行平滑使其非常小但是不为0。
</p>

<p>
<code>什么是拉普拉斯平滑</code>
</p>

\begin{eqnarray}
\nonumber
\hat{P(y)} = \frac{|D_y| + 1}{|D| + N}
\end{eqnarray}

\begin{eqnarray}
\nonumber
\hat{P(x_i | c)} = \frac{|D_{y, x_i}| + 1}{|D_c| + N_i}
\end{eqnarray}

<p>
其中，N表示y所有的类别数，N<sub>i表示第i个属性所有的类别数</sub>。
</p>
</div>
</div>
</div>

<div id="outline-container-org82e5e6d" class="outline-3">
<h3 id="org82e5e6d"><span class="section-number-3">7.2</span> 最小错误率贝叶斯</h3>
<div class="outline-text-3" id="text-7-2">
<p>
根据贝叶斯公式：
</p>

\begin{eqnarray}
\nonumber
p(w_i|x) = \frac{p(x|w_i)p(w_i)}{p(x)}
\end{eqnarray}

<p>
其中w<sub>i</sub> 表示第i个类别， \(p(w_i)\) 表示第i个类别的先验概率，可由样本得到， \(p(x|w_i)\) 为x的似然，根据概率密度函数推得，而我们就是要使得 \(p(w_i|x)\) 最大
</p>

<p>
对每个类别w<sub>i</sub>，以及多维变量x，我们有:
</p>

\begin{eqnarray}
\nonumber
p(x|w_i) = \frac{1}{\sqrt[d]{2 \pi} |\Sigma_i|^{1/2}} e^{-1/2 (x-\mu_i)^T \Sigma_i^{-1} (x - \mu_i)}
\end{eqnarray}

<p>
其中 \(\Sigma_i\) 表示第i个类别样本的协方差矩阵,|sigma|表示行列式的值，\(\mu_i\) 表示均值矩阵，d表示x的维度。
上述方程分两个步骤：
</p>
<ol class="org-ol">
<li>参数的估计，包括 &mu;<sub>i</sub> 和 &sigma;<sub>i</sub>，这是模型的训练过程</li>
<li>概率密度的计算，即带入新的x，根据第1步得到的参数，计算上式，这是模型的预测过程</li>
</ol>
</div>
</div>

<div id="outline-container-org8eb3c60" class="outline-3">
<h3 id="org8eb3c60"><span class="section-number-3">7.3</span> 最小风险贝叶斯</h3>
<div class="outline-text-3" id="text-7-3">
<p>
在最小错误率贝叶斯基础上，乘上对于该类别采取i措施后的损失 &lambda;(\lapha<sub>i</sub>, w<sub>j</sub>) ，i为第i种决策，w<sub>j为第j个类别</sub>
</p>


\begin{eqnarray}
\nonumber
\alpha = \min_i R(\alpha_i|x) = \min_i \sum_j \lambda(\alpha_i,w_j) p(w_j|x)
\end{eqnarray}

<p>
广义的最小风险贝叶斯，只要得到 p(w<sub>j</sub>|x)即可，无需通过最小错误率贝叶斯得到。
</p>

<p>
举个栗子：
\(\lambda_{1,1}, \lambda_{1,2}, \lambda_{2,1}, \lambda_{2,2}\) 分别表示将第一类判为第一类，将第二类判为第一类、将第一类判为第二类等等等的损失。
那么
</p>
\begin{eqnarray}
\nonumber
R(\alpha_1|x) = \sum_{j=1}^2 \lambda_{1,j} P(w_j|x) \\
R(\alpha_2|x) = \sum_{j=1}^2 \lambda_{2,j} P(w_j|x) 
\end{eqnarray}

<p>
如果前者大于后者，说明将x分给第一类的损失要大于分给第二类的损失，那么我们就将其判为第二类。
</p>
</div>
</div>

<div id="outline-container-org82a3dc1" class="outline-3">
<h3 id="org82a3dc1"><span class="section-number-3">7.4</span> 半朴素贝叶斯</h3>
<div class="outline-text-3" id="text-7-4">
<p>
半朴素贝叶斯打破了变量之间相互独立的假定，同时提出了 <code>独依赖估计(One-Dependent Estimator)</code> 策略，即假设每个变量只和一个父属性有关，即：
</p>

\begin{eqnarray}
\nonumber
P(Y|X) \propto P(Y) \prod\limits_{i=1}^{d} P(x_i|Y, pa_i)
\end{eqnarray}

<p>
其中pa<sub>i为x</sub><sub>i所依赖的父属性</sub>，该式求解方法与之前类似，关键是如何合理的得到pa<sub>i</sub>，目前有如下几种方法：
</p>

<ol class="org-ol">
<li><code>SPODE</code> (Super-Parent One-Dependent Estimator):假定所有属性都依赖于一个父属性（超父），通过交验证方法来确定该超父</li>
<li><code>TAN</code> (Tree Augmented Naive Bayes):在最大加权生成树的基础上，通过以下步骤确定依赖关系：
<ul class="org-ul">
<li>计算任意两个属性之间的条件互信息I(x<sub>i</sub>, x<sub>j</sub>|Y)</li>
<li>以属性为节点构建完全图，任意两个节点间边的权重设为该完全互信息</li>
<li>构建此完全图的最大加权生成树，挑选根变量，将边变为有向边</li>
<li>加入类别结点y，增加从y到每个属性的有向边</li>
</ul></li>
<li><code>AODE</code> (Average ODE):将每个结点作为超父来构建SPODE，通过集成学习进行估计</li>
</ol>
</div>
</div>

<div id="outline-container-orga871bdd" class="outline-3">
<h3 id="orga871bdd"><span class="section-number-3">7.5</span> 贝叶斯网络</h3>
<div class="outline-text-3" id="text-7-5">
<p>
略
</p>
</div>
</div>

<div id="outline-container-org539f2bf" class="outline-3">
<h3 id="org539f2bf"><span class="section-number-3">7.6</span> 高斯过程回归</h3>
<div class="outline-text-3" id="text-7-6">
<p>
ref:
</p>
<ul class="org-ul">
<li><a href="https://www.zhihu.com/question/46631426?sort=created">https://www.zhihu.com/question/46631426?sort=created</a></li>
<li><a href="http://www.360doc.com/content/17/0810/05/43535834_678049865.shtml">http://www.360doc.com/content/17/0810/05/43535834_678049865.shtml</a></li>
</ul>


<p>
高斯过程可以用于非线性回归、非线性分类、参数寻优等等
</p>


<p>
以往的建模需要对 p(y|X)建模，当用于预测时，则是 
</p>

\begin{eqnarray}
\nonumber
p(y_{N+1} | X_{N+1})
\end{eqnarray}

<p>
而高斯过程则 <code>还考虑了y_N 和 y_{N+1}</code> 之间的关系，即：
</p>

\begin{eqnarray}
\nonumber
p(y_{N+1} | X_{N+1}, y_{N})
\end{eqnarray}

<p>
高斯过程通过假设Y值服从联合正态分布，来考虑 y<sub>N</sub> 和 y<sub>N+1</sub> 之间的关系，因此需要给定参数包括：均值向量和协方差矩阵，即：
</p>

\begin{eqnarray}
\nonumber

\begin{bmatrix}
y_1 \\
y_2 \\
... \\
y_n \\
\end{bmatrix} \sim
N( \mathbf{0}, \begin{bmatrix}
k(x_1, x_1) , k(x_1, x_2), ..., k(x_1, x_n) \\
k(x_2, x_1) , k(x_2, x_2), ..., k(x_2, x_n) \\
... \\
k(x_n, x_1) , k(x_n, x_2), ..., k(x_n, x_n) 
\end{bmatrix} )
\end{eqnarray}


<p>
其中协方差矩阵又叫做 <code>核矩阵</code>  记为 \(\mathbf{K}\) ，仅和特征x有关，和y无关。
</p>

<p>
高斯过程的思想是： <code>假设Y服从高维正态分布（先验），而根据训练集可以得到最优的核矩阵 ，从而得到后验以估计测试集Y*</code>
</p>

<p>
我们有后验：
</p>

\begin{eqnarray}
\nonumber
p(y_*| \mathbf{y} \sim N(K_* K^{-1} \mathbf{y}, ~  K_{**} - K_* K^{-1} K_*^T)
\end{eqnarray}

<p>
其中，K<sub>*</sub>为训练集的核向量，有如下关系：
</p>

\begin{eqnarray}
\nonumber
\begin{bmatrix}
\mathbf{y} \\
y_*
\end{bmatrix} \sim
N(\mathbf{0}, \begin{bmatrix}
K, K_*^T \\
K_*, K_{**} \\
\end{bmatrix})
\end{eqnarray}

<p>
可以发现，在后验公式中，只有均值和训练集Y有关，方差则仅仅和核矩阵，也就是训练集和测试集的X有关，与训练集Y无关
</p>

<p>
<code>估计（训练）方法</code>
</p>

<p>
假设使用平方指数核(Squared Exponential Kernel)，那么有：
</p>

\begin{eqnarray}
\nonumber
k(x_1, x_2) = \sigma^2_f exp(\frac{-(x_1 - x_2)^2}{2 l^2})
\end{eqnarray}

<p>
那么所需要的确定的超参数 \(\theta = [\sigma^2_f, l]\) ，由于Y服从多维正态分布，因此似然函数为：
</p>

\begin{eqnarray}
\nonumber
L = log p(y| x, \theta) = - \frac{1}{2} log|\mathbf{K}| - \frac{1}{2} (y - \mu)^T \mathbf{K}^{-1} (y - \mu) - n*log(2\pi)/2
\end{eqnarray}

<p>
由于K是由theta决定的，所以通过梯度下降即可求出超参数theta，而根据核矩阵的计算方式也可以进行预测。
</p>
</div>
</div>



<div id="outline-container-orge64fb0e" class="outline-3">
<h3 id="orge64fb0e"><span class="section-number-3">7.7</span> 贝叶斯优化</h3>
<div class="outline-text-3" id="text-7-7">
</div>
<div id="outline-container-orge024dd5" class="outline-4">
<h4 id="orge024dd5"><span class="section-number-4">7.7.1</span> 简介</h4>
<div class="outline-text-4" id="text-7-7-1">
<p>
贝叶斯优化是一种逼近思想，当计算非常复杂、迭代次数较高时能起到很好的效果，多用于超参数确定
</p>

<p>
<code>基本思想</code>
是基于数据使用贝叶斯定理估计目标函数的后验分布，然后再根据分布选择下一个采样的超参数组合。它充分利用了前一个采样点的信息，其优化的工作方式是通过对目标函数形状的学习，并找到使结果向全局最大提升的参数
</p>

<p>
<code>高斯过程</code> 用于在贝叶斯优化中对目标函数建模，得到其后验分布
</p>

<p>
通过高斯过程建模之后，我们尝试抽样进行样本计算，而贝叶斯优化很容易在局部最优解上不断采样，这就涉及到了开发和探索之间的权衡。
</p>

<ul class="org-ul">
<li>开发 (exploitation)：   根据后验分布，在最可能出现全局最优解的区域进行采样, 开发高意味着均值高</li>
<li>探索 (exploration):     在还未取样的区域获取采样点，   探索高意味着方差高</li>
</ul>

<p>
<code>Acquisition Function</code>
用来寻找下一个x的函数
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgcd2e482" class="outline-2">
<h2 id="orgcd2e482"><span class="section-number-2">8</span> <span class="done DONE">DONE</span> EM算法</h2>
<div class="outline-text-2" id="text-8">
<ul class="org-ul">
<li>State "DONE"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2018-01-17 三 13:30]</span></span></li>
</ul>
</div>
<div id="outline-container-org8760387" class="outline-3">
<h3 id="org8760387"><span class="section-number-3">8.1</span> 简介</h3>
<div class="outline-text-3" id="text-8-1">
<p>
由于实际观测中存在属性未知的情况，针对这种“未观测”变量，EM算法此时被用来对模型的“隐变量”进行有效的估计。
</p>

<p>
EM算法是一种 <code>迭代式</code> 算法， 他的基本想法是：
</p>
<ol class="org-ol">
<li>如果参数已知，则可以根据训练数据推断出最优隐变量（E步）</li>
<li>如果隐变量的值已知，则可以方便的对参数进行极大似然估计（M步）</li>
</ol>

<p>
EM算法交替上述两个步骤，直至收敛，得到最优隐变量和参数
</p>

<p>
是一种 <code>非梯度</code> 的优化算法
</p>
</div>
</div>

<div id="outline-container-orgae1d71f" class="outline-3">
<h3 id="orgae1d71f"><span class="section-number-3">8.2</span> EM算法和K-means聚类</h3>
<div class="outline-text-3" id="text-8-2">
<p>
K均值聚类的计算方法其实就是EM算法
</p>

<ul class="org-ul">
<li>参数为聚类中心</li>
<li>隐变量为每个样本的类别</li>

<li>初始化聚类中心</li>
<li>计算每个样本的类别</li>
<li>根据样本类别得到聚类中心</li>
<li>重复步骤2</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org7edb53c" class="outline-2">
<h2 id="org7edb53c"><span class="section-number-2">9</span> <span class="done DONE">DONE</span> 集成学习</h2>
<div class="outline-text-2" id="text-9">
</div>
<div id="outline-container-orgf3a08c6" class="outline-3">
<h3 id="orgf3a08c6"><span class="section-number-3">9.1</span> 理论</h3>
<div class="outline-text-3" id="text-9-1">
<p>
(泛化能力弱 &lt;&#x2013;&gt; 偏差高、方差大)
</p>

<p>
每个基分类器错误率为 epsilon，基分类器有如下两类：
</p>
<ul class="org-ul">
<li>弱基分类器：偏差高（准确度低），方差小（抗过拟合，更简单）</li>
<li>强基分类器：偏差低，方差大</li>
</ul>

<p>
<code>假设基分类器错误率相互独立</code> ，由Hoeffding不等式可知，集成的错误率为：
</p>

\begin{eqnarray}
\nonumber
p(H(x) \neq f(x)) = \sum_{k=0}^{[T/2]} C_T^k (1 - \epsilon)^k \epsilon^{(1-k)} \le exp(-\frac{1}{2} T (1-2\epsilon)^2)

\end{eqnarray}

<p>
所以当学习器够多时，错误率时接近于0的，但是注意前提！
</p>

<p>
因此，问题的核心即是： <code>如何产生“好而不同”的个体学习器</code>
</p>

<p>
目前集成学习可以分成如下两类
</p>
</div>

<div id="outline-container-org7ea7ac5" class="outline-4">
<h4 id="org7ea7ac5"><span class="section-number-4">9.1.1</span> Boosting方法</h4>
<div class="outline-text-4" id="text-9-1-1">
<p>
<code>代表算法： AdaBoost, GBDT</code>
</p>

<p>
bosting采用的是弱基分类器，主要关注降低偏差， 证明见：<a href="https://www.zhihu.com/question/29036379">https://www.zhihu.com/question/29036379</a>
</p>

<p>
基本思想： 通过对之前训练集进行调整，使之前错分的样本更加受到关注，然后在训练下一个模型，知道学习器数目达到事先制定的值T，最终对T个基学习器进行加权结合。
</p>

<p>
Boosting方法要求基学习器对特定数据分布（数据权重）进行学习，主要有两种方法：
</p>
<ul class="org-ul">
<li>对于可以接受权重参数的基分类器，采用re-weighting方法，每次训练更新样本权重</li>
<li>对于无法接受权重参数的基分类器，采用re-sampling方法，每次学习基于数据分布（权重）进行采样，用采样样本进行训练</li>
</ul>

<p>
<code>re-weighting 和 re-sampling 对比</code>
boosing每一轮都要检查当前分类器是否满足基本条件（比如检查是否比之前的更好），re-weighting如果不满足，则直接跳出，可能分类器数目未达到T，使效果不好；而re-sampling方法如果不满足，则可以重新抽样，再训练分类器，直至满足未知，因此更稳健。
</p>
</div>

<div id="outline-container-org5c02660" class="outline-5">
<h5 id="org5c02660"><span class="section-number-5">9.1.1.1</span> Adaboost算法</h5>
<div class="outline-text-5" id="text-9-1-1-1">
<p>
<code>核心思想是让误分类的点权重变高，从而加大分错的惩罚</code>
adaboost算法仅仅提供框架，伪代码如下
</p>


<div class="figure">
<p><img src="pics/adaboost.png" alt="adaboost.png" />
</p>
</div>

<p>
优点：
</p>
<ol class="org-ol">
<li>adaboost是一种有很高精度的分类器</li>
<li>可以使用各种方法构建子分类器，adaboost算法提供的是框架</li>
<li>当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单</li>
<li>简单，不用做特征筛选</li>
<li>不用担心overfitting！</li>
</ol>
</div>
</div>

<div id="outline-container-orgbc2a576" class="outline-5">
<h5 id="orgbc2a576"><span class="section-number-5">9.1.1.2</span> GBDT(Gradient Boosting Decision Tree)</h5>
<div class="outline-text-5" id="text-9-1-1-2">
<p>
ref:
<a href="https://www.jianshu.com/p/005a4e6ac775">https://www.jianshu.com/p/005a4e6ac775</a>
<a href="https://www.zhihu.com/question/29036379">https://www.zhihu.com/question/29036379</a> （更为详细）
<a href="https://www.cnblogs.com/pinard/p/6140514.html">https://www.cnblogs.com/pinard/p/6140514.html</a> (最好）
<a href="https://blog.csdn.net/yangxudong/article/details/53872141">https://blog.csdn.net/yangxudong/article/details/53872141</a> （深入理解）
</p>

<p>
理解如下概念：
</p>
<ol class="org-ol">
<li>回归树       比如CART，以平方损失作为划分标准，在每一个连续值中迭代出最优划分，预测值为当前节点的均值</li>
<li>提升树       当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，提升树即是整个迭代过程生成的回归树的累加。</li>
<li><code>梯度提升树GBDT</code></li>
</ol>

<p>
对于一般损失函数， <code>每一步优化没有那么容易？</code> 比如说绝对损失和Huber损失，针对这一问题，Freidman提出了梯度提升算法：
</p>

<p>
用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值，拟合一个回归树
</p>

<p>
下式表明，残差事实上是最小均方损失的反向梯度：
</p>
\begin{eqnarray}
\nonumber
- \frac{\partial (\frac{1}{2} * (y - F_{i-1}(x))^2)}{\partial F(x)} = y - F_{i-1}(x)
\end{eqnarray}

<p>
<code>为什么可以这么做:</code>
<img src="pics/gbdt_epsilon.png" alt="gbdt_epsilon.png" />
</p>

<p>
<code>步骤</code>
</p>

<p>
1、初始化，估计使损失函数极小化的常数值，它是只有一个根节点的树，即gamma是一个常数值。
2、
（a）计算损失函数的负梯度在当前模型的值，将它作为残差的估计 （计算残差）
（b）估计回归树叶节点区域，以拟合残差的近似值 （拟合回归树）
（c）利用线性搜索估计叶节点区域的值，使损失函数极小化 （计算叶节点的最优gamma）
（d）更新回归树
3、 得到输出的最终模型 f(x)
</p>

<p>
伪代码如下：
</p>


<div class="figure">
<p><img src="pics/gbdt.png" alt="gbdt.png" />
</p>
</div>


<p>
<code>可以证明， Gradient Boosting相当于二分类的Adaboost算法， 而指数损失仅可用于二分类的情况</code>
</p>

<p>
<code>注意：</code>
</p>
<ul class="org-ul">
<li>AdaBoost算法那对异常值较为敏感，而GBDT通过引入bagging抽样的方法以及正则项，对噪声更加稳健，并且能够更好地防止过拟合。</li>
<li>梯度提升有学习率，用于每次更新 y<sub>hat</sub> 并且目的是为了防止过拟合，学习率越低越过拟合</li>
<li>梯度提升树可以用来做特征选择，给定每个特征的得分</li>
<li>XGBoost对缺失值有着自己的处理方法:
<ul class="org-ul">
<li>如果训练中出现缺失值，将缺失的数据分别分到左子树和右子树，选择较优的那个</li>
<li>如果测试汇总出现缺失值，则默认被分到右子树</li>
</ul></li>
<li>GBDT可以结合随机森林的booststrap抽样方法,以减少过拟合</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgcdff84c" class="outline-4">
<h4 id="orgcdff84c"><span class="section-number-4">9.1.2</span> bagging</h4>
<div class="outline-text-4" id="text-9-1-2">
<p>
<code>代表算法：随机森林</code>
</p>

<p>
bagging通过随机生成多个互相之间尽可能有较大差异的分类器，同时保证每个分类器的效果，最终进行整合。
</p>

<p>
算法复杂度为 T(O(N)+O(s)) 约等于O(N) N为样本总数， 非常高效，可并行
</p>

<p>
可以通过 “袋外估计” 对泛化误差进行无偏的估计
</p>

<p>
Bagging主要关注降低方差，基分类器应当为 <code>强基分类器（低偏差，高方差）</code>  因此在不剪枝决策树、神经网络等易受样本干扰的学习器上效果更为明显
</p>
</div>
</div>


<div id="outline-container-orgcc81602" class="outline-4">
<h4 id="orgcc81602"><span class="section-number-4">9.1.3</span> 为什么说bagging减少variance，而boosting减少bias</h4>
<div class="outline-text-4" id="text-9-1-3">
<p>
ref: <a href="https://www.zhihu.com/question/26760839">https://www.zhihu.com/question/26760839</a>
</p>
</div>
</div>
</div>

<div id="outline-container-orgf1e69f0" class="outline-3">
<h3 id="orgf1e69f0"><span class="section-number-3">9.2</span> 相关包学习</h3>
<div class="outline-text-3" id="text-9-2">
</div>
<div id="outline-container-org5d6f00a" class="outline-4">
<h4 id="org5d6f00a"><span class="section-number-4">9.2.1</span> GBDT</h4>
<div class="outline-text-4" id="text-9-2-1">
<p>
sklearn下面
如何调参：
ref: <a href="http://www.alliedjeep.com/147311.htm">http://www.alliedjeep.com/147311.htm</a>
</p>
</div>
</div>
<div id="outline-container-org0381ce3" class="outline-4">
<h4 id="org0381ce3"><span class="section-number-4">9.2.2</span> XGBoost</h4>
<div class="outline-text-4" id="text-9-2-2">
<p>
<code>安装：</code> 直接pip whl文件安装，注意numpy需要mkl版本的，见 <a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost">https://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost</a>
</p>

<p>
<code>与GBDT区别：</code>
</p>

<p>
ref: <a href="http://blog.csdn.net/sb19931201/article/details/52557382">http://blog.csdn.net/sb19931201/article/details/52557382</a>
</p>

<p>
1.传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 —可以通过booster [default=gbtree]设置参数:gbtree: tree-based models/gblinear: linear models
</p>

<p>
2.传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 —对损失函数做了改进（泰勒展开，一阶信息g和二阶信息h,上一章节有做介绍）
</p>

<p>
3.xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性 
—正则化包括了两个部分，都是为了防止过拟合，剪枝是都有的，叶子结点输出L2平滑是新增的。
</p>

<p>
4.shrinkage and column subsampling —还是为了防止过拟合
</p>

<ol class="org-ol">
<li>。。。</li>
</ol>
</div>
</div>
</div>
</div>

<div id="outline-container-org3745fd7" class="outline-2">
<h2 id="org3745fd7"><span class="section-number-2">10</span> <span class="done DONE">DONE</span> HMM、条件随机场、混合高斯</h2>
<div class="outline-text-2" id="text-10">
</div>
<div id="outline-container-orgf683091" class="outline-3">
<h3 id="orgf683091"><span class="section-number-3">10.1</span> <span class="done DONE">DONE</span> HMM</h3>
<div class="outline-text-3" id="text-10-1">
<p>
<code>概率图</code> 模型的一种，概率图模型包括了半朴素贝叶斯和贝叶斯网络
</p>

<p>
<code>前提</code>
</p>
<ul class="org-ul">
<li>隐含状态必须离散</li>
<li>显示状态可以离散也可以连续</li>
</ul>

<p>
<code>三个假设</code>
</p>
<ol class="org-ol">
<li>有限历史性假设</li>
<li>齐次性假设（状态和具体时间无关）</li>
<li>输出独立性假设（输出仅与当前状态有关）</li>
</ol>

<p>
<code>三个主要问题</code>
</p>
<ol class="org-ol">
<li>评估问题</li>
</ol>
<p>
已知模型参数，包括了隐状态转移矩阵和显状态转移矩阵，以及概率图初始状态PGM，求某一观测序列发生的概率
算法：前向算法
</p>
<ol class="org-ol">
<li>解码问题</li>
</ol>
<p>
给定观测序列和模型，找到一个最合适的状态序列解释观测序列
算法：Viterbi算法
</p>
<ol class="org-ol">
<li>学习（训练）问题</li>
</ol>
<p>
如何调整模型参数得到概率最大的观测序列
</p>
</div>
</div>

<div id="outline-container-orgef5a757" class="outline-3">
<h3 id="orgef5a757"><span class="section-number-3">10.2</span> 条件随机场CRF</h3>
<div class="outline-text-3" id="text-10-2">
<p>
概率图模型的一种，判别式模型，借助马尔科夫无向图
</p>
</div>
</div>

<div id="outline-container-org7c04630" class="outline-3">
<h3 id="org7c04630"><span class="section-number-3">10.3</span> 混合高斯模型GMM</h3>
<div class="outline-text-3" id="text-10-3">
<p>
<code>可用于聚类和预测</code>
</p>

<p>
ref: <a href="https://blog.csdn.net/ice110956/article/details/13775071">https://blog.csdn.net/ice110956/article/details/13775071</a>
半监督（由于标记样本成本高昂，因此半监督学习同时利用标记的样本和未标记的样本），生成式，EM算法进行估计
</p>

<p>
估计过程和k-means非常类似，只不过初始值时正态分布参数而不是聚类中心点
</p>
</div>
</div>
</div>

<div id="outline-container-org54eca1a" class="outline-2">
<h2 id="org54eca1a"><span class="section-number-2">11</span> <span class="todo TODO">TODO</span> 支持向量机<code>[4/6]</code></h2>
<div class="outline-text-2" id="text-11">
<ul class="org-ul">
<li>State "TODO"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-05-31 四 13:13]</span></span></li>
<li>State "TODO"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-05-31 四 13:11]</span></span></li>
<li>State "TODO"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-21 日 15:09]</span></span></li>
</ul>
</div>

<div id="outline-container-org89000e4" class="outline-3">
<h3 id="org89000e4"><span class="section-number-3">11.1</span> 前言</h3>
<div class="outline-text-3" id="text-11-1">
</div>
<div id="outline-container-org4a1c457" class="outline-4">
<h4 id="org4a1c457"><span class="section-number-4">11.1.1</span> 函数间隔与几何间隔</h4>
<div class="outline-text-4" id="text-11-1-1">
<p>
点到直线的距离：
</p>

<p>
几何间隔 = 函数间隔 / 直线法向量的模（二范数）
</p>

<p>
我们固定函数间隔令其为1，从而最大化几何间隔，使得存在两个超平面将样本点分开
</p>

<p>
<code>为什么要最大化几何间隔</code>
误分次数 &lt;= (2R/d)<sup>2</sup>
</p>

<p>
其中R为所有样本中模最大的向量（即最长的向量），反应了样本分布有多广，d表示了几何间隔
</p>
</div>
</div>
</div>

<div id="outline-container-org6c91a9c" class="outline-3">
<h3 id="org6c91a9c"><span class="section-number-3">11.2</span> <span class="done DONE">DONE</span> 线性可分支持向量机与对偶方法</h3>
<div class="outline-text-3" id="text-11-2">
<p>
线性可分支持向量机是指通过一个超平面可以完全将两个类别区分开（过于理想的情况，仅帮助推导与理解）
</p>
</div>

<div id="outline-container-org6150c79" class="outline-4">
<h4 id="org6150c79"><span class="section-number-4">11.2.1</span> 对偶问题</h4>
<div class="outline-text-4" id="text-11-2-1">
<p>
对于上述优化，可以直接用凸二次规划的计算包来解，但是为了更高效，可以将其转为对偶问题来解：
</p>

<p>
对上式每条约束添加拉格朗日乘子，得到拉格朗日函数：
</p>

\begin{eqnarray}
L(w, b, \alpha) = \frac{1}{2}||w||^2 + \sum_{i=1}^N \alpha_i (1 - y ^{(i)} (w^T x ^{(i)} +b))
\end{eqnarray}

<p>
分别对w和b求偏导，可以得到：
</p>

\begin{eqnarray}
\nonumber
w = \sum_{i=1}^N \alpha_i  y_i x_i \\
\nonumber
0 = \sum_{i=1}^N \alpha_i y_i
\end{eqnarray}
<p>
将上式带入（1）式，将w和b消去，即可得到该问题的对偶问题：
<img src="pics/svm_2.png" alt="svm_2.png" />
</p>


\begin{eqnarray}
\nonumber
&\min_{\alpha}& \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j - \sum_{i=1}^N \alpha_i \\
\nonumber
&s.t.& \ \sum_{i=1}^N \alpha_i y_i = 0 \\
&\ & \alpha_i > 0, \ i = 1,2,...,N
\end{eqnarray}

<p>
求解上述优化需要用到SMO(Sequential Minimal Optimization)算法，在这之前需要先了解KKT条件
</p>
</div>
</div>


<div id="outline-container-org047a7f4" class="outline-4">
<h4 id="org047a7f4"><span class="section-number-4">11.2.2</span> KKT条件(Karush-Kuhn-Tucker, 库恩塔克条件)</h4>
<div class="outline-text-4" id="text-11-2-2">
<p>
针对非等式约束的优化问题，我们将其写为：
</p>

\begin{eqnarray}
\nonumber
&\min& f(X)\\
\nonumber
&s.t.& h_j(X) = 0, j=1,2,...,p \\
\nonumber
& \ \ & g_k(X) \le 0, k = 1,2,...q
\end{eqnarray}

<p>
其中p和q分别为等式和不等式约束的个数，则可以定义不等式约束下的拉格朗日函数L：
</p>

\begin{eqnarray}
\nonumber
L(X, \lambda, \alpha) = f(X) + \sum_{j=1}^p \lambda_j h_j(X) + \sum_{k=1}^q \alpha_k g_k(X)
\end{eqnarray}

<p>
则KKT条
件为：
</p>
\begin{eqnarray}
\frac{\partial L}{\partial X} |_{X=X^*} = 0 \\
\lambda_j \neq 0 \\
\alpha_k \ge 0 \\
\alpha_k g_k(X^*) =0 \\
h_j(X^*) = 0 \\
g_k(X^*) \le 0
\end{eqnarray}

<p>
其中，(1)是对拉格朗日函数取极值时候带来的一个必要条件，(2)是拉格朗日系数约束（同等式情况），(3)是不等式约束情况，(4)是互补松弛条件，(5)、(6)是原约束条件。
</p>


<p>
<code>在支持向量机中如何使用KKT条件</code>
</p>

<p>
（这一块很多书上讲的都不是非常详细，所以需要自己理解）
</p>

<p>
<code>注意</code> ：针对未对偶之前的优化函数，可以写成标准形式：
</p>


\begin{eqnarray}
\nonumber
&\min& \frac{1}{2} ||w||^2 \\
\nonumber
&s.t.& \ 1- y ^{(i)} (w^T x ^{(i)} +b) \le 0, \ i = 1,2,...,N
\end{eqnarray}

<p>
对应KKT条件中的不等式约束 \(g_i(X) = 1 - y ^{(i)} (w^T x ^{(i)} + b)\) ，而 alpha<sub>i</sub> 即为其对偶问题的决策变量，因此有：
</p>

\begin{eqnarray}
\alpha_i \ge 0 \\
1 - y ^{(i)} (w^T x ^{(i)} + b) \le 0 \\
\alpha_i (1 - y ^{(i)} (w^T x ^{(i)} + b)) = 0 
\end{eqnarray}

<p>
上式反映出，如果 alpha<sub>1</sub> = 0，则意味着样本不会对f(X)有任何影响；如果alpha<sub>i</sub> &gt; 0， 则必有 \(y ^{(i)} (w^T x ^{(i)} + b) =1\) ,所对应的样本在最大间隔边界上（结合图想一想为什么），是一个支持向量。
</p>

<p>
从而可以得到支持向量机的一个重要性质： <code>训练完成后，大部分训练样本都不需要保留，最后结果只和支持向量有关</code>
</p>
</div>
</div>


<div id="outline-container-org369417a" class="outline-4">
<h4 id="org369417a"><span class="section-number-4">11.2.3</span> SMO算法(Sequential Minimal Optimization， 序列最小化）</h4>
<div class="outline-text-4" id="text-11-2-3">
<p>
<code>坐标下降法</code> 
</p>

<p>
一次优化一个变量，固定其他所有变量，找到决策变量下对应的最优解，然后再换其他变量作为优化变量，迭代至收敛
</p>

<p>
<code>SMO算法</code>
SMO算法的思路是，每次选择两个变量： alpha<sub>i</sub> 和 alpha<sub>j</sub>， 并固定其他参数，那么初始化后，SMO将重复如下步骤直至收敛：
</p>
<ol class="org-ol">
<li>选取一对需要更新的变量 alpha<sub>i</sub>, alpha<sub>j</sub></li>
<li>固定alpha<sub>i</sub>, alpha<sub>j以外的参数</sub>，求解优化方程，获得更新后的alpha<sub>i</sub>, alpha<sub>j</sub></li>
</ol>

<p>
<code>为什么高效</code>
</p>

<p>
之所以说SMO高效，是因为优化两个参数的过程可以做到十分高效：
</p>
<ol class="org-ol">
<li>首先，另 \(\alpha_i y_i + \alpha_j y_j = c, \alpha_i \ge 0, \alpha_j \ge 0\) ，其中 \(c = -\sum_{k \neq i,j} \alpha_k y_k\) ，满足对偶问题的零和约束</li>
<li>将上市带入目标函数，消去 alpha<sub>j</sub>，只剩下alpha<sub>i的单变量二次规划问题</sub>，且仅有一个非负约束，该二次规划有闭式解</li>
</ol>

<p>
<code>与KKT条件的关系</code>
</p>

<p>
当alpha<sub>i</sub>, alpha<sub>j</sub> 中至少有一个不满足KKT条件时，目标函数就会在迭代后减小，而为了使减少速度最快，其违背KKT条件的程度也要越大
</p>

<p>
因此SMO采取了一个启发式的算法，是 <code>选取的两变量对应样本之间的间隔最大</code> ，这将会给目标函数带来更大的影响。
</p>

<p>
<code>如何得到w和b</code>
</p>

<p>
估计出所有的alpha之后，w很方便可以根据前文拉格朗日求导等于0后的公式得到，而b则是根据当前所有的支持向量分别求b后再平均得到：
</p>

\begin{eqnarray}
\nonumber
b = \frac{1}{|S|} \sum_{s \in S} (y_s - \sum_{i \in s} \alpha_i y_i x_i^T x_s)
\end{eqnarray}

<p>
其中，S为所有的支持向量集合，判断样本是否为支持向量可以根据KKT条件的公式(2)
</p>
</div>
</div>
</div>

<div id="outline-container-orgdca9974" class="outline-3">
<h3 id="orgdca9974"><span class="section-number-3">11.3</span> <span class="done DONE">DONE</span> 线性不可分支持向量机</h3>
<div class="outline-text-3" id="text-11-3">
<ul class="org-ul">
<li>State "DONE"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-18 四 15:26]</span></span></li>
</ul>
</div>
<div id="outline-container-org0d0decb" class="outline-4">
<h4 id="org0d0decb"><span class="section-number-4">11.3.1</span> 核函数</h4>
<div class="outline-text-4" id="text-11-3-1">
<p>
当样本线性不可分时，考虑将其映射到高维空间 \(x \rightarrow \phi(x)\) ，但是随之而来的是复杂的计算量，因此引进了核函数：
</p>

<p>
线性可分的情况下，无论是优化方程还是求w<sup>Tx时都要遇到求向量内积的情况</sup>，即 $&phi;(x)<sup>T</sup> &phi;(x) $ ，因此可以设想一个函数：
</p>

\begin{eqnarray}
\nonumber
k(x_i, x_j) = <\phi(x_i), \phi(x_j)> = \phi(x_i)^T \phi(x_j)
\end{eqnarray}

<p>
上述函数称为 <code>核函数</code> ，经证明，当k(·,·)是对称函数时，核矩阵K总是半正定的。换句话说，只要一个对称函数对应的核矩阵是半正定的，就可以作为核函数使用。
</p>
</div>

<div id="outline-container-org1c8c780" class="outline-5">
<h5 id="org1c8c780"><span class="section-number-5">11.3.1.1</span> 核函数的优势</h5>
<div class="outline-text-5" id="text-11-3-1-1">
<p>
由于支持向量机中所有x的运算均是求内积，因此核函数在将数据映射到高维的同时，又避免了高维x的复杂计算，仅仅是在低纬度下计算内积。
</p>
</div>
</div>

<div id="outline-container-org2e020fd" class="outline-5">
<h5 id="org2e020fd"><span class="section-number-5">11.3.1.2</span> 哪些通用核函数</h5>
<div class="outline-text-5" id="text-11-3-1-2">
<ol class="org-ol">
<li>多项式核：</li>
</ol>

\begin{eqnarray}
\nonumber
k(x_i, x_j) = (x_i^T x_j + 1)^d
\end{eqnarray}

<p>
其中i，j表示第i，j个样本
</p>

<ol class="org-ol">
<li>高斯核</li>
</ol>

\begin{eqnarray}
\nonumber
k(x_i, x_j) = exp(- \frac{||x_i - x_j||^2}{2 \sigma^2})
\end{eqnarray}

<p>
其中, sigma为带宽
</p>
<ul class="org-ul">
<li>高斯核会将原始空间映射为无穷维空间</li>
<li>如果σ选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；</li>
<li>反过来，如果σ选得很小，则可以将任意的数据映射为线性可分——可能会出现非常严重的过拟合问题</li>

<li>拉普拉斯核</li>
</ul>

\begin{eqnarray}
\nonumber
k(x_i, x_j) = exp(- \frac{||x_i - x_j||}{\sigma})
\end{eqnarray}

<p>
σ&gt;0
</p>

<ol class="org-ol">
<li>Sigmoid核</li>
</ol>

\begin{eqnarray}
\nonumber
k(x_i, x_j) = tanh(\beta x_i^T x_j + \theta)
\end{eqnarray}

<p>
tanh为双曲线正切函数，β &gt;0, θ&lt;0
</p>
</div>
</div>

<div id="outline-container-org4bf3cdb" class="outline-5">
<h5 id="org4bf3cdb"><span class="section-number-5">11.3.1.3</span> 如何选择核函数</h5>
<div class="outline-text-5" id="text-11-3-1-3">
<p>
ref: <a href="https://www.zhihu.com/question/21883548">https://www.zhihu.com/question/21883548</a>
</p>

<p>
（1）如果特征维数很高，往往线性可分（SVM解决非线性分类问题的思路就是将样本映射到更高维的特征空间中），可以采用LR或者线性核的SVM；
（2）如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM；
（3）如果不满足上述两点，即特征维数少，样本数量正常，可以使用高斯核的SVM。
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgac533f8" class="outline-3">
<h3 id="orgac533f8"><span class="section-number-3">11.4</span> <span class="done DONE">DONE</span> 线性支持向量机(软间隔与正则化)</h3>
<div class="outline-text-3" id="text-11-4">
</div>
<div id="outline-container-org1fc9aec" class="outline-4">
<h4 id="org1fc9aec"><span class="section-number-4">11.4.1</span> 软间隔</h4>
<div class="outline-text-4" id="text-11-4-1">
<p>
当样本不一定线性可分，而是存在一些误分类样本时，需要引入 “软间隔” 的概念，即允许某些样本不满足约束：
</p>

\begin{eqnarray}
\nonumber
y_i (w^T_i +b) \ge 1
\end{eqnarray}

<p>
于是，优化目标可以写成：
</p>

\begin{eqnarray}
\nonumber
\min \frac{1}{2} ||w||^2 + C \sum_{i=1}^N l_{0/1}(y_i(w^T x_i + b) - 1)
\end{eqnarray}

<p>
其中，C&gt;0
</p>

<ul class="org-ul">
<li>当C无穷大时，将迫使每个样本均满足 \(y_i (w^T_i +b) \ge 1\) 约束，于是其等价于一般形式</li>
<li>当C取有限值时，将允许一些不满足的约束</li>
</ul>

<p>
<code>软间隔SVM最大几何间隔d满足</code>
</p>

\begin{eqnarray}
\nonumber
d \ge \frac{2}{m \sqrt{C}}
\end{eqnarray}
<p>
其中m为支持向量个数，C见上文
</p>
</div>
</div>


<div id="outline-container-org56f445c" class="outline-4">
<h4 id="org56f445c"><span class="section-number-4">11.4.2</span> 损失函数</h4>
<div class="outline-text-4" id="text-11-4-2">
<p>
而l<sub>0/1</sub>为0/1损失函数，即函数值小于0时为1，否则为0。
</p>

<p>
由于该函数性质非凸，非连续的性质不好，因此引入其他类型的损失函数：
</p>

<ol class="org-ol">
<li>hinge损失: max(0, 1-x)</li>
<li>指数损失: exp(-x)</li>
<li>logistic损失: log(1+exp(-z))</li>
</ol>
</div>
</div>

<div id="outline-container-orgc64fcd6" class="outline-4">
<h4 id="orgc64fcd6"><span class="section-number-4">11.4.3</span> 加入松弛变量</h4>
<div class="outline-text-4" id="text-11-4-3">
<p>
引入松弛变量，原优化方程变为：
</p>

\begin{eqnarray}
\nonumber
&\min_{w,b,\zeta}& \frac{1}{2} ||w||^2 + C \sum_{i=1}^N \zeta_i \\
&s.t.& y_i(w^T x_i + b) \ge 1 - \zeta_i \\
&\ \ & \zeta_i \ge 0, \ i=1,2,...,N
\end{eqnarray}

<p>
这就是常用的 <code>软间隔支持向量机</code>
</p>

<p>
根据其拉格朗日函数以及偏导等于0，类似前文带入可以得到其对偶问题：
</p>


\begin{eqnarray}
\nonumber
&\min_{\alpha}& \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j - \sum_{i=1}^N \alpha_i \\
\nonumber
&s.t.& \ \sum_{i=1}^N \alpha_i y_i = 0 \\
&\ & C> \alpha_i > 0, \ i = 1,2,...,N
\end{eqnarray}

<p>
相对于线性可分支持向量机，其唯一区别就是多了一个 α 上界为C的约束
</p>
</div>

<div id="outline-container-org2b2e3f0" class="outline-5">
<h5 id="org2b2e3f0"><span class="section-number-5">11.4.3.1</span> KKT条件：</h5>
<div class="outline-text-5" id="text-11-4-3-1">
\begin{eqnarray}
\nonumber
\alpha_i \ge 0 \\
\nonumber
\mu_i \ge 0 \\
\nonumber
y_i f(x_i) - 1 + \zeta_i \ge 0 \\
\nonumber
\alpha_i (y_i f(x_i) - 1 + \zeta_i) = 0 \\
\nonumber
\zeta_i \ge 0 \\
\nonumber
\mu_i \zeta_i = 0 
\end{eqnarray} 

<p>
<code>上述条件有着重要的意义：</code>
</p>

<ol class="org-ol">
<li>α<sub>i</sub>=0时，样本不会在表示w的求和中出现，此时样本不会对 f(x<sub>i</sub>)有着任何影响，位于两个最大间隔（边界）之外，</li>
<li>α<sub>i</sub>&gt;0时，必有 y<sub>i</sub> f(x<sub>i</sub>) = 1 - &zeta;<sub>i</sub>，则该样本是支持向量</li>
</ol>
<p>
根据软间隔支持向量机拉格朗日函数对zeta求偏导后的结果：C = α<sub>i</sub>+mu<sub>i</sub>
</p>
<ol class="org-ol">
<li>α<sub>i</sub>&lt;C时，mu<sub>i</sub>&gt;0，而根据最后一个KKT条件，zeta<sub>i</sub> = 0，所以样本刚好落在边界上，为支持向量</li>
<li>α<sub>i</sub>=C时，mu<sub>I</sub>=0，此时zeta<sub>i</sub>&lt;=1，样本落在最大间隔（边界）内部</li>
</ol>

<p>
以上在求解SMO算法时有着重要的意义
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org008bbbe" class="outline-3">
<h3 id="org008bbbe"><span class="section-number-3">11.5</span> <span class="todo SOMEDAY">SOMEDAY</span> 支持向量回归</h3>
</div>

<div id="outline-container-org51f8a39" class="outline-3">
<h3 id="org51f8a39"><span class="section-number-3">11.6</span> <span class="todo TODO">TODO</span> 推导过程</h3>
<div class="outline-text-3" id="text-11-6">
</div>
<div id="outline-container-org0e7a114" class="outline-4">
<h4 id="org0e7a114"><span class="section-number-4">11.6.1</span> 几何间隔推导出优化方程一般形式</h4>
</div>
<div id="outline-container-org02dbf52" class="outline-4">
<h4 id="org02dbf52"><span class="section-number-4">11.6.2</span> 拉格朗日对偶问题推导</h4>
</div>
<div id="outline-container-orge848e6f" class="outline-4">
<h4 id="orge848e6f"><span class="section-number-4">11.6.3</span> KKT条件推导</h4>
</div>
<div id="outline-container-org4b4ff63" class="outline-4">
<h4 id="org4b4ff63"><span class="section-number-4">11.6.4</span> SMO算法推导</h4>
</div>
<div id="outline-container-org0822bae" class="outline-4">
<h4 id="org0822bae"><span class="section-number-4">11.6.5</span> 核函数推导</h4>
</div>
<div id="outline-container-org829b139" class="outline-4">
<h4 id="org829b139"><span class="section-number-4">11.6.6</span> 软间隔推导</h4>
</div>
</div>
</div>


<div id="outline-container-orgd9b54ee" class="outline-2">
<h2 id="orgd9b54ee"><span class="section-number-2">12</span> <span class="done DONE">DONE</span> 文本挖掘</h2>
<div class="outline-text-2" id="text-12">
</div>
<div id="outline-container-orgcadeb03" class="outline-3">
<h3 id="orgcadeb03"><span class="section-number-3">12.1</span> word2vec</h3>
<div class="outline-text-3" id="text-12-1">
<p>
<code>以往的词向量方法：</code>
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-right">word1</td>
<td class="org-right">word2</td>
<td class="org-left">&#x2026;</td>
</tr>

<tr>
<td class="org-left">sample1</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">sample2</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">&#x2026;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
这将导致矩阵非常稀疏，内存压力非常大，而且词与词之间的关系不明显
</p>

<p>
<code>word2vec提供的词向量方法</code>
（数字是我瞎编的）
1.首先确定特征数目n,构建如下词向量:
<code>这是word2vec模型的本质方法</code>
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-right">word1</td>
<td class="org-right">word2</td>
<td class="org-left">&#x2026;</td>
</tr>

<tr>
<td class="org-left">1</td>
<td class="org-right">0.123</td>
<td class="org-right">0.567</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">2</td>
<td class="org-right">0.839</td>
<td class="org-right">0.283</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">&#x2026;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">n</td>
<td class="org-right">0.657</td>
<td class="org-right">0.911</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>

<ol class="org-ol">
<li>对每个样本（句子）的词向量进行加权平均（比如用IDF：总文件数除以包含该词的数目，或者tf词频），得到维度为n的特征：</li>
</ol>
<p>
<code>该方法只有在做机器学习模型时用到</code>
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-left">&#x2026;</td>
<td class="org-right">n</td>
</tr>

<tr>
<td class="org-left">sample1</td>
<td class="org-right">0.53</td>
<td class="org-right">0.32</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">0.33</td>
</tr>

<tr>
<td class="org-left">sample2</td>
<td class="org-right">0.98</td>
<td class="org-right">0.12</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">0.44</td>
</tr>

<tr>
<td class="org-left">&#x2026;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
<code>word2vec特点</code>
能够发现语法关系，比如以下词的词频向量近似满足如下关系：
</p>
<ul class="org-ul">
<li>“biggest”-“big”+“small”=“smallest”</li>
<li>“国王” - “男性” + “女性” = “王后”</li>
</ul>

<p>
<code>word2vec不足</code>
</p>
<ul class="org-ul">
<li>只考虑了上下文单词存在与否，没考虑上下文单词的顺序（对应Doc2vec进行了改进）</li>
</ul>
</div>

<div id="outline-container-org6f51d36" class="outline-4">
<h4 id="org6f51d36"><span class="section-number-4">12.1.1</span> 模型</h4>
<div class="outline-text-4" id="text-12-1-1">
</div>
<div id="outline-container-org409a17b" class="outline-5">
<h5 id="org409a17b"><span class="section-number-5">12.1.1.1</span> CBOW模型</h5>
<div class="outline-text-5" id="text-12-1-1-1">
<p>
通过一个三层神经网络实现，确定每个词的词向量
</p>

<ul class="org-ul">
<li>输入层： 特征词上下文相关的词对应的词向量（ <code>初始随机的n维向量</code> ），词的总数我们成为 <code>窗口大小</code> ，如果窗口大小为8，那么输入层神经元数目为8</li>
<li>隐含层：一般为1个隐含层即可</li>
<li>输出层：输出所有词的softmax概率，为一个向量，长度为词汇表（所有单词）大小</li>
</ul>

<p>
<code>最终的词向量由神经网络中的权重得到</code>
</p>
</div>
</div>

<div id="outline-container-org21395d8" class="outline-5">
<h5 id="org21395d8"><span class="section-number-5">12.1.1.2</span> Skip-Gram模型</h5>
<div class="outline-text-5" id="text-12-1-1-2">
<p>
同样通过三层神经网络实现，但是思路与CBOW模型刚好相反
-输入层：特征词的词向量
-输出层：softmax概率前8（8为窗口大小）的词向量
</p>
</div>
</div>

<div id="outline-container-orgd277c35" class="outline-5">
<h5 id="orgd277c35"><span class="section-number-5">12.1.1.3</span> word2vec的改进</h5>
<div class="outline-text-5" id="text-12-1-1-3">
<p>
<code>引入霍夫曼树</code>
</p>
</div>
</div>
</div>

<div id="outline-container-orgfb4d211" class="outline-4">
<h4 id="orgfb4d211"><span class="section-number-4">12.1.2</span> 超参数与调参</h4>
<div class="outline-text-4" id="text-12-1-2">
<p>
超参数：
</p>
<ul class="org-ul">
<li>模型选择 skip-gram慢，对罕见字有利；CBOW快</li>
<li>单词向量维度n</li>
<li>训练窗口大小，如果为5，表示前5个和后5个，skip通常在10个左右，CBOW在5附近</li>
<li>采样阈值，过滤掉频率过小的词</li>
<li>学习率，神经网络梯度下降系数</li>
<li>训练算法，分层softmax，对罕见字有利；负采样，对常见词和低维向量有利</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org86e9aab" class="outline-3">
<h3 id="org86e9aab"><span class="section-number-3">12.2</span> doc2vec</h3>
<div class="outline-text-3" id="text-12-2">
<p>
word2vec忽略了前后词排列顺序的影响，而doc2vec没有
</p>

<p>
<code>通过新增了一个段落向量实现</code>
</p>
</div>
</div>
</div>

<div id="outline-container-org321cb63" class="outline-2">
<h2 id="org321cb63"><span class="section-number-2">13</span> <span class="done DONE">DONE</span> 聚类算法<code>[3/3]</code></h2>
<div class="outline-text-2" id="text-13">
<ul class="org-ul">
<li>State "DONE"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-04-04 三 16:06]</span></span></li>
<li>State "TODO"       from "DONE"       <span class="timestamp-wrapper"><span class="timestamp">[2018-04-04 三 15:16]</span></span></li>
<li>State "DONE"       from "TODO"       <span class="timestamp-wrapper"><span class="timestamp">[2018-01-22 一 21:20]</span></span></li>
<li>State "TODO"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2018-01-22 一 16:32]</span></span></li>
</ul>
</div>
<div id="outline-container-orgfd750a7" class="outline-3">
<h3 id="orgfd750a7"><span class="section-number-3">13.1</span> <span class="done DONE">DONE</span> Kmeans</h3>
<div class="outline-text-3" id="text-13-1">
<ul class="org-ul">
<li>State "TODO"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2018-01-22 一 16:31]</span></span></li>
</ul>
</div>
</div>

<div id="outline-container-org9e04321" class="outline-3">
<h3 id="org9e04321"><span class="section-number-3">13.2</span> <span class="done DONE">DONE</span> 层次聚类</h3>
<div class="outline-text-3" id="text-13-2">
<ul class="org-ul">
<li>State "TODO"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2018-01-22 一 16:31]</span></span></li>
</ul>
</div>
</div>

<div id="outline-container-orge0732b2" class="outline-3">
<h3 id="orge0732b2"><span class="section-number-3">13.3</span> <span class="done DONE">DONE</span> DBSCAN聚类</h3>
<div class="outline-text-3" id="text-13-3">
<p>
DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)
</p>

<p>
ref: <a href="https://www.cnblogs.com/pinard/p/6208966.html">https://www.cnblogs.com/pinard/p/6208966.html</a>
</p>

<p>
<code>密度直达</code> ， <code>密度可达</code>
</p>

<p>
<code>优点：</code>
</p>
<ol class="org-ol">
<li>具有噪声，不会对所有点聚类，对异常值不敏感</li>
<li>既可以对凸样本聚类，又可以对凹样本聚类</li>
<li>聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</li>
</ol>

<p>
<code>缺点：</code> 
</p>
<ol class="org-ol">
<li>如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。</li>
<li>如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org42b745f" class="outline-2">
<h2 id="org42b745f"><span class="section-number-2">14</span> <span class="done DONE">DONE</span> 数值优化专题</h2>
<div class="outline-text-2" id="text-14">
<ul class="org-ul">
<li>State "DONE"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2018-01-25 四 19:11]</span></span></li>
</ul>
<p>
ref :
</p>
<ul class="org-ul">
<li><a href="http://blog.csdn.net/fangqingan_java/article/details/46289691">http://blog.csdn.net/fangqingan_java/article/details/46289691</a></li>
<li><a href="http://www.hankcs.com/ml/l-bfgs.html">http://www.hankcs.com/ml/l-bfgs.html</a></li>
</ul>
</div>

<div id="outline-container-orgfbc2c84" class="outline-3">
<h3 id="orgfbc2c84"><span class="section-number-3">14.1</span> 预备知识</h3>
<div class="outline-text-3" id="text-14-1">
</div>
<div id="outline-container-org5f16995" class="outline-4">
<h4 id="org5f16995"><span class="section-number-4">14.1.1</span> 损失函数</h4>
<div class="outline-text-4" id="text-14-1-1">
<p>
损失函数用来描述模型的预测值和真实值的不一致程度，它是一个 <code>非负实值函数</code> ，一般要求对其求最小化，一般损失函数表示为：
</p>

\begin{equation}
L(Y, f(x)) = \sum_{i=1}^N l(y_i, f(x_i))
\end{equation}

<p>
<code>损失函数和代价函数的区别：</code>
</p>

<ul class="org-ul">
<li>损失函数针对一个样本</li>
<li>代价函数针对多个样本，且一般以平均损失的形式展现</li>
</ul>

<p>
常见的损失函数有如下几种：
</p>
</div>

<div id="outline-container-orge42e261" class="outline-5">
<h5 id="orge42e261"><span class="section-number-5">14.1.1.1</span> 0-1损失(Binary Loss)</h5>
<div class="outline-text-5" id="text-14-1-1-1">
<ul class="org-ul">
<li>y<sub>i</sub> = f(x<sub>i</sub>)时为1</li>
<li>否则为0</li>
</ul>
</div>
</div>

<div id="outline-container-org1d960d8" class="outline-5">
<h5 id="org1d960d8"><span class="section-number-5">14.1.1.2</span> 感知损失（Perceptron Loss）</h5>
<div class="outline-text-5" id="text-14-1-1-2">
<ul class="org-ul">
<li>|y<sub>i</sub> - f(x<sub>i</sub>)| &gt; t 时为1</li>
<li>否则为0</li>
</ul>
</div>
</div>

<div id="outline-container-org6853dca" class="outline-5">
<h5 id="org6853dca"><span class="section-number-5">14.1.1.3</span> Hinge Loss</h5>
<div class="outline-text-5" id="text-14-1-1-3">
<p>
Hinge 损失用来解决间隔最大化的问题，比如在svm中解决几何间隔最大化
</p>

<p>
定义为 l<sub>i</sub> = max(0, 1 - y<sub>i</sub>*f(x<sub>i</sub>))     y<sub>i</sub> 为-1或+1
</p>
</div>
</div>

<div id="outline-container-orgbd8bd4e" class="outline-5">
<h5 id="orgbd8bd4e"><span class="section-number-5">14.1.1.4</span> 对数损失</h5>
<div class="outline-text-5" id="text-14-1-1-4">
<p>
在极大似然估计的情况下，由于是连乘的形式处理起来不方便，因此取对数，转为连加，比如logistic回归
</p>

<p>
l<sub>i</sub> = y<sub>i</sub> * log(f(x<sub>i</sub>)) + (1-y<sub>i</sub>) * log(1 - f(x<sub>i</sub>))       y<sub>i为0或者1</sub>
</p>
</div>
</div>

<div id="outline-container-orgd0f559b" class="outline-5">
<h5 id="orgd0f559b"><span class="section-number-5">14.1.1.5</span> 平方损失</h5>
<div class="outline-text-5" id="text-14-1-1-5">
<p>
不多解释
</p>
</div>
</div>

<div id="outline-container-orga5a91f9" class="outline-5">
<h5 id="orga5a91f9"><span class="section-number-5">14.1.1.6</span> 绝对损失(Absolute Loss)</h5>
<div class="outline-text-5" id="text-14-1-1-6">
<p>
l<sub>i</sub> = |y<sub>i</sub> - f(x<sub>i</sub>)|
</p>
</div>
</div>

<div id="outline-container-org03b877e" class="outline-5">
<h5 id="org03b877e"><span class="section-number-5">14.1.1.7</span> 指数损失</h5>
<div class="outline-text-5" id="text-14-1-1-7">
<p>
adaboost用的就是指数损失（推导暂时不要求掌握）
</p>

<p>
<code>注意：指数损失必须是二分类问题</code>
l<sub>i</sub> = exp(- y<sub>i</sub> * f(x<sub>i</sub>))       y<sub>i</sub> 为 -1 或 +1
</p>
</div>
</div>
</div>





<div id="outline-container-org66e1a22" class="outline-4">
<h4 id="org66e1a22"><span class="section-number-4">14.1.2</span> 函数几个重要的点</h4>
<div class="outline-text-4" id="text-14-1-2">
</div>
<div id="outline-container-org0e8aa64" class="outline-5">
<h5 id="org0e8aa64"><span class="section-number-5">14.1.2.1</span> 拐点</h5>
<div class="outline-text-5" id="text-14-1-2-1">
<p>
二阶导数等于0，凹凸性改变
</p>
</div>
</div>
<div id="outline-container-org4e32e38" class="outline-5">
<h5 id="org4e32e38"><span class="section-number-5">14.1.2.2</span> 极值点</h5>
<div class="outline-text-5" id="text-14-1-2-2">
<p>
驻点要求一阶导数必须存在，而极值点对导数没有要求
</p>
</div>
</div>
<div id="outline-container-org461012d" class="outline-5">
<h5 id="org461012d"><span class="section-number-5">14.1.2.3</span> 驻点</h5>
<div class="outline-text-5" id="text-14-1-2-3">
<p>
一阶导数等于0，单调性改变
</p>
</div>
</div>
<div id="outline-container-org29ba8e5" class="outline-5">
<h5 id="org29ba8e5"><span class="section-number-5">14.1.2.4</span> 鞍点（saddle point）</h5>
<div class="outline-text-5" id="text-14-1-2-4">
<p>
目标函数在此点上的梯度（一阶导数）值为 0， 但从改点出发的一个方向是函数的极大值点，而在另一个方向是函数的极小值点。
</p>

<p>
判断鞍点的一个充分条件是：函数在一阶导数为零处（驻点）的海塞矩阵为不定矩阵(特征值有正有负)。
</p>

<p>
<code>补充</code>
</p>

<p>
实对称矩阵正交相似于对角矩阵
即与对角矩阵合同
而对角矩阵的主对角线上的元素即A的特征值
所以对称矩阵A正定 A的特征值都大于0
</p>
</div>
</div>
</div>


<div id="outline-container-org2871ac2" class="outline-4">
<h4 id="org2871ac2"><span class="section-number-4">14.1.3</span> 梯度和海塞矩阵</h4>
<div class="outline-text-4" id="text-14-1-3">
<p>
梯度是指原函数对参数的一阶偏导
</p>

<p>
海塞矩阵是对参数的二阶偏导组合，为KxK维矩阵，K为参数个数
</p>
</div>
</div>
</div>

<div id="outline-container-orgceedb86" class="outline-3">
<h3 id="orgceedb86"><span class="section-number-3">14.2</span> 优化方法</h3>
<div class="outline-text-3" id="text-14-2">
</div>
<div id="outline-container-orgeba8a8e" class="outline-4">
<h4 id="orgeba8a8e"><span class="section-number-4">14.2.1</span> 优化问题划分：</h4>
<div class="outline-text-4" id="text-14-2-1">
</div>
<div id="outline-container-org9fa40e5" class="outline-5">
<h5 id="org9fa40e5"><span class="section-number-5">14.2.1.1</span> 凸优化</h5>
<div class="outline-text-5" id="text-14-2-1-1">
<ul class="org-ul">
<li>什么是凸集</li>
<li>什么是凸函数</li>
<li>什么是凸优化</li>
</ul>

<p>
<code>对于凸优化问题，任何局部最优解都是全局最优解！！</code>
</p>
</div>
</div>

<div id="outline-container-org7b952f6" class="outline-5">
<h5 id="org7b952f6"><span class="section-number-5">14.2.1.2</span> 无约束最优化</h5>
<div class="outline-text-5" id="text-14-2-1-2">
<ul class="org-ul">
<li>GD</li>
<li>SGD</li>
<li>TR</li>
<li>CG</li>
<li>Newton</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>
</div>
</div>

<div id="outline-container-org226d727" class="outline-5">
<h5 id="org226d727"><span class="section-number-5">14.2.1.3</span> 约束最优化</h5>
<div class="outline-text-5" id="text-14-2-1-3">
<ul class="org-ul">
<li>KKT条件</li>
</ul>
</div>
</div>

<div id="outline-container-orgb77c61a" class="outline-5">
<h5 id="orgb77c61a"><span class="section-number-5">14.2.1.4</span> 局部最优化</h5>
<div class="outline-text-5" id="text-14-2-1-4">
<p>
几个要记住的定理：
<img src="pics/optimal.png" alt="optimal.png" />
</p>
</div>
</div>
</div>



<div id="outline-container-org8d55957" class="outline-4">
<h4 id="org8d55957"><span class="section-number-4">14.2.2</span> 详细的优化方法：</h4>
<div class="outline-text-4" id="text-14-2-2">
</div>
<div id="outline-container-org2a16318" class="outline-5">
<h5 id="org2a16318"><span class="section-number-5">14.2.2.1</span> 坐标下降</h5>
<div class="outline-text-5" id="text-14-2-2-1">
<p>
变动一个参数，保持其余参数不变，找到该参数最优解，不断迭代至参数不变
</p>

<p>
SMO算法是变动两个参数，固定其他，来求解
</p>
</div>
</div>

<div id="outline-container-orga6df1e9" class="outline-5">
<h5 id="orga6df1e9"><span class="section-number-5">14.2.2.2</span> 梯度下降</h5>
<div class="outline-text-5" id="text-14-2-2-2">
<p>
参数每次迭代均按照该参数偏导的负数，乘一定步长作为增量
</p>

<p>
证明 <code>梯度下降可以找到极小值</code> ：
</p>

<p>
f(x)在点x的一阶泰勒展开为：
</p>

\begin{eqnarray}
\nonumber
f(x + \Delta x) &=& f(x) + \Delta x^T \frac{\partial f(x)}{\partial x} \\ 
\nonumber
f(x + \alpha p) &=& f(x) + \alpha * g(x) * p + o(\alpha * |p|) 
\end{eqnarray}

<p>
而：
</p>

\begin{eqnarray}
\nonumber
g(x) * p = |g(x)| * |p| * cos \theta
\end{eqnarray}

<p>
当 θ取180°时取最小值，且为负，保证了每次迭代f(x)都会减小。
</p>

<p>
<code>如果是凸优化，根据定理，可以找到最小值</code>
</p>

<p>
<code>在机器学习中的应用：</code>
</p>

<p>
梯度下降针对的是求和形式的优化问题：
</p>

\begin{eqnarray}
\nonumber
f(w) = \sum_{i=1}^N f_i(w, x_i, y_i)
\end{eqnarray}

<p>
提的下降形式为：
</p>

\begin{eqnarray}
w_{t+1} = w_t - \eta_{t+1} \sum_{i=1}^N \nabla f_i(w_t, x_i, y_i) \tag{(1)}
\end{eqnarray}

<p>
其中 w<sub>t</sub>，w<sub>t+1</sub>，&nabla; f<sub>i</sub> 均为列向量，长度等于变量数，t为第t期的值，i为第i个样本，&nabla;<sub>f</sub><sub>i</sub>(w<sub>t</sub>, x<sub>i</sub>, y<sub>i</sub>)表示f在第i个样本下的梯度向量。
</p>
</div>
</div>

<div id="outline-container-org0fab132" class="outline-5">
<h5 id="org0fab132"><span class="section-number-5">14.2.2.3</span> 随机梯度下降</h5>
<div class="outline-text-5" id="text-14-2-2-3">
<p>
由于梯度下降需要计算每个样本的梯度向量，样本量大时非常复杂，因此引入梯度下降，每次只需随机抽取一个样本进行更新：
</p>

\begin{eqnarray}
w_{t+1} = w_t - \eta_{t+1} \nabla f_i(w_t, x_i, y_i) \tag{(2)}
\end{eqnarray}

<p>
其中i为从1到N中随机抽取的样本
</p>

<p>
随机梯度下降提高了速度，但是降低了精度(极值处梯度不为0)。
</p>

<p>
后来提出的 <code>SAG，SVRG，SDCA</code> 都是在降低方差，使其可以精确收敛
</p>

<p>
“不在大型数据集上使用L-BFGS的原因之一是，在线算法可能收敛得更快。这里甚至有一个L-BFGS的在线学习算法，但据我所知，在大型数据集上它们都不如一些SGD的改进算法（包括 AdaGrad 或 AdaDelta）的表现好。”
</p>
</div>
</div>

<div id="outline-container-org82e96ac" class="outline-5">
<h5 id="org82e96ac"><span class="section-number-5">14.2.2.4</span> 动量梯度下降法</h5>
<div class="outline-text-5" id="text-14-2-2-4">
\begin{eqnarray}
\nonumber
v_{dW} &=& \beta v_{dW} + (1 - \beta)dW \\
v_{db} &=& \beta v_{db} + (1 - \beta)dv \\
W &=& W - \alpha v_{dW} \\ 
b &=& b - \alpha v_{db} 
\end{eqnarray}

<p>
其中 &beta; 为超参数，一般取0.9，表示平均了前 1/(1-&beta;) 期的梯度变化v。
</p>

<p>
该方法避免了梯度在下降过程中左右摇摆、速度慢的问题，相当于在原来的梯度的基础上，加上了以前的梯度，并且以 &beta; 加权
</p>
</div>
</div>

<div id="outline-container-org2f82317" class="outline-5">
<h5 id="org2f82317"><span class="section-number-5">14.2.2.5</span> GradDelta &amp; AdaDelta</h5>
<div class="outline-text-5" id="text-14-2-2-5">
<p>
<code>AdaGrade</code>
</p>
\begin{eqnarray}
\nonumber
w_{t+1, i} = w_{t,i} - \frac{\eta}{\sqrt{G_{t,i} + \epsilon}} dw
\end{eqnarray}

<p>
其中，G<sub>t,i</sub> 为第i个参数，t时刻之前的历史所有梯度的累加平方和矩阵，\epsilon是平滑项，防止除0
</p>

<p>
<code>优势</code> ： 学习率不断的变小，避免了手动调节学习率。缺点：由于分母不断变大，到后面学习能力越来越弱，因此提出了AdaDelta
</p>

<p>
<code>AdaDelta</code>
考虑了一个时间窗口，而不是将所有的历史梯度累加
</p>
</div>
</div>


<div id="outline-container-org30c091a" class="outline-5">
<h5 id="org30c091a"><span class="section-number-5">14.2.2.6</span> 共轭梯度法</h5>
<div class="outline-text-5" id="text-14-2-2-6">
<p>
ref ： 
</p>
<ol class="org-ol">
<li><a href="http://blog.csdn.net/lipengcn/article/details/52698895">http://blog.csdn.net/lipengcn/article/details/52698895</a></li>
<li><a href="https://www.zhihu.com/question/27157047">https://www.zhihu.com/question/27157047</a></li>
<li><a href="https://blog.csdn.net/fangqingan_java/article/details/47011943">https://blog.csdn.net/fangqingan_java/article/details/47011943</a></li>
</ol>
</div>

<ol class="org-ol">
<li><a id="org9326683"></a>什么是共轭<br />
<div class="outline-text-6" id="text-14-2-2-6-1">
<p>
复数中的共轭： \(z=a+bi\) 的复共轭（简称共轭）为 \(\bar{z} = a-bi\) ，常数的共轭为其自身
</p>

<p>
向量集合共轭：给定向量集合 p 和对称正定矩阵A， \(p_i^T A p_j = 0, \ i \neq j\) ， <code>此时p之间是线性独立的</code>
</p>
</div>
</li>

<li><a id="orgbfd5d22"></a>什么是共轭矩阵<br />
<div class="outline-text-6" id="text-14-2-2-6-2">
<p>
我们称 A 和 A* 互为共轭矩阵当且仅当：
</p>

\begin{eqnarray}
\nonumber
(A*)_{i,j} = \bar{A_{j,i}}
\end{eqnarray}
</div>
</li>

<li><a id="org1973795"></a>什么是共轭梯度<br />
<div class="outline-text-6" id="text-14-2-2-6-3">
<p>
共轭梯度算法解决了一类特殊的优化问题：
</p>

\begin{eqnarray}
\nonumber
\min{\phi(x)} = \frac{1}{2} x^T A x - b^T x
\end{eqnarray}


<p>
推导见ref2. 详细算法见ref3.
</p>

<p>
算法包括了：
</p>
<ul class="org-ul">
<li>CG-Preliminary算法</li>
<li>CG算法</li>
</ul>

<p>
<code>本质上，就是把目标函数分成许多方向，然后不同方向分别求出极值再综合起来，方向之间两两共轭确保了每个方向的极值将得到最终的极值</code>
</p>
</div>
</li>

<li><a id="orgdeefffe"></a>共轭梯度优点<br />
<div class="outline-text-6" id="text-14-2-2-6-4">
<ul class="org-ul">
<li>仅仅需要一阶梯度</li>
<li>存储量小</li>
<li>收敛快</li>
<li>稳定性高</li>
</ul>
</div>
</li>
</ol>
</div>


<div id="outline-container-orgf9a61d6" class="outline-5">
<h5 id="orgf9a61d6"><span class="section-number-5">14.2.2.7</span> 牛顿法</h5>
<div class="outline-text-5" id="text-14-2-2-7">
<p>
梯度下降是进行一阶泰勒展开，而共轭梯度法则是进行二阶泰勒展开
</p>


\begin{eqnarray}
\nonumber
f(x + \Delta x) &=& f(x) + \Delta x^T \frac{\partial f(x)}{\partial x} + \frac{1}{2} \Delta x^T H_n \Delta x\\ 
\end{eqnarray}

<p>
我们需要找一个 Delta x ，使得f(x)在x出最小，将上式对 Delta x求偏导，并且令他等于0，得到：
</p>

\begin{eqnarray}
\nonumber
\frac{\partial f(x + \Delta x )}{\partial \Delta x} = g_n + H_n \Delta x = 0 \\
\nonumber
\Delta x = - H^{-1}_n g_n
\end{eqnarray}

<p>
所以牛顿法的迭代式为：
</p>

\begin{eqnarray}
\nonumber
x_{n+1} = x_n - \alpha (H_n^{-1} g_n)
\end{eqnarray}

<p>
这其中牵扯到海塞矩阵以及其求逆的形式，如果数据维度过大，将导致难以存储和计算
</p>

<p>
α为步长，应当越来越小，可以直接令其等于优化方程的值(backtracking line search)
</p>
</div>
</div>

<div id="outline-container-org8a121d9" class="outline-5">
<h5 id="org8a121d9"><span class="section-number-5">14.2.2.8</span> 拟牛顿法</h5>
<div class="outline-text-5" id="text-14-2-2-8">
<p>
由于维度过大时海塞矩阵的逆难以计算，拟牛顿法提出了一个对H<sup>-1</sup>的近似求法
ref: <a href="http://www.hankcs.com/ml/l-bfgs.html">http://www.hankcs.com/ml/l-bfgs.html</a>
</p>

<p>
<code>拟牛顿条件</code> 
</p>

\begin{eqnarray}
\nonumber
H_n (x_n - x_{n-1}) = (g_n - g_{n-1}) \\
\nonumber
H_n s_n = y_n \\
\nonumber
H_n^{-1} y_n = s_n
\end{eqnarray}

<p>
其中g为梯度，H为海塞矩阵，他保证了H<sub>n+1</sub> 至少对x<sub>n</sub> - x<sub>n-1</sub>是近似海塞矩阵
</p>

<p>
<code>对称性条件</code>
海塞矩阵的近似也要是对称矩阵
</p>
</div>

<ol class="org-ol">
<li><a id="orgcdb7255"></a>BFGS<br />
<div class="outline-text-6" id="text-14-2-2-8-1">
<p>
可以推得：
</p>

\begin{eqnarray}
\nonumber
H_{n+1}^{-1} = (I - \rho_n y_n s_n^T) H_n^{-1} (I - \rho_n s_n y_n^T) + \rho_n s_n s_n^T, \qquad \rho_n = (y_n^T s_n)^{-1}
\end{eqnarray}

<p>
<code>注意点</code>
</p>
<ol class="org-ol">
<li>只要H<sub>n</sub><sup>-1</sup> 正定， H<sub>n+1</sub><sup>-1</sup>就一定正定，所以只需要选择一个H<sub>0</sub><sup>-1</sup>即可，甚至可以是单位矩阵</li>
<li>H<sub>n+1</sub><sup>-1</sup>加上s<sub>n</sub>，y<sub>n</sub> 可倒推出H<sub>n</sub><sup>-1</sup></li>
</ol>


<div class="figure">
<p><img src="pics/BFGS.png" alt="BFGS.png" />
</p>
</div>
</div>
</li>


<li><a id="orgb09fa3c"></a>L-BFGS<br />
<div class="outline-text-6" id="text-14-2-2-8-2">
<p>
BFGS仍然需要每次迭代 s<sub>n</sub>, y<sub>n</sub> 并没有减小内存的负担，而 Limit-BFGS 每次只适用最近的m个 s<sub>n</sub>, y<sub>n</sub> 因此只储存m个样本。
</p>
</div>
</li>
</ol>
</div>
</div>
</div>





<div id="outline-container-orge26dd43" class="outline-3">
<h3 id="orge26dd43"><span class="section-number-3">14.3</span> 程序编写</h3>
</div>
</div>

<div id="outline-container-org05494f8" class="outline-2">
<h2 id="org05494f8"><span class="section-number-2">15</span> <span class="todo TODO">TODO</span> 调参专题</h2>
<div class="outline-text-2" id="text-15">
<ul class="org-ul">
<li>State "TODO"       from ""           <span class="timestamp-wrapper"><span class="timestamp">[2018-04-04 三 16:06]</span></span></li>
</ul>
</div>

<div id="outline-container-orgdbdb1d9" class="outline-3">
<h3 id="orgdbdb1d9"><span class="section-number-3">15.1</span> 网格搜索</h3>
<div class="outline-text-3" id="text-15-1">
<p>
GridSearch ，给定参数上下界和步长，不断地组合和尝试
用于：
</p>
<ul class="org-ul">
<li>超参数数量少</li>
<li>模型训练时间较短</li>
</ul>
</div>
</div>

<div id="outline-container-orgee8b987" class="outline-3">
<h3 id="orgee8b987"><span class="section-number-3">15.2</span> 随机搜索</h3>
<div class="outline-text-3" id="text-15-2">
<p>
给定参数上下界，在内部随机进行参数选取
</p>
</div>
</div>

<div id="outline-container-org0817372" class="outline-3">
<h3 id="org0817372"><span class="section-number-3">15.3</span> 基于梯度的优化</h3>
<div class="outline-text-3" id="text-15-3">
<p>
<a href="http://fa.bianp.net/blog/2016/hyperparameter-optimization-with-approximate-gradient/">http://fa.bianp.net/blog/2016/hyperparameter-optimization-with-approximate-gradient/</a>
<a href="https://arxiv.org/abs/1502.03492">https://arxiv.org/abs/1502.03492</a>
</p>
</div>
</div>

<div id="outline-container-org2e3a115" class="outline-3">
<h3 id="org2e3a115"><span class="section-number-3">15.4</span> 坐标下降</h3>
<div class="outline-text-3" id="text-15-4">
<p>
ref： <a href="https://www.zhihu.com/question/34470160">https://www.zhihu.com/question/34470160</a>
</p>
</div>
<div id="outline-container-orge5dbcda" class="outline-4">
<h4 id="orge5dbcda"><span class="section-number-4">15.4.1</span> 调整过程影响类参数</h4>
<div class="outline-text-4" id="text-15-4-1">
<p>
如果是随机森林，那么只有子分类器数目
如果是GBDT，那么有子分类器数目和学习率
</p>
</div>
</div>

<div id="outline-container-org175b52b" class="outline-4">
<h4 id="org175b52b"><span class="section-number-4">15.4.2</span> 调整子模型影响类参数</h4>
<div class="outline-text-4" id="text-15-4-2">
<p>
包括分类条件（gini或信息增益）、最大特征数、最大深度、分裂最小样本数、叶节点最小样本数、最大叶节点数等等
</p>

<p>
如果发生抖动情况，即效果和参数数值不存在明显关系，则忽略该参数的调整
</p>

<p>
<code>数据和特征决定了机器学习的上限</code> 调参只能逼近上限
</p>
</div>
</div>
</div>

<div id="outline-container-orgf418393" class="outline-3">
<h3 id="orgf418393"><span class="section-number-3">15.5</span> 贝叶斯优化</h3>
<div class="outline-text-3" id="text-15-5">
<p>
bayes<sub>opt</sub>: <a href="https://github.com/fmfn/BayesianOptimization">https://github.com/fmfn/BayesianOptimization</a>
</p>

<p>
每次训练参数时都考虑前面所有参数提供的先验信息，并且在完成之后，通过最大后验给出最优参数，并再次更新该先验。
</p>
</div>
</div>
</div>

<div id="outline-container-orgd78e5e8" class="outline-2">
<h2 id="orgd78e5e8"><span class="section-number-2">16</span> 神经网络与深度学习</h2>
<div class="outline-text-2" id="text-16">
</div>
<div id="outline-container-org08f578f" class="outline-3">
<h3 id="org08f578f"><span class="section-number-3">16.1</span> 神经网络与深度学习简介</h3>
<div class="outline-text-3" id="text-16-1">
<ul class="org-ul">
<li>神经网络无需赘述</li>
<li>"深度学习"是为了让层数较多的多层神经网络可以训练，能够work而演化出来的一系列的 新的结构和新的方法。</li>
</ul>

<p>
新的网络结构中最著名的就是CNN，它解决了传统较深的网络参数太多，很难训练的问题，使用了“局部感受野”和“权植共享”的概念，大大减少了网络参数的数量
</p>

<ul class="org-ul">
<li>原来多层神经网络做的步骤是：特征映射到值。特征是人工挑选。</li>
<li>深度学习做的步骤是 信号-&gt;特征-&gt;值。 特征是由网络自己选择。</li>
</ul>
</div>
</div>

<div id="outline-container-org97392e3" class="outline-3">
<h3 id="org97392e3"><span class="section-number-3">16.2</span> 感知机学习</h3>
<div class="outline-text-3" id="text-16-2">
<p>
感知机是一个模仿神经元的模型
</p>

<p>
[此处有图(自己想象)]
</p>

<p>
接受多个输入（x1，x2，x3&#x2026;），产生一个输出（output），超参数为阈值，待估参数为每个x的权重
</p>

<p>
当加权和大于阈值时，信号激活，输出1，否则输出0
</p>
</div>
</div>


<div id="outline-container-org060c179" class="outline-3">
<h3 id="org060c179"><span class="section-number-3">16.3</span> 一般神经网络</h3>
<div class="outline-text-3" id="text-16-3">
<p>
实际决策中，模型要复杂得多，由多个感知机组成，可能是多层的结构，甚至有多个输出。
</p>

<p>
神经网络需要在给定输入和输出下，估计出每个神经元最优的权重向量w和阈值b(-threshold)
</p>

<p>
但是，如果每个神经元输出结果是0或者1，将会使结果过于敏感，因此要通过sigmoid函数将其转为连续输出
</p>
</div>

<div id="outline-container-org644a772" class="outline-4">
<h4 id="org644a772"><span class="section-number-4">16.3.1</span> 激活函数</h4>
<div class="outline-text-4" id="text-16-3-1">
<ol class="org-ol">
<li>sigmoid</li>
</ol>
<p>
优点：求导方便
缺点：计算量大，易导致梯度消失
</p>
<ol class="org-ol">
<li>tanh</li>
</ol>
<p>
优点：零均值，在特征相差明显时效果较好，比sigmoid效果好
缺点：计算量大，收敛速度慢
</p>
<ol class="org-ol">
<li>ReLu ( max(0, x) )</li>
</ol>
<p>
<code>起到单侧抑制的作用</code>
<code>由于非负区间的梯度为常数，因此不存在梯度消失问题(Vanishing Gradient Problem)</code>
优点：(1) 训练速度非常快 (2)不会产生梯度消失 (3)一定程度上降低了模型的复杂度，使更稀疏
缺点：learning rate（梯度下降参数）过大时神经元十分脆弱，导致 <code>神经元死亡</code> 即输出激活值永远是0，参数不再更新
</p>
<ol class="org-ol">
<li>pReLu</li>
</ol>
<p>
pReLu = x, if x &gt; 0
pReLu = ax, if x &lt; 0
避免了死神经元的问题
</p>
<ol class="org-ol">
<li>softmax</li>
</ol>
<p>
e<sup>z</sup> / sum<sub>k</sub><sup>K</sup> e<sup>k</sup>
z为某神经元的线性值，K为当前层所有神经元的线性值。softmax特点是多分类，如果某一值明显大于其他值，那么softmax后会明显趋于1，其他的都趋于0，就完成了分类目的。
</p>

<p>
选择：
首选通用的ReLu，如果出现死神经元，上PReLu，再和Sigmoid、tanh等对比，找最好的
</p>
</div>
</div>
</div>



<div id="outline-container-orgf99cf82" class="outline-3">
<h3 id="orgf99cf82"><span class="section-number-3">16.4</span> BP反向传播网络</h3>
<div class="outline-text-3" id="text-16-4">
<p>
ref : <a href="http://blog.csdn.net/u014303046/article/details/78200010">http://blog.csdn.net/u014303046/article/details/78200010</a>
</p>

<p>
<code>损失函数和代价函数区别</code> 
</p>

<ul class="org-ul">
<li>损失函数主要指的是对于单个样本的损失或误差；</li>
<li>代价函数表示多样本同时输入模型的时候总体的误差——每个样本误差的和然后取平均值。</li>
</ul>

<p>
<code>什么是反向传播网络</code>
</p>
<ul class="org-ul">
<li>后项传播（正向），估计出神经元误差</li>
<li>前向传播（反向），估计出参数</li>
</ul>

<p>
<code>注意</code>
</p>
<ol class="org-ol">
<li>输入层神经元个数等于特征数</li>
<li>为什么会Relu会产生死神经元</li>
<li>死神经元(输出为0，参数不更新）和梯度消失（梯度为0）区别</li>
</ol>

<p>
<code>优点和不足</code>
残差传播到最前面的层已经变得很小，会出现梯度扩散，影响精度
</p>
</div>

<div id="outline-container-orgbd4bc8f" class="outline-4">
<h4 id="orgbd4bc8f"><span class="section-number-4">16.4.1</span> 推导</h4>
<div class="outline-text-4" id="text-16-4-1">
<p>
<code>牢记四个公式以及其推导过程</code>
</p>


<p>
<code>单样本情况下：</code>
</p>

<p>
定义l为神经网络层编号，j为神经元编号，z为线性值，a为激活值，sigmoid(z)=a
</p>

<p>
那么，如果需要为第l层的第j个神经元的线性值添加一个扰动 \(\Delta z_j^{[l]}\) ，需要使得最后的损失函数尽可能的变小，那么需要在其负梯度上进行，我们定义这个梯度为其误差 :
</p>

\begin{eqnarray}
\nonumber
\delta_j^{[l]} = \frac{\partial L(a, y)}{\partial z_j^{[l]}}
\end{eqnarray}
</div>

<div id="outline-container-org582fa47" class="outline-5">
<h5 id="org582fa47"><span class="section-number-5">16.4.1.1</span> 公式一：输出层误差</h5>
<div class="outline-text-5" id="text-16-4-1-1">
\begin{eqnarray}
\nonumber
\delta_j^{[l]} = \frac{\partial L }{\partial a_j^{[l]}} Sigmoid^{'} (z_j^{[l]})
\end{eqnarray}

<p>
证明：
</p>


<div class="figure">
<p><img src="pics/bp_equ_1.png" alt="bp_equ_1.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org94f169c" class="outline-5">
<h5 id="org94f169c"><span class="section-number-5">16.4.1.2</span> 公式二：隐含层误差</h5>
<div class="outline-text-5" id="text-16-4-1-2">
\begin{eqnarray}
\nonumber
\delta_j^{[l]} = \sum_k w_{kj}^{[l+1]} \delta_k^{[l+1]} Sigmoid^{'} (z_j^{[l]})
\end{eqnarray}

<p>
w<sub>kj</sub><sup>[l]</sup> 表示第l-1层的第j个神经元指向第l层的第k个神经元的权重
</p>

<p>
证明：
<img src="pics/bp_equ_2.png" alt="bp_equ_2.png" />
</p>
</div>
</div>

<div id="outline-container-orgfda8f24" class="outline-5">
<h5 id="orgfda8f24"><span class="section-number-5">16.4.1.3</span> 公式三：参数变化率，即w和b的梯度</h5>
<div class="outline-text-5" id="text-16-4-1-3">
\begin{eqnarray}
\nonumber
\frac{\partial L}{\partial b_j^{[l]}} &=& \delta _j^{[l]} \\
\nonumber
\frac{\partial L}{\partial w_{jk}^{[l]}} &=& a_k^{[l-1]} \delta _j^{[l]}
\end{eqnarray}

<p>
证明：
<img src="pics/bp_equ_3.png" alt="bp_equ_3.png" />
</p>
</div>
</div>

<div id="outline-container-org8efcf72" class="outline-5">
<h5 id="org8efcf72"><span class="section-number-5">16.4.1.4</span> 公式四：参数更新规则</h5>
<div class="outline-text-5" id="text-16-4-1-4">
<p>
以 α 为步长，负梯度为方向迭代，公式略
</p>

<p>
<code>多样本情况下:</code>
</p>

<p>
n表示第l层神经元个数，m为样本数
</p>
<ul class="org-ul">
<li>每一层的误差不再是一个n维的向量，而是一个nxm的矩阵</li>
<li>更新b的时候要对每一层的误差矩阵求行均值</li>
<li>得到的w依然是 nxn 的矩阵</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org9ace37b" class="outline-3">
<h3 id="org9ace37b"><span class="section-number-3">16.5</span> 卷积神经网络</h3>
<div class="outline-text-3" id="text-16-5">
<p>
ref: <a href="http://blog.csdn.net/aws3217150/article/details/46405095">http://blog.csdn.net/aws3217150/article/details/46405095</a>
</p>


<p>
要点：
</p>
<ol class="org-ol">
<li>主要用在图像识别问题上</li>
<li>自带正则化功能，大大减少了参数数目，因此减少了过拟合的程度</li>
</ol>
</div>

<div id="outline-container-org6273d02" class="outline-4">
<h4 id="org6273d02"><span class="section-number-4">16.5.1</span> 卷积</h4>
<div class="outline-text-4" id="text-16-5-1">
<p>
卷积通过核矩阵，将一个较大的矩阵进行缩减
</p>


<div class="figure">
<p><img src="pics/convolved.png" alt="convolved.png" />
</p>
</div>

<ul class="org-ul">
<li>如果原始矩阵不仅有长度、宽度，还有深度，则每一深度进行累加（比如：彩图深度为3）</li>
<li>如果有多个核函数，每个核函数分别计算</li>
<li>每个核函数总能得到一个 NxN 的卷积特征</li>
</ul>
</div>

<div id="outline-container-org4a38da4" class="outline-5">
<h5 id="org4a38da4"><span class="section-number-5">16.5.1.1</span> 卷积中的超参数：</h5>
<div class="outline-text-5" id="text-16-5-1-1">
<ol class="org-ol">
<li>补充0的长度P，一个是为了使图片的形状更方便我们进行卷积，另一个是因为它可以提高识别表现(详细原因请参考cs231n的课程)，比如5x5的图，P=2时得到7x7的图</li>
<li>核函数的大小和数量</li>
<li>步长 Stride，卷积中默认卷积核一次移动一个单位，其实可以移动Stride单位</li>
</ol>

<p>
假设图片宽度为W， 卷积核宽度为F， 步长为S，补0参数P，输出卷积特征的宽度为H，则有：
</p>

\begin{eqnarray}
\nonumber
H = (W - F + 2P) / S + 1
\end{eqnarray}
</div>
</div>
</div>


<div id="outline-container-org888c59b" class="outline-4">
<h4 id="org888c59b"><span class="section-number-4">16.5.2</span> 池化(Pooling)</h4>
<div class="outline-text-4" id="text-16-5-2">
<p>
pooling是一个采样过程，一般采取max-pooling
</p>


<div class="figure">
<p><img src="pics/pooling.png" alt="pooling.png" />
</p>
</div>
</div>
</div>



<div id="outline-container-org5a93130" class="outline-4">
<h4 id="org5a93130"><span class="section-number-4">16.5.3</span> 全连接</h4>
<div class="outline-text-4" id="text-16-5-3">
<p>
和一般人工神经网络一样
</p>
</div>
</div>

<div id="outline-container-org5f8c401" class="outline-4">
<h4 id="org5f8c401"><span class="section-number-4">16.5.4</span> 卷积层参数的确定</h4>
<div class="outline-text-4" id="text-16-5-4">
<p>
<code>共享权重</code>
</p>

<p>
同一个卷积核，核上每个元素的权重都一样，即如果是5x5的卷积核，则一个核一共只需估计5x5+1=26个参数（1位bias项）
</p>

<p>
例：
卷积特征 96x96x10（10个核）
-&gt; 经过2x2，步长为2的池化后，得到48x48x10个特征
-&gt; 再经过 5x5，步长为1，16个卷积核进行卷积，得到(5x5x10+1)x16=4016个参数，输出特征为44x44x16
-&gt; 再pooling，得到22x22x16个输出特征
-&gt; 此时，全连接到100个神经元的隐含层，需要的参数为：(22x22x16+1)*100=774500
</p>

<p>
最后得到的参数=774500 + 4016 + 隐含层到输出层的参数
</p>
</div>
</div>
</div>

<div id="outline-container-org9f67f12" class="outline-3">
<h3 id="org9f67f12"><span class="section-number-3">16.6</span> 循环神经网络(Recurrent Neural Network)</h3>
<div class="outline-text-3" id="text-16-6">
<p>
ref：
    <a href="https://zybuluo.com/hanbingtao/note/541458">https://zybuluo.com/hanbingtao/note/541458</a>
    <a href="https://zhuanlan.zhihu.com/p/24720659">https://zhuanlan.zhihu.com/p/24720659</a>
</p>

<p>
<code>以层的概念理解神经网络结构，而不是节点</code>
</p>
</div>

<div id="outline-container-org9db7915" class="outline-4">
<h4 id="org9db7915"><span class="section-number-4">16.6.1</span> 单向循环神经网络</h4>
<div class="outline-text-4" id="text-16-6-1">

<div class="figure">
<p><img src="pics/rnn_1.png" alt="rnn_1.png" />
</p>
</div>

<p>
中间的隐含层为循环层，循环层每一个节点的值不仅受与其连接的x的影响，还和上一个循环层节点的影响。
</p>

\begin{eqnarray}
\nonumber
o_t = g(V_{s_t}) \\
\nonumber
s_t = f(Ux_t + W _{t-1}) 
\end{eqnarray}

<p>
其中每个循环层神经元的 W, V, U 都是完全一样的，这是循环神经网络的 <code>共享权重</code> 特征，是递归网络相对于前馈网络而言最为突出的优势。
</p>

<p>
<code>时间结构共享是递归网络的核心中的核心。</code>
</p>
</div>
</div>

<div id="outline-container-orgaabb95a" class="outline-4">
<h4 id="orgaabb95a"><span class="section-number-4">16.6.2</span> 双向循环神经网络</h4>
<div class="outline-text-4" id="text-16-6-2">
<p>
上述神经网络方向是 S<sub>t-1</sub> 到 S<sub>t</sub> ，而我们还可以加入一个反向的循环层
</p>


<div class="figure">
<p><img src="pics/rnn_2.png" alt="rnn_2.png" />
</p>
</div>

<p>
此时需要同时保存两个层：
</p>

\begin{eqnarray}
\nonumber
o_t = g(V_{t} + V_{t}' ) \\
\end{eqnarray}
</div>
</div>

<div id="outline-container-org326e759" class="outline-4">
<h4 id="org326e759"><span class="section-number-4">16.6.3</span> 训练方法：BPTT</h4>
<div class="outline-text-4" id="text-16-6-3">
<p>
见：<a href="https://zybuluo.com/hanbingtao/note/541458">https://zybuluo.com/hanbingtao/note/541458</a>
</p>
</div>
</div>


<div id="outline-container-orgd32c648" class="outline-4">
<h4 id="orgd32c648"><span class="section-number-4">16.6.4</span> softmax 层</h4>
<div class="outline-text-4" id="text-16-6-4">
<p>
ref： <a href="https://www.zhihu.com/question/23765351">https://www.zhihu.com/question/23765351</a>
</p>

\begin{eqnarray}
\nonumber
S_i = \frac{e^{V_i}}{\sum_j e^{v_j}}
\end{eqnarray}

<p>
其中V<sub>i表示V中第i个元素</sub>，Softmax即是该元素的指数，和所有元素指数和的比值
</p>


<div class="figure">
<p><img src="pics/rnn_5.png" alt="rnn_5.png" />
</p>
</div>
</div>
</div>


<div id="outline-container-org204d67f" class="outline-4">
<h4 id="org204d67f"><span class="section-number-4">16.6.5</span> 优缺点</h4>
<div class="outline-text-4" id="text-16-6-5">
<p>
优点：
</p>
<ol class="org-ol">
<li>解决时序问题</li>
<li>共享权重</li>
</ol>

<p>
缺点：
</p>
<ol class="org-ol">
<li>时序过长时出现梯度爆炸和梯度消失问题</li>
</ol>

<p>
<code>什么是梯度爆炸和梯度消失</code>
</p>


<div class="figure">
<p><img src="pics/run_3.png" alt="run_3.png" />
</p>
</div>

<p>
当t-k很大时，误差将极快的收敛到0或者无穷大
</p>

<p>
如何解决：长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU）
</p>
</div>
</div>





<div id="outline-container-org605cc11" class="outline-4">
<h4 id="org605cc11"><span class="section-number-4">16.6.6</span> 输入与输出</h4>
<div class="outline-text-4" id="text-16-6-6">
<p>
<code>方式：</code>
</p>


<div class="figure">
<p><img src="pics/rnn_4.png" alt="rnn_4.png" />
</p>
</div>

<ul class="org-ul">
<li>many to one：常用在情感分析中，将一句话关联到一个情感向量上去。</li>
<li>many to many：第一个many to many在DNN-HMM语音识别框架中常有用到</li>
<li>many to many(variable length)：第二个many to many常用在机器翻译两个不同语言时。</li>
</ul>

<p>
<code>类型</code>
</p>

<p>
与其他前馈网络不同，该网络必须包含时间参数
输入张量形状：(time<sub>steps</sub>, n<sub>samples</sub>, dim<sub>input</sub>)
输出张量形状：(time<sub>steps</sub>, n<sub>samples</sub>, dim<sub>output</sub>)
</p>
</div>
</div>
</div>


<div id="outline-container-orga2ea9ef" class="outline-3">
<h3 id="orga2ea9ef"><span class="section-number-3">16.7</span> 递归神经网络(Recursive Neural Network)&#xa0;&#xa0;&#xa0;<span class="tag"><span class="__">了解</span></span></h3>
<div class="outline-text-3" id="text-16-7">
<p>
ref: <a href="https://zybuluo.com/hanbingtao/note/626300">https://zybuluo.com/hanbingtao/note/626300</a>
</p>

<p>
循环神经网络将句子看成一个序列，然而很多时候句子是有结构的（语法树：树状结构）
</p>

<p>
需要更多人工标注的语料
</p>
</div>
</div>

<div id="outline-container-orgcfd45aa" class="outline-3">
<h3 id="orgcfd45aa"><span class="section-number-3">16.8</span> LSTM</h3>
<div class="outline-text-3" id="text-16-8">
<p>
为了解决循环神经网络（以后默认RNN为循环神经网络）存在的梯度爆炸和梯度消失的问题，LSTM(Long Short Term Memory Network, LSTM)被提出来
</p>

<p>
LSTM在循环层中不仅存储原来的状态 h<sub>t</sub> 还存储一个长期的状态 c<sub>t</sub> (单元状态)，如下图所示
</p>


<div class="figure">
<p><img src="pics/LSTM_0.png" alt="LSTM_0.png" />
</p>
</div>

<p>
其中每一个状态都是向量。
</p>

<p>
<code>LSTM的关键，就是怎样控制长期状态c</code>
</p>

<p>
为此，LSTM提出门(gate)的概念，类似阀门，可以控制只让一部分的状态进来，门事实上是一个全连接层，输入一个向量，输出一个0到1之间的实数等长向量，用我们要控制的向量乘以门得到的结果向量，就可以达到控制的目的。
</p>

<p>
LSTM有三个门：
</p>

<ul class="org-ul">
<li>遗忘门：它决定了上一时刻的单元状态 c<sub>t-1</sub> 有多少保留到当前时刻单元状态 c<sub>t</sub></li>
<li>输入门：它决定了当前时刻网络的输入 x<sub>t</sub> 有多少保存到单元状态 c<sub>t</sub></li>
<li>输出门：它决定了当前时刻的单元状态 c<sub>t</sub> 有多少输出到循环层节点的输出值 h<sub>t</sub></li>
</ul>

<p>
总体流程如下图：（非常关键）
</p>


<div class="figure">
<p><img src="pics/LSTM_1.png" alt="LSTM_1.png" />
</p>
</div>

<p>
上图解释如下：
</p>

<ol class="org-ol">
<li>首先，输入 x<sub>t</sub> 经过变换：</li>
</ol>

\begin{eqnarray}
\nonumber
f_t = \sigma (W_f [h_{t-1}, x_t] + b_f)
\end{eqnarray}

<p>
得到遗忘门向量，作用于 c<sub>t-1</sub>上（直接按元素乘以 c<sub>t-1</sub>)
</p>

<ol class="org-ol">
<li>再将 x<sub>t</sub> 经过输入门的变化（公式类似），得到输入门向量 i<sub>t</sub> ， <code>输入门是作用在当前的单元状态 c'_t 上的</code></li>

<li>计算用于描绘当前输入的单元状态 c'<sub>t</sub> ，根据上一次的输出 h<sub>t-1</sub> 和这次的输入 x<sub>t</sub>，得到结果后乘以输入门，达到控制效果，然后加到经过遗忘门后的上一期单元状态中，得到更新后当期单元状态 c<sub>t</sub> ，可输出为 c<sub>t</sub> 。由于遗忘门的控制，它可以保存很久很久之前的信息，由于输入门的控制，它又可以避免当前无关紧要的内容进入记忆。</li>

<li>计算 h<sub>t</sub> 的输出门 O<sub>t</sub> ，它控制了长期记忆对当前输出的影响。</li>
<li>得到LSTM最终输出，它是由输出门和单元状态共同确定的：</li>
</ol>

\begin{eqnarray}
\nonumber
h_t = O_t \odot tanh(c_t)
\end{eqnarray}
</div>
</div>


<div id="outline-container-org199f626" class="outline-3">
<h3 id="org199f626"><span class="section-number-3">16.9</span> 实现</h3>
<div class="outline-text-3" id="text-16-9">
</div>
<div id="outline-container-org7d568cb" class="outline-4">
<h4 id="org7d568cb"><span class="section-number-4">16.9.1</span> 方案1. win10 tensorflow 安装</h4>
<div class="outline-text-4" id="text-16-9-1">
<p>
ref:
<a href="http://blog.csdn.net/weixin_36368407/article/details/54177380">http://blog.csdn.net/weixin_36368407/article/details/54177380</a>
</p>
</div>

<div id="outline-container-org9b411af" class="outline-5">
<h5 id="org9b411af"><span class="section-number-5">16.9.1.1</span> Cuda &amp; cudnn</h5>
</div>

<div id="outline-container-org9c4e79b" class="outline-5">
<h5 id="org9c4e79b"><span class="section-number-5">16.9.1.2</span> TensorFlow</h5>
</div>
</div>

<div id="outline-container-orgcedb8be" class="outline-4">
<h4 id="orgcedb8be"><span class="section-number-4">16.9.2</span> 方案2. 基于theano的Keras win10 安装</h4>
<div class="outline-text-4" id="text-16-9-2">
<p>
<a href="http://blog.csdn.net/circle2015/article/details/54235127">http://blog.csdn.net/circle2015/article/details/54235127</a>
</p>

<ol class="org-ol">
<li>安装 mingw</li>
</ol>

<div class="org-src-container">
<pre class="src src-bash">conda install mingw libpython
</pre>
</div>

<ol class="org-ol">
<li>pip install theano</li>
<li>pip install keras</li>
<li>主文件夹中找到 .keras的配置文件，修改默认后台为theano</li>
<li>配置theano</li>
</ol>

<p>
c://Users//ray//.theanorc.txt 文件，里面加入theano的配置项
</p>
<pre class="example">
[global]
floatX=float32
device=cpu
[blas]
ldflags=-LC:\\Users\\yangr\\Documents\\OpenBLAS\\bin -LC:\\Users\\yangr\\Documents\\OpenBLAS\\lib -lopenblas
[gcc]  
cxxflags=-IC:\\Users\\yangr\\Anaconda3\\MinGW
</pre>

<p>
其中openBlas加速库要提前下载：
</p>
<ol class="org-ol">
<li>下载openblas库<a href="http://sourceforge.net/projects/openblas/files/v0.2.14">http://sourceforge.net/projects/openblas/files/v0.2.14</a></li>
<li>下载mingw64 <a href="http://sourceforge.net/projects/openblas/files/v0.2.14/mingw64_dll.zip/download">http://sourceforge.net/projects/openblas/files/v0.2.14/mingw64_dll.zip/download</a> ，并将其添加到openblas/bin/</li>
<li>路径输入至.theanorc.txt</li>
</ol>

<p>
<code>如果遇到问题</code>
</p>

<div class="org-src-container">
<pre class="src src-python">File <span style="color: #2d9574;">"E:\Things_Installed_Here\Anaconda_Py\envs\Py341\lib\site-packages\theano-0.7.0-py3.4.egg\theano\gof\cmodule.py"</span>, line <span style="color: #a45bad;">331</span>, <span style="color: #4f97d7; font-weight: bold;">in</span> dlimport
    <span style="color: #7590db;">rval</span> = <span style="color: #4f97d7;">__import__</span><span style="color: #4f97d7;">(</span>module_name, <span style="color: #bc6ec5;">{}</span>, <span style="color: #bc6ec5;">{}</span>, <span style="color: #bc6ec5;">[</span>module_name<span style="color: #bc6ec5;">]</span><span style="color: #4f97d7;">)</span>
<span style="color: #ce537a; font-weight: bold;">ImportError</span>: DLL load failed: The specified module could <span style="color: #4f97d7; font-weight: bold;">not</span> be found.
</pre>
</div>

<p>
则如下操作：
</p>
<div class="org-src-container">
<pre class="src src-bash">$ conda remove mingw
$ conda install m2w64-toolchain
</pre>
</div>
</div>
</div>
<div id="outline-container-orgcd3f193" class="outline-4">
<h4 id="orgcd3f193"><span class="section-number-4">16.9.3</span> Keras Tips</h4>
<div class="outline-text-4" id="text-16-9-3">
</div>
<div id="outline-container-org33b6708" class="outline-5">
<h5 id="org33b6708"><span class="section-number-5">16.9.3.1</span> 自定义 metrics 性能评估函数</h5>
<div class="outline-text-5" id="text-16-9-3-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">import</span> keras.backend <span style="color: #4f97d7; font-weight: bold;">as</span> K
<span style="color: #4f97d7; font-weight: bold;">def</span> <span style="color: #bc6ec5; font-weight: bold;">f1_score</span><span style="color: #4f97d7;">(</span>y_true, y_pred<span style="color: #4f97d7;">)</span>:

    <span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">Count positive samples.</span>
    <span style="color: #7590db;">c1</span> = K.<span style="color: #4f97d7;">sum</span><span style="color: #4f97d7;">(</span>K.<span style="color: #4f97d7;">round</span><span style="color: #bc6ec5;">(</span>K.clip<span style="color: #2d9574;">(</span>y_true * y_pred, <span style="color: #a45bad;">0</span>, <span style="color: #a45bad;">1</span><span style="color: #2d9574;">)</span><span style="color: #bc6ec5;">)</span><span style="color: #4f97d7;">)</span>
    <span style="color: #7590db;">c2</span> = K.<span style="color: #4f97d7;">sum</span><span style="color: #4f97d7;">(</span>K.<span style="color: #4f97d7;">round</span><span style="color: #bc6ec5;">(</span>K.clip<span style="color: #2d9574;">(</span>y_pred, <span style="color: #a45bad;">0</span>, <span style="color: #a45bad;">1</span><span style="color: #2d9574;">)</span><span style="color: #bc6ec5;">)</span><span style="color: #4f97d7;">)</span>
    <span style="color: #7590db;">c3</span> = K.<span style="color: #4f97d7;">sum</span><span style="color: #4f97d7;">(</span>K.<span style="color: #4f97d7;">round</span><span style="color: #bc6ec5;">(</span>K.clip<span style="color: #2d9574;">(</span>y_true, <span style="color: #a45bad;">0</span>, <span style="color: #a45bad;">1</span><span style="color: #2d9574;">)</span><span style="color: #bc6ec5;">)</span><span style="color: #4f97d7;">)</span>

    <span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">If there are no true samples, fix the F1 score at 0.</span>
    <span style="color: #4f97d7; font-weight: bold;">if</span> c3 == <span style="color: #a45bad;">0</span>:
        <span style="color: #4f97d7; font-weight: bold;">return</span> <span style="color: #a45bad;">0</span>

    <span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">How many selected items are relevant?</span>
    <span style="color: #7590db;">precision</span> = c1 / c2

    <span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">How many relevant items are selected?</span>
    <span style="color: #7590db;">recall</span> = c1 / c3

    <span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">Calculate f1_score</span>
    <span style="color: #7590db;">f1_score</span> = <span style="color: #a45bad;">2</span> * <span style="color: #4f97d7;">(</span>precision * recall<span style="color: #4f97d7;">)</span> / <span style="color: #4f97d7;">(</span>precision + recall<span style="color: #4f97d7;">)</span>
    <span style="color: #4f97d7; font-weight: bold;">return</span> f1_score
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgd86036d" class="outline-4">
<h4 id="orgd86036d"><span class="section-number-4">16.9.4</span> Experiment 1. 信贷违约预测模型</h4>
</div>

<div id="outline-container-org56da443" class="outline-4">
<h4 id="org56da443"><span class="section-number-4">16.9.5</span> Experiment 2. 一个简单的缺失语言自动补全</h4>
</div>


<div id="outline-container-org92458aa" class="outline-4">
<h4 id="org92458aa"><span class="section-number-4">16.9.6</span> Experiment 3. 手写数字识别</h4>
</div>
</div>
</div>
<div id="outline-container-orgb9090e7" class="outline-2">
<h2 id="orgb9090e7"><span class="section-number-2">17</span> 工具</h2>
<div class="outline-text-2" id="text-17">
<ul class="org-ul">
<li>storm</li>
<li>Flink</li>
<li>elasticsearch</li>
<li>kafka</li>
</ul>
</div>
</div>
<div id="outline-container-org36ee947" class="outline-2">
<h2 id="org36ee947"><span class="section-number-2">18</span> 推荐算法 / 推荐系统</h2>
<div class="outline-text-2" id="text-18">
<p>
推荐书籍：
</p>
<ul class="org-ul">
<li>《推荐系统实践》项亮<a href="http://book.douban.com/subject/10769749/">http://book.douban.com/subject/10769749/</a></li>
<li>《Recommender Systems Handbook》Paul B. Kantor  <a href="http://book.douban.com/subject/3695850/">http://book.douban.com/subject/3695850/</a></li>
</ul>
</div>

<div id="outline-container-org07cd1d4" class="outline-3">
<h3 id="org07cd1d4"><span class="section-number-3">18.1</span> 前言</h3>
<div class="outline-text-3" id="text-18-1">
</div>
<div id="outline-container-org6ea490e" class="outline-4">
<h4 id="org6ea490e"><span class="section-number-4">18.1.1</span> 隐式反馈和显式反馈</h4>
<div class="outline-text-4" id="text-18-1-1">
<ul class="org-ul">
<li>显式反馈：用户对某物品表示了明显的喜好，如标记是否喜欢、评分、收藏等。特点：行为较少</li>
<li>隐式反馈：未表示明显喜好，用户兴趣不明确，如浏览日志、购买行为。特点：数量庞大</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org48091a6" class="outline-3">
<h3 id="org48091a6"><span class="section-number-3">18.2</span> 推荐算法类别</h3>
<div class="outline-text-3" id="text-18-2">
</div>
<div id="outline-container-org328aac2" class="outline-4">
<h4 id="org328aac2"><span class="section-number-4">18.2.1</span> 基于内容的推荐</h4>
<div class="outline-text-4" id="text-18-2-1">
<p>
利用项目的内在品质或者固有属性来进行推荐，比如音乐的流派、类型，电影的风格、类别等，不需要构建UI矩阵（User-Item矩阵）
</p>

<ul class="org-ul">
<li>优点：避免 <code>冷启动</code> 问题，即一个item从来没被关注过，则不会有推荐算法推荐，导致马太效应，流行的越流行，不流行的越不流行</li>
<li>缺点：很可能推荐的东西和所浏览过的一致</li>
</ul>
</div>
</div>

<div id="outline-container-orgce7b409" class="outline-4">
<h4 id="orgce7b409"><span class="section-number-4">18.2.2</span> 基于协同过滤的推荐</h4>
<div class="outline-text-4" id="text-18-2-2">
</div>
<div id="outline-container-org2ddd632" class="outline-5">
<h5 id="org2ddd632"><span class="section-number-5">18.2.2.1</span> 基于内存的</h5>
<div class="outline-text-5" id="text-18-2-2-1">
<p>
都需要将数据全部读入内存中
</p>

<ul class="org-ul">
<li>基于用户的协同推荐</li>
<li>基于项目的协同推荐</li>
</ul>
</div>
</div>


<div id="outline-container-orgdd5712c" class="outline-5">
<h5 id="orgdd5712c"><span class="section-number-5">18.2.2.2</span> 基于模型的</h5>
<div class="outline-text-5" id="text-18-2-2-2">
<ul class="org-ul">
<li>包括 Aspect Model, pLSA, LDA, 聚类, SVD, Matrix Factorization, 贝叶斯网络</li>
<li>训练时间较长，但是训练出来之后推荐过程较快</li>
<li>能够较好的处理稀疏矩阵的问题</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org9dd4fcf" class="outline-4">
<h4 id="org9dd4fcf"><span class="section-number-4">18.2.3</span> 基于知识的推荐 / 混合推荐</h4>
</div>
</div>


<div id="outline-container-org8f918c7" class="outline-3">
<h3 id="org8f918c7"><span class="section-number-3">18.3</span> 基于内容的推荐</h3>
<div class="outline-text-3" id="text-18-3">
<p>
Andrew NG课程
</p>

<p>
ref:  <a href="http://lib.csdn.net/article/machinelearning/1101">http://lib.csdn.net/article/machinelearning/1101</a>
</p>

<p>
假设Item有k个属性，每个Item对应得到长为(k+1)的属性向量x（包含了偏移项，为0）。那么每个User对Item每个属性存在一个偏好向量 θ，该向量长度同样为k+1，而 θ是未知的、不可观测的，需要进行估计。
</p>

<p>
在UI矩阵中，存在一部分未知的值，如果将每个User的 θ可以估计出来，那么未知的值也可以通过 θ<sup>T</sup> x 得到，因此我们根据UI矩阵现有的值，估计 θ。见如下公式，其中 y(i,j)是UI矩阵第i行第j列的值，j表示第j个用户，i表示第i个Item，第二项为正则项，防止过拟合。
</p>



<div class="figure">
<p><img src="pics/content_based_RS.png" alt="content_based_RS.png" />
</p>
</div>


<p>
该式可用梯度下降方法求解，详细见博客。
</p>
</div>
</div>

<div id="outline-container-org15f81a7" class="outline-3">
<h3 id="org15f81a7"><span class="section-number-3">18.4</span> 基于内存的协同过滤推荐</h3>
<div class="outline-text-3" id="text-18-4">
<p>
ref: <a href="https://www.cnblogs.com/baihuaxiu/p/6617389.html">https://www.cnblogs.com/baihuaxiu/p/6617389.html</a>
</p>
</div>

<div id="outline-container-orgcf56311" class="outline-4">
<h4 id="orgcf56311"><span class="section-number-4">18.4.1</span> 基于用户的</h4>
<div class="outline-text-4" id="text-18-4-1">
<p>
每个用户为样本，每个项目为特征，计算每个用户的相似度，在最相似的用户中，推荐其他人有但是其没有的项目
</p>
</div>
</div>

<div id="outline-container-orgefd4e96" class="outline-4">
<h4 id="orgefd4e96"><span class="section-number-4">18.4.2</span> 基于项目的</h4>
<div class="outline-text-4" id="text-18-4-2">
<p>
每个项目为样本，每个用户位特征，计算每个项目的相似度，将最相似的项目关联，如果用户买了其中一个另一个没买，则进行推荐
</p>
</div>
</div>


<div id="outline-container-org81ea6ad" class="outline-4">
<h4 id="org81ea6ad"><span class="section-number-4">18.4.3</span> 存在的问题</h4>
<div class="outline-text-4" id="text-18-4-3">
<p>
比如一些非常流行的商品可能很多人都喜欢，这种商品推荐给你就没什么意义了，所以计算的时候需要对这种商品加一个权重或者把这种商品完全去掉也行。
</p>

<p>
再有，对于一些通用的东西，比如买书的时候的工具书，如现代汉语词典，新华字典神马的，通用性太强了，推荐也没什么必要了。
</p>
</div>
</div>
</div>


<div id="outline-container-org7ab9dcf" class="outline-3">
<h3 id="org7ab9dcf"><span class="section-number-3">18.5</span> 基于模型的协同过滤推荐</h3>
<div class="outline-text-3" id="text-18-5">
<p>
对于一个UI矩阵，里面元素是用户对商品的打分。因为很多用户并未尝试过某些商品，所以必将有大量元素未知。
</p>

<p>
<code>推荐系统的目标就是预测出UI矩阵中的未知元素</code>
</p>
</div>

<div id="outline-container-org5300881" class="outline-4">
<h4 id="org5300881"><span class="section-number-4">18.5.1</span> 矩阵分解</h4>
<div class="outline-text-4" id="text-18-5-1">
<p>
ref:
</p>
<ul class="org-ul">
<li><a href="http://blog.csdn.net/sun_168/article/details/20637833">http://blog.csdn.net/sun_168/article/details/20637833</a></li>
<li><a href="https://www.jianshu.com/p/71bcad876c05">https://www.jianshu.com/p/71bcad876c05</a></li>
</ul>

<p>
另UI矩阵为R, 矩阵分解可以得到 R = UV （U表示用户因子矩阵，V为商品因子矩阵），假设共有n个用户，m个商品
</p>

\begin{eqnarray}
\nonumber
R_{n \times m} = U_{n \times k} V · I_{k \times m}
\end{eqnarray}

<p>
我们要做的是估计出U和I矩阵，从而对R矩阵中的缺失值进行预测
</p>

<p>
解决方法：
</p>
</div>
<div id="outline-container-orgd6c3689" class="outline-5">
<h5 id="orgd6c3689"><span class="section-number-5">18.5.1.1</span> 构建目标函数</h5>
<div class="outline-text-5" id="text-18-5-1-1">
<p>
ref: <a href="http://blog.csdn.net/google19890102/article/details/51124556">http://blog.csdn.net/google19890102/article/details/51124556</a>
</p>

\begin{eqnarray}
\nonumber
L = \sum_{i=1}^n \sum_{j=1}^m (R_{ij} - U_i^T I_j)^2
\end{eqnarray}

<p>
由于L对U和I都是凸函数，因此可以直接用梯度下降法，求局部最优解
</p>


<p>
为了防止过拟合，可在其中加入正则项
</p>

  \begin{eqnarray}
\nonumber
L = \sum_{i=1}^n \sum_{j=1}^m [(R_{ij} - U_i^T I_j)^2 + \lambda_1 ||U_i||^2 + \lambda_2 ||I_j||^2]
\end{eqnarray}
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org3983cb1" class="outline-2">
<h2 id="org3983cb1"><span class="section-number-2">19</span> 机器学习算法调试</h2>
<div class="outline-text-2" id="text-19">
</div>
<div id="outline-container-org95252ea" class="outline-3">
<h3 id="org95252ea"><span class="section-number-3">19.1</span> 梯度检查</h3>
</div>
</div>
<div id="outline-container-orgaae19bd" class="outline-2">
<h2 id="orgaae19bd"><span class="section-number-2">20</span> 数据库</h2>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: 杨 睿</p>
<p class="date">Created: 2018-06-08 五 17:17</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
